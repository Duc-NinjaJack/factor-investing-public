{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a96f0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QVM Engine v3j - Integrated Strategy with Validated Factors (Full Implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23695d",
   "metadata": {},
   "source": [
    "=============================================================================\n",
    "CONFIGURATION AND DATABASE SETUP\n",
    "============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1381881",
   "metadata": {},
   "source": [
    "# QVM Engine v3j - Integrated Strategy with Validated Factors\n",
    "\n",
    "**Objective:** Full implementation of QVM Engine v3j with statistically validated factors:\n",
    "- Regime detection\n",
    "- Value factors (P/E + FCF Yield)\n",
    "- Quality factors (ROAA + Piotroski F-Score)\n",
    "- Momentum factors (Multi-horizon + Low-Volatility)\n",
    "- Integrated portfolio construction\n",
    "\n",
    "**File:** 08_integrated_strategy_with_validated_factors.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28e081a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Database connectivity\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d720a",
   "metadata": {},
   "source": [
    "# IMPORTS AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b78da015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported production modules.\n",
      "   - Project Root set to: /Users/raymond/Documents/Projects/factor-investing-public\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add Project Root to Python Path\n",
    "try:\n",
    "    current_path = Path.cwd()\n",
    "    while not (current_path / 'production').is_dir():\n",
    "        if current_path.parent == current_path:\n",
    "            raise FileNotFoundError(\"Could not find the 'production' directory.\")\n",
    "        current_path = current_path.parent\n",
    "    \n",
    "    project_root = current_path\n",
    "    \n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    \n",
    "    from production.database.connection import get_database_manager\n",
    "    from production.database.mappings.financial_mapping_manager import FinancialMappingManager\n",
    "    print(f\"✅ Successfully imported production modules.\")\n",
    "    print(f\"   - Project Root set to: {project_root}\")\n",
    "\n",
    "except (ImportError, FileNotFoundError) as e:\n",
    "    print(f\"❌ ERROR: Could not import production modules. Please check your directory structure.\")\n",
    "    print(f\"   - Final Path Searched: {project_root}\")\n",
    "    print(f\"   - Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2430f0f",
   "metadata": {},
   "source": [
    "# CONFIGURATION AND DATABASE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "273e1ea5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️  QVM Engine v3j Validated Factors Configuration Loaded:\n",
      "   - Strategy: QVM_Engine_v3j_Validated_Factors\n",
      "   - Period: 2016-01-01 to 2025-07-28\n",
      "   - Universe: Top 200 stocks by ADTV\n",
      "   - Value Factors: P/E + FCF Yield (33% weight)\n",
      "   - Quality Factors: ROAA + Piotroski F-Score (33% weight)\n",
      "   - Momentum Factors: Multi-horizon + Low-Volatility (34% weight)\n",
      "   - Regime Detection: Fixed thresholds with 4-regime classification\n",
      "   - Performance: Pre-computed data + Vectorized operations\n"
     ]
    }
   ],
   "source": [
    "QVM_CONFIG = {\n",
    "    # Backtest Parameters\n",
    "    \"strategy_name\": \"QVM_Engine_v3j_Validated_Factors\",\n",
    "    \"backtest_start_date\": \"2016-01-01\",\n",
    "    \"backtest_end_date\": \"2025-07-28\",\n",
    "    \"rebalance_frequency\": \"M\", # Monthly\n",
    "    \"transaction_cost_bps\": 30, # Flat 30bps\n",
    "    \n",
    "    # Universe Construction\n",
    "    \"universe\": {\n",
    "        \"lookback_days\": 63,\n",
    "        \"top_n_stocks\": 200,  # Top 200 stocks by ADTV\n",
    "        \"max_position_size\": 0.05,\n",
    "        \"max_sector_exposure\": 0.30,\n",
    "        \"target_portfolio_size\": 20,\n",
    "    },\n",
    "    \n",
    "    # Factor Configuration - Validated Factors Structure\n",
    "    \"factors\": {\n",
    "        \"value_weight\": 0.33,      # Value factors (P/E + FCF Yield)\n",
    "        \"quality_weight\": 0.33,    # Quality factors (ROAA + F-Score)\n",
    "        \"momentum_weight\": 0.34,   # Momentum factors (Momentum + Low-Vol)\n",
    "        \n",
    "        # Value Factors (0.33 total weight)\n",
    "        \"value_factors\": {\n",
    "            \"pe_weight\": 0.5,        # 0.165 of total (contrarian - lower is better)\n",
    "            \"fcf_yield_weight\": 0.5  # 0.165 of total (positive - higher is better)\n",
    "        },\n",
    "        \n",
    "        # Quality Factors (0.33 total weight)\n",
    "        \"quality_factors\": {\n",
    "            \"roaa_weight\": 0.5,    # 0.165 of total (positive - higher is better)\n",
    "            \"fscore_weight\": 0.5   # 0.165 of total (positive - higher is better)\n",
    "        },\n",
    "        \n",
    "        # Momentum Factors (0.34 total weight)\n",
    "        \"momentum_factors\": {\n",
    "            \"momentum_weight\": 0.5, # 0.17 of total (mixed signals)\n",
    "            \"low_vol_weight\": 0.5   # 0.17 of total (defensive - inverse volatility)\n",
    "        },\n",
    "        \n",
    "        # Factor Calculation Parameters\n",
    "        \"momentum_horizons\": [21, 63, 126, 252], # 1M, 3M, 6M, 12M\n",
    "        \"skip_months\": 1,\n",
    "        \"fundamental_lag_days\": 45,  # 45-day lag for announcement delay\n",
    "        \"volatility_lookback\": 252,  # 252-day rolling window for low-vol\n",
    "        \"fcf_imputation_rate\": 0.30  # Expected CapEx imputation rate\n",
    "    },\n",
    "    \n",
    "    \"regime\": {\n",
    "        \"lookback_period\": 90,          # 90 days lookback period\n",
    "        \"volatility_threshold\": 0.0140, # 1.40% (75th percentile from real data)\n",
    "        \"return_threshold\": 0.0012,     # 0.12% (75th percentile from real data)\n",
    "        \"low_return_threshold\": 0.0002  # 0.02% (corrected 25th percentile)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n⚙️  QVM Engine v3j Validated Factors Configuration Loaded:\")\n",
    "print(f\"   - Strategy: {QVM_CONFIG['strategy_name']}\")\n",
    "print(f\"   - Period: {QVM_CONFIG['backtest_start_date']} to {QVM_CONFIG['backtest_end_date']}\")\n",
    "print(f\"   - Universe: Top {QVM_CONFIG['universe']['top_n_stocks']} stocks by ADTV\")\n",
    "print(f\"   - Value Factors: P/E + FCF Yield (33% weight)\")\n",
    "print(f\"   - Quality Factors: ROAA + Piotroski F-Score (33% weight)\")\n",
    "print(f\"   - Momentum Factors: Multi-horizon + Low-Volatility (34% weight)\")\n",
    "print(f\"   - Regime Detection: Fixed thresholds with 4-regime classification\")\n",
    "print(f\"   - Performance: Pre-computed data + Vectorized operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ec981",
   "metadata": {},
   "source": [
    "# DATABASE CONNECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3440044e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:45:16,612 - production.database.connection - INFO - Database configuration loaded from /Users/raymond/Documents/Projects/factor-investing-public/config/database.yml\n",
      "2025-08-04 01:45:16,613 - production.database.connection - INFO - DatabaseManager initialized for environment: production\n",
      "2025-08-04 01:45:16,878 - production.database.connection - INFO - SQLAlchemy engine created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Database connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_db_connection():\n",
    "    \"\"\"Establishes a SQLAlchemy database engine connection.\"\"\"\n",
    "    try:\n",
    "        db_manager = get_database_manager()\n",
    "        engine = db_manager.get_engine()\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        print(f\"\\n✅ Database connection established successfully.\")\n",
    "        return engine\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FAILED to connect to the database.\")\n",
    "        print(f\"   - Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the engine for this session\n",
    "engine = create_db_connection()\n",
    "\n",
    "if engine is None:\n",
    "    raise ConnectionError(\"Database connection failed. Halting execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9778321c",
   "metadata": {},
   "source": [
    "# VALIDATED FACTORS CALCULATION CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e7888b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class ValidatedFactorsCalculator:\n",
    "    \"\"\"\n",
    "    Calculator for the three statistically validated factors:\n",
    "    1. Low-Volatility Factor (defensive momentum)\n",
    "    2. Piotroski F-Score Factor (quality assessment)\n",
    "    3. FCF Yield Factor (value enhancement)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        print(\"✅ ValidatedFactorsCalculator initialized\")\n",
    "    \n",
    "    def calculate_low_volatility_factor(self, price_data: pd.DataFrame, lookback_days: int = 252) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calculate Low-Volatility factor using inverse 252-day rolling volatility.\n",
    "        \n",
    "        Args:\n",
    "            price_data: DataFrame with 'ticker', 'date', 'close' columns\n",
    "            lookback_days: Rolling window for volatility calculation (default: 252)\n",
    "        \n",
    "        Returns:\n",
    "            Series with low-volatility scores (inverse relationship)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Pivot data for vectorized calculation\n",
    "            price_pivot = price_data.pivot(index='date', columns='ticker', values='close')\n",
    "            \n",
    "            # Calculate rolling volatility\n",
    "            volatility = price_pivot.rolling(lookback_days).std() * np.sqrt(252)\n",
    "            \n",
    "            # Apply inverse relationship (lower volatility = higher score)\n",
    "            low_vol_score = 1 / volatility\n",
    "            \n",
    "            # Stack back to long format\n",
    "            low_vol_stacked = low_vol_score.stack().reset_index()\n",
    "            low_vol_stacked.columns = ['date', 'ticker', 'low_vol_score']\n",
    "            \n",
    "            # Remove infinite values and outliers\n",
    "            low_vol_stacked = low_vol_stacked.replace([np.inf, -np.inf], np.nan)\n",
    "            low_vol_stacked = low_vol_stacked.dropna()\n",
    "            \n",
    "            # Winsorize outliers (top and bottom 1%)\n",
    "            q_low = low_vol_stacked['low_vol_score'].quantile(0.01)\n",
    "            q_high = low_vol_stacked['low_vol_score'].quantile(0.99)\n",
    "            low_vol_stacked['low_vol_score'] = low_vol_stacked['low_vol_score'].clip(q_low, q_high)\n",
    "            \n",
    "            print(f\"   ✅ Low-Volatility factor calculated: {len(low_vol_stacked):,} observations\")\n",
    "            return low_vol_stacked\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating Low-Volatility factor: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_piotroski_fscore(self, tickers: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate Piotroski F-Score with sector-specific implementations.\n",
    "        \n",
    "        Args:\n",
    "            tickers: List of tickers to analyze\n",
    "            analysis_date: Date for analysis\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with F-Scores by sector\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get sector information\n",
    "            sector_query = text(\"\"\"\n",
    "                SELECT ticker, sector\n",
    "                FROM master_info\n",
    "                WHERE ticker IN :tickers\n",
    "            \"\"\")\n",
    "            \n",
    "            ticker_list = tuple(tickers)\n",
    "            sector_df = pd.read_sql(sector_query, self.engine, params={'tickers': ticker_list})\n",
    "            \n",
    "            if sector_df.empty:\n",
    "                print(\"   ⚠️  No sector data found\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Group by sector and calculate F-Scores\n",
    "            fscore_results = []\n",
    "            \n",
    "            for sector in sector_df['sector'].unique():\n",
    "                sector_tickers = sector_df[sector_df['sector'] == sector]['ticker'].tolist()\n",
    "                \n",
    "                if sector == 'Banking':\n",
    "                    sector_fscores = self._calculate_banking_fscore(sector_tickers, analysis_date)\n",
    "                elif sector == 'Securities':\n",
    "                    sector_fscores = self._calculate_securities_fscore(sector_tickers, analysis_date)\n",
    "                else:\n",
    "                    sector_fscores = self._calculate_nonfin_fscore(sector_tickers, analysis_date)\n",
    "                \n",
    "                if not sector_fscores.empty:\n",
    "                    sector_fscores['sector'] = sector\n",
    "                    fscore_results.append(sector_fscores)\n",
    "            \n",
    "            if fscore_results:\n",
    "                combined_fscores = pd.concat(fscore_results, ignore_index=True)\n",
    "                print(f\"   ✅ Piotroski F-Score calculated: {len(combined_fscores):,} observations across {len(combined_fscores['sector'].unique())} sectors\")\n",
    "                return combined_fscores\n",
    "            else:\n",
    "                print(\"   ⚠️  No F-Score data calculated\")\n",
    "                return pd.DataFrame()\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating Piotroski F-Score: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _calculate_nonfin_fscore(self, tickers: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate F-Score for non-financial companies (9 tests).\n",
    "        \n",
    "        Tests:\n",
    "        1. ROA > 0 (NetProfit_TTM / AvgTotalAssets)\n",
    "        2. CFO > 0 (NetCFO_TTM > 0)\n",
    "        3. ΔROA > 0 (ROA improvement)\n",
    "        4. Accruals < CFO (quality of earnings)\n",
    "        5. ΔLeverage < 0 (decreasing leverage)\n",
    "        6. ΔCurrent Ratio > 0 (improving liquidity)\n",
    "        7. No new shares issued\n",
    "        8. ΔGross Margin > 0 (improving profitability)\n",
    "        9. ΔAsset Turnover > 0 (improving efficiency)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get data from intermediary_calculations_enhanced\n",
    "            query = text(\"\"\"\n",
    "                SELECT \n",
    "                    ticker,\n",
    "                    year,\n",
    "                    quarter,\n",
    "                    NetProfit_TTM,\n",
    "                    NetCFO_TTM,\n",
    "                    AvgTotalAssets,\n",
    "                    TotalDebt,\n",
    "                    CurrentAssets,\n",
    "                    CurrentLiabilities,\n",
    "                    TotalEquity,\n",
    "                    Revenue_TTM,\n",
    "                    GrossProfit_TTM,\n",
    "                    SharesOutstanding\n",
    "                FROM intermediary_calculations_enhanced\n",
    "                WHERE ticker IN :tickers\n",
    "                AND year >= YEAR(:analysis_date) - 2\n",
    "                ORDER BY ticker, year, quarter\n",
    "            \"\"\")\n",
    "            \n",
    "            data = pd.read_sql(query, self.engine, \n",
    "                             params={'tickers': tuple(tickers), 'analysis_date': analysis_date})\n",
    "            \n",
    "            if data.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Calculate metrics and tests\n",
    "            results = []\n",
    "            \n",
    "            for ticker in data['ticker'].unique():\n",
    "                ticker_data = data[data['ticker'] == ticker].sort_values(['year', 'quarter'])\n",
    "                \n",
    "                if len(ticker_data) < 2:  # Need at least 2 periods for changes\n",
    "                    continue\n",
    "                \n",
    "                # Get current and previous period\n",
    "                current = ticker_data.iloc[-1]\n",
    "                previous = ticker_data.iloc[-2]\n",
    "                \n",
    "                # Test 1: ROA > 0\n",
    "                roa_current = current['NetProfit_TTM'] / current['AvgTotalAssets'] if current['AvgTotalAssets'] > 0 else 0\n",
    "                test1 = 1 if roa_current > 0 else 0\n",
    "                \n",
    "                # Test 2: CFO > 0\n",
    "                test2 = 1 if current['NetCFO_TTM'] > 0 else 0\n",
    "                \n",
    "                # Test 3: ΔROA > 0\n",
    "                roa_previous = previous['NetProfit_TTM'] / previous['AvgTotalAssets'] if previous['AvgTotalAssets'] > 0 else 0\n",
    "                test3 = 1 if roa_current > roa_previous else 0\n",
    "                \n",
    "                # Test 4: Accruals < CFO (simplified: NetProfit < CFO)\n",
    "                test4 = 1 if current['NetProfit_TTM'] < current['NetCFO_TTM'] else 0\n",
    "                \n",
    "                # Test 5: ΔLeverage < 0 (decreasing debt/equity)\n",
    "                leverage_current = current['TotalDebt'] / current['TotalEquity'] if current['TotalEquity'] > 0 else 0\n",
    "                leverage_previous = previous['TotalDebt'] / previous['TotalEquity'] if previous['TotalEquity'] > 0 else 0\n",
    "                test5 = 1 if leverage_current < leverage_previous else 0\n",
    "                \n",
    "                # Test 6: ΔCurrent Ratio > 0\n",
    "                cr_current = current['CurrentAssets'] / current['CurrentLiabilities'] if current['CurrentLiabilities'] > 0 else 0\n",
    "                cr_previous = previous['CurrentAssets'] / previous['CurrentLiabilities'] if previous['CurrentLiabilities'] > 0 else 0\n",
    "                test6 = 1 if cr_current > cr_previous else 0\n",
    "                \n",
    "                # Test 7: No new shares issued (simplified: shares unchanged or decreased)\n",
    "                test7 = 1 if current['SharesOutstanding'] <= previous['SharesOutstanding'] else 0\n",
    "                \n",
    "                # Test 8: ΔGross Margin > 0\n",
    "                gm_current = current['GrossProfit_TTM'] / current['Revenue_TTM'] if current['Revenue_TTM'] > 0 else 0\n",
    "                gm_previous = previous['GrossProfit_TTM'] / previous['Revenue_TTM'] if previous['Revenue_TTM'] > 0 else 0\n",
    "                test8 = 1 if gm_current > gm_previous else 0\n",
    "                \n",
    "                # Test 9: ΔAsset Turnover > 0\n",
    "                at_current = current['Revenue_TTM'] / current['AvgTotalAssets'] if current['AvgTotalAssets'] > 0 else 0\n",
    "                at_previous = previous['Revenue_TTM'] / previous['AvgTotalAssets'] if previous['AvgTotalAssets'] > 0 else 0\n",
    "                test9 = 1 if at_current > at_previous else 0\n",
    "                \n",
    "                # Calculate total F-Score\n",
    "                fscore = test1 + test2 + test3 + test4 + test5 + test6 + test7 + test8 + test9\n",
    "                \n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'fscore': fscore,\n",
    "                    'test1_roa': test1,\n",
    "                    'test2_cfo': test2,\n",
    "                    'test3_delta_roa': test3,\n",
    "                    'test4_accruals': test4,\n",
    "                    'test5_delta_leverage': test5,\n",
    "                    'test6_delta_current_ratio': test6,\n",
    "                    'test7_no_new_shares': test7,\n",
    "                    'test8_delta_gross_margin': test8,\n",
    "                    'test9_delta_asset_turnover': test9\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating non-financial F-Score: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _calculate_banking_fscore(self, tickers: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate F-Score for banking companies (9 tests).\n",
    "        \n",
    "        Tests:\n",
    "        1. NIM > 0 (Net Interest Margin)\n",
    "        2. ROA > 0 (NetProfit_TTM / AvgTotalAssets)\n",
    "        3. ΔROA > 0 (ROA improvement)\n",
    "        4. ΔNIM > 0 (NIM improvement)\n",
    "        5. ΔEfficiency Ratio < 0 (improving efficiency)\n",
    "        6. ΔCapital Adequacy > 0 (improving capital)\n",
    "        7. No new shares issued\n",
    "        8. ΔRevenue Growth > 0\n",
    "        9. ΔAsset Quality > 0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get data from intermediary_calculations_banking\n",
    "            query = text(\"\"\"\n",
    "                SELECT \n",
    "                    ticker,\n",
    "                    year,\n",
    "                    quarter,\n",
    "                    NetProfit_TTM,\n",
    "                    AvgTotalAssets,\n",
    "                    NetInterestIncome_TTM,\n",
    "                    AvgInterestEarningAssets,\n",
    "                    OperatingExpenses_TTM,\n",
    "                    Revenue_TTM,\n",
    "                    TotalEquity,\n",
    "                    NonPerformingLoans,\n",
    "                    TotalLoans,\n",
    "                    SharesOutstanding\n",
    "                FROM intermediary_calculations_banking\n",
    "                WHERE ticker IN :tickers\n",
    "                AND year >= YEAR(:analysis_date) - 2\n",
    "                ORDER BY ticker, year, quarter\n",
    "            \"\"\")\n",
    "            \n",
    "            data = pd.read_sql(query, self.engine, \n",
    "                             params={'tickers': tuple(tickers), 'analysis_date': analysis_date})\n",
    "            \n",
    "            if data.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Calculate metrics and tests\n",
    "            results = []\n",
    "            \n",
    "            for ticker in data['ticker'].unique():\n",
    "                ticker_data = data[data['ticker'] == ticker].sort_values(['year', 'quarter'])\n",
    "                \n",
    "                if len(ticker_data) < 2:  # Need at least 2 periods for changes\n",
    "                    continue\n",
    "                \n",
    "                # Get current and previous period\n",
    "                current = ticker_data.iloc[-1]\n",
    "                previous = ticker_data.iloc[-2]\n",
    "                \n",
    "                # Test 1: NIM > 0\n",
    "                nim_current = current['NetInterestIncome_TTM'] / current['AvgInterestEarningAssets'] if current['AvgInterestEarningAssets'] > 0 else 0\n",
    "                test1 = 1 if nim_current > 0 else 0\n",
    "                \n",
    "                # Test 2: ROA > 0\n",
    "                roa_current = current['NetProfit_TTM'] / current['AvgTotalAssets'] if current['AvgTotalAssets'] > 0 else 0\n",
    "                test2 = 1 if roa_current > 0 else 0\n",
    "                \n",
    "                # Test 3: ΔROA > 0\n",
    "                roa_previous = previous['NetProfit_TTM'] / previous['AvgTotalAssets'] if previous['AvgTotalAssets'] > 0 else 0\n",
    "                test3 = 1 if roa_current > roa_previous else 0\n",
    "                \n",
    "                # Test 4: ΔNIM > 0\n",
    "                nim_previous = previous['NetInterestIncome_TTM'] / previous['AvgInterestEarningAssets'] if previous['AvgInterestEarningAssets'] > 0 else 0\n",
    "                test4 = 1 if nim_current > nim_previous else 0\n",
    "                \n",
    "                # Test 5: ΔEfficiency Ratio < 0 (improving efficiency)\n",
    "                eff_current = current['OperatingExpenses_TTM'] / current['Revenue_TTM'] if current['Revenue_TTM'] > 0 else 0\n",
    "                eff_previous = previous['OperatingExpenses_TTM'] / previous['Revenue_TTM'] if previous['Revenue_TTM'] > 0 else 0\n",
    "                test5 = 1 if eff_current < eff_previous else 0\n",
    "                \n",
    "                # Test 6: ΔCapital Adequacy > 0 (improving capital ratio)\n",
    "                cap_current = current['TotalEquity'] / current['AvgTotalAssets'] if current['AvgTotalAssets'] > 0 else 0\n",
    "                cap_previous = previous['TotalEquity'] / previous['AvgTotalAssets'] if previous['AvgTotalAssets'] > 0 else 0\n",
    "                test6 = 1 if cap_current > cap_previous else 0\n",
    "                \n",
    "                # Test 7: No new shares issued\n",
    "                test7 = 1 if current['SharesOutstanding'] <= previous['SharesOutstanding'] else 0\n",
    "                \n",
    "                # Test 8: ΔRevenue Growth > 0\n",
    "                test8 = 1 if current['Revenue_TTM'] > previous['Revenue_TTM'] else 0\n",
    "                \n",
    "                # Test 9: ΔAsset Quality > 0 (decreasing NPL ratio)\n",
    "                npl_current = current['NonPerformingLoans'] / current['TotalLoans'] if current['TotalLoans'] > 0 else 0\n",
    "                npl_previous = previous['NonPerformingLoans'] / previous['TotalLoans'] if previous['TotalLoans'] > 0 else 0\n",
    "                test9 = 1 if npl_current < npl_previous else 0\n",
    "                \n",
    "                # Calculate total F-Score\n",
    "                fscore = test1 + test2 + test3 + test4 + test5 + test6 + test7 + test8 + test9\n",
    "                \n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'fscore': fscore,\n",
    "                    'test1_nim': test1,\n",
    "                    'test2_roa': test2,\n",
    "                    'test3_delta_roa': test3,\n",
    "                    'test4_delta_nim': test4,\n",
    "                    'test5_delta_efficiency': test5,\n",
    "                    'test6_delta_capital': test6,\n",
    "                    'test7_no_new_shares': test7,\n",
    "                    'test8_revenue_growth': test8,\n",
    "                    'test9_asset_quality': test9\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating banking F-Score: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _calculate_securities_fscore(self, tickers: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate F-Score for securities companies (9 tests).\n",
    "        \n",
    "        Tests:\n",
    "        1. Trading Income > 0 (NetTradingIncome_TTM)\n",
    "        2. Brokerage Revenue > 0 (BrokerageRevenue_TTM)\n",
    "        3. ΔTrading Income > 0\n",
    "        4. ΔBrokerage Revenue > 0\n",
    "        5. ΔEfficiency Ratio < 0 (improving efficiency)\n",
    "        6. ΔCapital Adequacy > 0 (improving capital)\n",
    "        7. No new shares issued\n",
    "        8. ΔRevenue Growth > 0\n",
    "        9. ΔAsset Quality > 0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get data from intermediary_calculations_securities\n",
    "            query = text(\"\"\"\n",
    "                SELECT \n",
    "                    ticker,\n",
    "                    year,\n",
    "                    quarter,\n",
    "                    NetTradingIncome_TTM,\n",
    "                    BrokerageRevenue_TTM,\n",
    "                    OperatingExpenses_TTM,\n",
    "                    Revenue_TTM,\n",
    "                    TotalEquity,\n",
    "                    AvgTotalAssets,\n",
    "                    SharesOutstanding\n",
    "                FROM intermediary_calculations_securities\n",
    "                WHERE ticker IN :tickers\n",
    "                AND year >= YEAR(:analysis_date) - 2\n",
    "                ORDER BY ticker, year, quarter\n",
    "            \"\"\")\n",
    "            \n",
    "            data = pd.read_sql(query, self.engine, \n",
    "                             params={'tickers': tuple(tickers), 'analysis_date': analysis_date})\n",
    "            \n",
    "            if data.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Calculate metrics and tests\n",
    "            results = []\n",
    "            \n",
    "            for ticker in data['ticker'].unique():\n",
    "                ticker_data = data[data['ticker'] == ticker].sort_values(['year', 'quarter'])\n",
    "                \n",
    "                if len(ticker_data) < 2:  # Need at least 2 periods for changes\n",
    "                    continue\n",
    "                \n",
    "                # Get current and previous period\n",
    "                current = ticker_data.iloc[-1]\n",
    "                previous = ticker_data.iloc[-2]\n",
    "                \n",
    "                # Test 1: Trading Income > 0\n",
    "                test1 = 1 if current['NetTradingIncome_TTM'] > 0 else 0\n",
    "                \n",
    "                # Test 2: Brokerage Revenue > 0\n",
    "                test2 = 1 if current['BrokerageRevenue_TTM'] > 0 else 0\n",
    "                \n",
    "                # Test 3: ΔTrading Income > 0\n",
    "                test3 = 1 if current['NetTradingIncome_TTM'] > previous['NetTradingIncome_TTM'] else 0\n",
    "                \n",
    "                # Test 4: ΔBrokerage Revenue > 0\n",
    "                test4 = 1 if current['BrokerageRevenue_TTM'] > previous['BrokerageRevenue_TTM'] else 0\n",
    "                \n",
    "                # Test 5: ΔEfficiency Ratio < 0 (improving efficiency)\n",
    "                eff_current = current['OperatingExpenses_TTM'] / current['Revenue_TTM'] if current['Revenue_TTM'] > 0 else 0\n",
    "                eff_previous = previous['OperatingExpenses_TTM'] / previous['Revenue_TTM'] if previous['Revenue_TTM'] > 0 else 0\n",
    "                test5 = 1 if eff_current < eff_previous else 0\n",
    "                \n",
    "                # Test 6: ΔCapital Adequacy > 0 (improving capital ratio)\n",
    "                cap_current = current['TotalEquity'] / current['AvgTotalAssets'] if current['AvgTotalAssets'] > 0 else 0\n",
    "                cap_previous = previous['TotalEquity'] / previous['AvgTotalAssets'] if previous['AvgTotalAssets'] > 0 else 0\n",
    "                test6 = 1 if cap_current > cap_previous else 0\n",
    "                \n",
    "                # Test 7: No new shares issued\n",
    "                test7 = 1 if current['SharesOutstanding'] <= previous['SharesOutstanding'] else 0\n",
    "                \n",
    "                # Test 8: ΔRevenue Growth > 0\n",
    "                test8 = 1 if current['Revenue_TTM'] > previous['Revenue_TTM'] else 0\n",
    "                \n",
    "                # Test 9: ΔAsset Quality > 0 (simplified: improving ROA)\n",
    "                roa_current = current['NetTradingIncome_TTM'] / current['AvgTotalAssets'] if current['AvgTotalAssets'] > 0 else 0\n",
    "                roa_previous = previous['NetTradingIncome_TTM'] / previous['AvgTotalAssets'] if previous['AvgTotalAssets'] > 0 else 0\n",
    "                test9 = 1 if roa_current > roa_previous else 0\n",
    "                \n",
    "                # Calculate total F-Score\n",
    "                fscore = test1 + test2 + test3 + test4 + test5 + test6 + test7 + test8 + test9\n",
    "                \n",
    "                results.append({\n",
    "                    'ticker': ticker,\n",
    "                    'fscore': fscore,\n",
    "                    'test1_trading_income': test1,\n",
    "                    'test2_brokerage_revenue': test2,\n",
    "                    'test3_delta_trading': test3,\n",
    "                    'test4_delta_brokerage': test4,\n",
    "                    'test5_delta_efficiency': test5,\n",
    "                    'test6_delta_capital': test6,\n",
    "                    'test7_no_new_shares': test7,\n",
    "                    'test8_revenue_growth': test8,\n",
    "                    'test9_asset_quality': test9\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating securities F-Score: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def calculate_fcf_yield(self, tickers: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate FCF Yield factor with imputation handling.\n",
    "        \n",
    "        Args:\n",
    "            tickers: List of tickers to analyze\n",
    "            analysis_date: Date for analysis\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with FCF Yield scores\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get fundamental data for FCF calculation\n",
    "            fundamental_query = text(\"\"\"\n",
    "                WITH fundamental_data AS (\n",
    "                    SELECT \n",
    "                        fv.ticker,\n",
    "                        fv.year,\n",
    "                        fv.quarter,\n",
    "                        fv.item_id,\n",
    "                        fv.statement_type,\n",
    "                        SUM(fv.value / 1e9) as value_bn\n",
    "                    FROM fundamental_values fv\n",
    "                    WHERE fv.ticker IN :tickers\n",
    "                    AND fv.item_id IN (1, 2, 3)  -- NetProfit, TotalAssets, CapEx\n",
    "                    AND fv.year >= YEAR(:analysis_date) - 2\n",
    "                    GROUP BY fv.ticker, fv.year, fv.quarter, fv.item_id, fv.statement_type\n",
    "                ),\n",
    "                netprofit_ttm AS (\n",
    "                    SELECT ticker, year, quarter, value_bn as netprofit_ttm\n",
    "                    FROM fundamental_data\n",
    "                    WHERE item_id = 1 AND statement_type = 'PL'\n",
    "                ),\n",
    "                totalassets_ttm AS (\n",
    "                    SELECT ticker, year, quarter, value_bn as totalassets_ttm\n",
    "                    FROM fundamental_data\n",
    "                    WHERE item_id = 2 AND statement_type = 'BS'\n",
    "                ),\n",
    "                capex_ttm AS (\n",
    "                    SELECT ticker, year, quarter, value_bn as capex_ttm\n",
    "                    FROM fundamental_data\n",
    "                    WHERE item_id = 3 AND statement_type = 'CF'\n",
    "                )\n",
    "                SELECT \n",
    "                    np.ticker,\n",
    "                    np.year,\n",
    "                    np.quarter,\n",
    "                    np.netprofit_ttm,\n",
    "                    ta.totalassets_ttm,\n",
    "                    cx.capex_ttm\n",
    "                FROM netprofit_ttm np\n",
    "                LEFT JOIN totalassets_ttm ta ON np.ticker = ta.ticker AND np.year = ta.year AND np.quarter = ta.quarter\n",
    "                LEFT JOIN capex_ttm cx ON np.ticker = cx.ticker AND np.year = cx.year AND np.quarter = cx.quarter\n",
    "                WHERE np.netprofit_ttm > 0\n",
    "                AND ta.totalassets_ttm > 0\n",
    "            \"\"\")\n",
    "            \n",
    "            fundamental_data = pd.read_sql(fundamental_query, self.engine,\n",
    "                                         params={'tickers': tuple(tickers), 'analysis_date': analysis_date})\n",
    "            \n",
    "            if fundamental_data.empty:\n",
    "                print(\"   ⚠️  No fundamental data found for FCF calculation\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Get market cap data\n",
    "            market_cap_query = text(\"\"\"\n",
    "                SELECT ticker, market_cap\n",
    "                FROM vcsc_daily_data_complete\n",
    "                WHERE ticker IN :tickers\n",
    "                AND trading_date = :analysis_date\n",
    "            \"\"\")\n",
    "            \n",
    "            market_cap_data = pd.read_sql(market_cap_query, self.engine,\n",
    "                                        params={'tickers': tuple(tickers), 'analysis_date': analysis_date})\n",
    "            \n",
    "            if market_cap_data.empty:\n",
    "                print(\"   ⚠️  No market cap data found\")\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Calculate FCF and FCF Yield\n",
    "            fundamental_data = fundamental_data.merge(market_cap_data, on='ticker', how='inner')\n",
    "            \n",
    "            # Impute missing CapEx (conservative estimate: -5% of NetCFO)\n",
    "            imputation_rate = 0.0\n",
    "            if 'capex_ttm' in fundamental_data.columns:\n",
    "                missing_capex = fundamental_data['capex_ttm'].isna().sum()\n",
    "                total_obs = len(fundamental_data)\n",
    "                imputation_rate = missing_capex / total_obs if total_obs > 0 else 0\n",
    "                \n",
    "                # Impute with conservative estimate\n",
    "                fundamental_data['capex_ttm'] = fundamental_data['capex_ttm'].fillna(\n",
    "                    -0.05 * fundamental_data['netprofit_ttm']\n",
    "                )\n",
    "            \n",
    "            # Calculate FCF (simplified: NetProfit - CapEx)\n",
    "            fundamental_data['fcf'] = fundamental_data['netprofit_ttm'] - fundamental_data['capex_ttm']\n",
    "            \n",
    "            # Calculate FCF Yield\n",
    "            fundamental_data['fcf_yield'] = fundamental_data['fcf'] / fundamental_data['market_cap']\n",
    "            \n",
    "            # Clean and filter\n",
    "            fcf_results = fundamental_data[['ticker', 'fcf', 'fcf_yield']].copy()\n",
    "            fcf_results = fcf_results.dropna()\n",
    "            fcf_results = fcf_results[fcf_results['fcf_yield'] > 0]  # Positive FCF Yield only\n",
    "            \n",
    "            # Winsorize outliers\n",
    "            q_low = fcf_results['fcf_yield'].quantile(0.01)\n",
    "            q_high = fcf_results['fcf_yield'].quantile(0.99)\n",
    "            fcf_results['fcf_yield'] = fcf_results['fcf_yield'].clip(q_low, q_high)\n",
    "            \n",
    "            print(f\"   ✅ FCF Yield calculated: {len(fcf_results):,} observations (imputation rate: {imputation_rate:.2%})\")\n",
    "            return fcf_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating FCF Yield: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "class SectorAwareFactorCalculator:\n",
    "    \"\"\"\n",
    "    Sector-aware factor calculator with quality-adjusted P/E and validated factors integration.\n",
    "    \"\"\"\n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "        self.validated_calculator = ValidatedFactorsCalculator(engine)\n",
    "    \n",
    "    def calculate_sector_aware_pe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate quality-adjusted P/E by sector.\"\"\"\n",
    "        if 'roaa' not in data.columns or 'sector' not in data.columns:\n",
    "            return data\n",
    "        \n",
    "        # Create ROAA quintiles within each sector\n",
    "        def safe_qcut(x):\n",
    "            try:\n",
    "                if len(x) < 5:\n",
    "                    return pd.Series(['Q3'] * len(x), index=x.index)\n",
    "                return pd.qcut(x, 5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'], duplicates='drop')\n",
    "            except ValueError:\n",
    "                return pd.Series(['Q3'] * len(x), index=x.index)\n",
    "        \n",
    "        data['roaa_quintile'] = data.groupby('sector')['roaa'].transform(safe_qcut)\n",
    "        \n",
    "        # Fill missing quintiles with Q3\n",
    "        data['roaa_quintile'] = data['roaa_quintile'].fillna('Q3')\n",
    "        \n",
    "        # Quality-adjusted P/E weights (higher quality = higher weight)\n",
    "        quality_weights = {\n",
    "            'Q1': 0.5,  # Low quality\n",
    "            'Q2': 0.7,\n",
    "            'Q3': 1.0,  # Medium quality\n",
    "            'Q4': 1.3,\n",
    "            'Q5': 1.5   # High quality\n",
    "        }\n",
    "        \n",
    "        data['quality_adjusted_pe'] = data['roaa_quintile'].map(quality_weights)\n",
    "        return data\n",
    "    \n",
    "    def calculate_momentum_score(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate multi-horizon momentum score with correct signal directions.\"\"\"\n",
    "        momentum_columns = [col for col in data.columns if col.startswith('momentum_')]\n",
    "        \n",
    "        if not momentum_columns:\n",
    "            return data\n",
    "        \n",
    "        # Apply correct signal directions:\n",
    "        # - 3M and 6M: Positive signals (higher is better)\n",
    "        # - 1M and 12M: Contrarian signals (lower is better)\n",
    "        momentum_score = 0.0\n",
    "        \n",
    "        for col in momentum_columns:\n",
    "            if 'momentum_63d' in col or 'momentum_126d' in col:  # 3M and 6M - positive\n",
    "                momentum_score += data[col]\n",
    "            elif 'momentum_21d' in col or 'momentum_252d' in col:  # 1M and 12M - contrarian\n",
    "                momentum_score -= data[col]  # Negative for contrarian\n",
    "        \n",
    "        # Equal weight the components\n",
    "        data['momentum_score'] = momentum_score / len(momentum_columns)\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49683c6f",
   "metadata": {},
   "source": [
    "# CORE CLASSES AND ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd233b3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class RegimeDetector:\n",
    "    \"\"\"\n",
    "    Simple regime detection based on volatility and return thresholds.\n",
    "    FIXED: Now properly accepts and uses threshold parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, lookback_period: int = 90, volatility_threshold: float = 0.0140, \n",
    "                 return_threshold: float = 0.0012, low_return_threshold: float = 0.0002):\n",
    "        self.lookback_period = lookback_period\n",
    "        self.volatility_threshold = volatility_threshold\n",
    "        self.return_threshold = return_threshold\n",
    "        self.low_return_threshold = low_return_threshold\n",
    "        print(f\"✅ RegimeDetector initialized with thresholds:\")\n",
    "        print(f\"   - Volatility: {self.volatility_threshold:.2%}\")\n",
    "        print(f\"   - Return: {self.return_threshold:.2%}\")\n",
    "        print(f\"   - Low Return: {self.low_return_threshold:.2%}\")\n",
    "    \n",
    "    def detect_regime(self, price_data: pd.DataFrame) -> str:\n",
    "        \"\"\"Detect market regime based on volatility and return.\"\"\"\n",
    "        # Use available data, but require at least 60 days\n",
    "        min_required_days = 60\n",
    "        if len(price_data) < min_required_days:\n",
    "            print(f\"   ⚠️  Insufficient data: {len(price_data)} < {min_required_days}\")\n",
    "            return 'Sideways'\n",
    "        \n",
    "        # Use all available data up to lookback_period\n",
    "        available_days = min(len(price_data), self.lookback_period)\n",
    "        recent_data = price_data.tail(available_days)\n",
    "        returns = recent_data['close'].pct_change().dropna()\n",
    "        \n",
    "        volatility = returns.std()\n",
    "        avg_return = returns.mean()\n",
    "        \n",
    "        # Debug output\n",
    "        print(f\"   🔍 Regime Debug: Vol={volatility:.4f} ({volatility:.2%}), AvgRet={avg_return:.4f} ({avg_return:.2%})\")\n",
    "        print(f\"   🔍 Thresholds: VolThresh={self.volatility_threshold:.4f}, RetThresh={self.return_threshold:.4f}, LowRetThresh={self.low_return_threshold:.4f}\")\n",
    "        \n",
    "        if volatility > self.volatility_threshold:\n",
    "            if avg_return > self.return_threshold:\n",
    "                print(f\"   📈 Detected: Bull (Vol={volatility:.2%} > {self.volatility_threshold:.2%}, Ret={avg_return:.2%} > {self.return_threshold:.2%})\")\n",
    "                return 'Bull'\n",
    "            else:\n",
    "                print(f\"   📉 Detected: Bear (Vol={volatility:.2%} > {self.volatility_threshold:.2%}, Ret={avg_return:.2%} <= {self.return_threshold:.2%})\")\n",
    "                return 'Bear'\n",
    "        else:\n",
    "            if abs(avg_return) < self.low_return_threshold:\n",
    "                print(f\"   ↔️  Detected: Sideways (Vol={volatility:.2%} <= {self.volatility_threshold:.2%}, |Ret|={abs(avg_return):.2%} < {self.low_return_threshold:.2%})\")\n",
    "                return 'Sideways'\n",
    "            else:\n",
    "                print(f\"   ⚠️  Detected: Stress (Vol={volatility:.2%} <= {self.volatility_threshold:.2%}, |Ret|={abs(avg_return):.2%} >= {self.low_return_threshold:.2%})\")\n",
    "                return 'Stress'\n",
    "    \n",
    "    def get_regime_allocation(self, regime: str) -> float:\n",
    "        \"\"\"Get target allocation based on regime.\"\"\"\n",
    "        regime_allocations = {\n",
    "            'Bull': 1.0,      # Fully invested\n",
    "            'Bear': 0.8,      # 80% invested\n",
    "            'Sideways': 0.6,  # 60% invested\n",
    "            'Stress': 0.4     # 40% invested\n",
    "        }\n",
    "        return regime_allocations.get(regime, 0.6)\n",
    "\n",
    "## QVM ENGINE V3J WITH VALIDATED FACTORS\n",
    "\n",
    "class QVMEngineV3jValidatedFactors:\n",
    "    \"\"\"\n",
    "    QVM Engine v3j with Validated Factors (All Components).\n",
    "    Uses pre-computed data and vectorized operations for dramatically faster rebalancing.\n",
    "    Implements the three statistically validated factors:\n",
    "    - Value factors (P/E + FCF Yield)\n",
    "    - Quality factors (ROAA + Piotroski F-Score)\n",
    "    - Momentum factors (Multi-horizon + Low-Volatility)\n",
    "    \"\"\"\n",
    "    def __init__(self, config: dict, price_data: pd.DataFrame, fundamental_data: pd.DataFrame,\n",
    "                 returns_matrix: pd.DataFrame, benchmark_returns: pd.Series, db_engine, precomputed_data: dict):\n",
    "        \n",
    "        self.config = config\n",
    "        self.engine = db_engine\n",
    "        self.precomputed_data = precomputed_data\n",
    "        \n",
    "        # Slice data to the exact backtest window\n",
    "        start = pd.Timestamp(config['backtest_start_date'])\n",
    "        end = pd.Timestamp(config['backtest_end_date'])\n",
    "        \n",
    "        self.price_data_raw = price_data[price_data['date'].between(start, end)].copy()\n",
    "        self.fundamental_data_raw = fundamental_data[fundamental_data['date'].between(start, end)].copy()\n",
    "        self.daily_returns_matrix = returns_matrix.loc[start:end].copy()\n",
    "        self.benchmark_returns = benchmark_returns.loc[start:end].copy()\n",
    "        \n",
    "        # Initialize components\n",
    "        self.regime_detector = RegimeDetector(\n",
    "            lookback_period=config['regime']['lookback_period'],\n",
    "            volatility_threshold=config['regime']['volatility_threshold'],\n",
    "            return_threshold=config['regime']['return_threshold'],\n",
    "            low_return_threshold=config['regime']['low_return_threshold']\n",
    "        )\n",
    "        self.sector_calculator = SectorAwareFactorCalculator(db_engine)\n",
    "        self.validated_calculator = ValidatedFactorsCalculator(db_engine)\n",
    "        self.mapping_manager = FinancialMappingManager()\n",
    "        \n",
    "        # Pre-process precomputed data for faster access\n",
    "        self._setup_precomputed_data()\n",
    "        \n",
    "        print(\"✅ QVMEngineV3jValidatedFactors initialized.\")\n",
    "        print(f\"   - Strategy: {config['strategy_name']}\")\n",
    "        print(f\"   - Period: {self.daily_returns_matrix.index.min().date()} to {self.daily_returns_matrix.index.max().date()}\")\n",
    "        print(f\"   - Value Factors: P/E + FCF Yield (33% weight)\")\n",
    "        print(f\"   - Quality Factors: ROAA + Piotroski F-Score (33% weight)\")\n",
    "        print(f\"   - Momentum Factors: Multi-horizon + Low-Volatility (34% weight)\")\n",
    "        print(f\"   - Performance: Pre-computed data + Vectorized operations\")\n",
    "\n",
    "    def _setup_precomputed_data(self):\n",
    "        \"\"\"Setup precomputed data for fast access during rebalancing.\"\"\"\n",
    "        # Create fast lookup structures\n",
    "        self.universe_lookup = self.precomputed_data['universe'].set_index(['trading_date', 'ticker']).index\n",
    "        self.fundamental_lookup = self.precomputed_data['fundamentals'].set_index(['date', 'ticker'])\n",
    "        self.momentum_lookup = self.precomputed_data['momentum'].set_index(['trading_date', 'ticker'])\n",
    "        \n",
    "        print(\"   ✅ Pre-computed data indexed for fast access\")\n",
    "\n",
    "    def run_backtest(self) -> (pd.Series, pd.DataFrame):\n",
    "        \"\"\"Executes the full backtesting pipeline with optimized performance.\"\"\"\n",
    "        print(\"\\n🚀 Starting QVM Engine v3j validated factors backtest execution...\")\n",
    "        \n",
    "        rebalance_dates = self._generate_rebalance_dates()\n",
    "        daily_holdings, diagnostics = self._run_optimized_backtesting_loop(rebalance_dates)\n",
    "        net_returns = self._calculate_net_returns(daily_holdings)\n",
    "        \n",
    "        print(\"✅ QVM Engine v3j validated factors backtest execution complete.\")\n",
    "        return net_returns, diagnostics\n",
    "\n",
    "    def _generate_rebalance_dates(self) -> list:\n",
    "        \"\"\"Generates monthly rebalance dates based on actual trading days.\"\"\"\n",
    "        all_trading_dates = self.daily_returns_matrix.index\n",
    "        rebal_dates_calendar = pd.date_range(\n",
    "            start=self.config['backtest_start_date'],\n",
    "            end=self.config['backtest_end_date'],\n",
    "            freq=self.config['rebalance_frequency']\n",
    "        )\n",
    "        actual_rebal_dates = [all_trading_dates[all_trading_dates.searchsorted(d, side='left')-1] for d in rebal_dates_calendar if d >= all_trading_dates.min()]\n",
    "        print(f\"   - Generated {len(actual_rebal_dates)} monthly rebalance dates.\")\n",
    "        return sorted(list(set(actual_rebal_dates)))\n",
    "\n",
    "    def _run_optimized_backtesting_loop(self, rebalance_dates: list) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"Optimized backtesting loop using pre-computed data and validated factors.\"\"\"\n",
    "        daily_holdings = pd.DataFrame(0.0, index=self.daily_returns_matrix.index, columns=self.daily_returns_matrix.columns)\n",
    "        diagnostics_log = []\n",
    "        \n",
    "        for i, rebal_date in enumerate(rebalance_dates):\n",
    "            print(f\"   - Processing rebalance {i+1}/{len(rebalance_dates)}: {rebal_date.date()}...\", end=\"\")\n",
    "            \n",
    "            # Fast universe lookup (no database query)\n",
    "            universe = self._get_universe_from_precomputed(rebal_date)\n",
    "            if len(universe) < 5:\n",
    "                print(\" ⚠️ Universe too small. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Detect regime\n",
    "            regime = self._detect_current_regime(rebal_date)\n",
    "            regime_allocation = self.regime_detector.get_regime_allocation(regime)\n",
    "            \n",
    "            # Fast factor calculation with validated factors (no database queries)\n",
    "            factors_df = self._get_validated_factors_from_precomputed(universe, rebal_date)\n",
    "            if factors_df.empty:\n",
    "                print(\" ⚠️ No factor data. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Apply entry criteria\n",
    "            qualified_df = self._apply_entry_criteria(factors_df)\n",
    "            if qualified_df.empty:\n",
    "                print(\" ⚠️ No qualified stocks. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Construct portfolio\n",
    "            target_portfolio = self._construct_portfolio(qualified_df, regime_allocation)\n",
    "            if target_portfolio.empty:\n",
    "                print(\" ⚠️ Portfolio empty. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Apply holdings\n",
    "            start_period = rebal_date + pd.Timedelta(days=1)\n",
    "            end_period = rebalance_dates[i+1] if i + 1 < len(rebalance_dates) else self.daily_returns_matrix.index.max()\n",
    "            holding_dates = self.daily_returns_matrix.index[(self.daily_returns_matrix.index >= start_period) & (self.daily_returns_matrix.index <= end_period)]\n",
    "            \n",
    "            daily_holdings.loc[holding_dates] = 0.0\n",
    "            valid_tickers = target_portfolio.index.intersection(daily_holdings.columns)\n",
    "            daily_holdings.loc[holding_dates, valid_tickers] = target_portfolio[valid_tickers].values\n",
    "            \n",
    "            # Calculate turnover\n",
    "            if i > 0:\n",
    "                try:\n",
    "                    prev_holdings_idx = self.daily_returns_matrix.index.get_loc(rebal_date) - 1\n",
    "                except KeyError:\n",
    "                    prev_dates = self.daily_returns_matrix.index[self.daily_returns_matrix.index < rebal_date]\n",
    "                    if len(prev_dates) > 0:\n",
    "                        prev_holdings_idx = self.daily_returns_matrix.index.get_loc(prev_dates[-1])\n",
    "                    else:\n",
    "                        prev_holdings_idx = -1\n",
    "                \n",
    "                prev_holdings = daily_holdings.iloc[prev_holdings_idx] if prev_holdings_idx >= 0 else pd.Series(dtype='float64')\n",
    "            else:\n",
    "                prev_holdings = pd.Series(dtype='float64')\n",
    "\n",
    "            turnover = (target_portfolio - prev_holdings.reindex(target_portfolio.index).fillna(0)).abs().sum() / 2.0\n",
    "            \n",
    "            diagnostics_log.append({\n",
    "                'date': rebal_date,\n",
    "                'universe_size': len(universe),\n",
    "                'portfolio_size': len(target_portfolio),\n",
    "                'regime': regime,\n",
    "                'regime_allocation': regime_allocation,\n",
    "                'turnover': turnover\n",
    "            })\n",
    "            print(f\" ✅ Universe: {len(universe)}, Portfolio: {len(target_portfolio)}, Regime: {regime}, Turnover: {turnover:.2%}\")\n",
    "\n",
    "        if diagnostics_log:\n",
    "            return daily_holdings, pd.DataFrame(diagnostics_log).set_index('date')\n",
    "        else:\n",
    "            return daily_holdings, pd.DataFrame()\n",
    "\n",
    "    def _get_universe_from_precomputed(self, analysis_date: pd.Timestamp) -> list:\n",
    "        \"\"\"Get universe from pre-computed data (no database query).\"\"\"\n",
    "        # Filter precomputed universe data for the analysis date\n",
    "        universe_data = self.precomputed_data['universe']\n",
    "        date_universe = universe_data[universe_data['trading_date'] == analysis_date]\n",
    "        return date_universe['ticker'].tolist()\n",
    "\n",
    "    def _detect_current_regime(self, analysis_date: pd.Timestamp) -> str:\n",
    "        \"\"\"Detect current market regime.\"\"\"\n",
    "        lookback_days = self.config['regime']['lookback_period']\n",
    "        start_date = analysis_date - pd.Timedelta(days=lookback_days)\n",
    "        \n",
    "        benchmark_data = self.benchmark_returns.loc[start_date:analysis_date]\n",
    "        \n",
    "        # More lenient data requirement: need at least 60 days (2/3 of 90 days)\n",
    "        min_required_days = max(60, lookback_days // 2)\n",
    "        \n",
    "        if len(benchmark_data) < min_required_days:\n",
    "            print(f\"   ⚠️  Insufficient data: {len(benchmark_data)} < {min_required_days} (need {min_required_days} days)\")\n",
    "            return 'Sideways'\n",
    "        \n",
    "        # Convert returns to price series for regime detection\n",
    "        price_series = (1 + benchmark_data).cumprod()\n",
    "        price_data = pd.DataFrame({'close': price_series})\n",
    "        \n",
    "        # Call regime detector with price data\n",
    "        regime = self.regime_detector.detect_regime(price_data)\n",
    "        \n",
    "        # Debug output\n",
    "        print(f\"   🔍 Regime Debug: Date={analysis_date.strftime('%Y-%m-%d')}, Data={len(benchmark_data)} days, Regime={regime}\")\n",
    "        \n",
    "        return regime\n",
    "\n",
    "    def _get_validated_factors_from_precomputed(self, universe: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"Get validated factors from pre-computed data and calculate additional factors.\"\"\"\n",
    "        try:\n",
    "            # Get fundamental data with proper lagging\n",
    "            lag_days = self.config['factors']['fundamental_lag_days']\n",
    "            lag_date = analysis_date - pd.Timedelta(days=lag_days)\n",
    "            \n",
    "            # Get fundamental data for the lagged date\n",
    "            fundamental_data = self.precomputed_data['fundamentals']\n",
    "            fundamental_df = fundamental_data[\n",
    "                (fundamental_data['date'] <= lag_date) & \n",
    "                (fundamental_data['ticker'].isin(universe))\n",
    "            ].copy()\n",
    "            \n",
    "            if fundamental_df.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Get the most recent fundamental data for each ticker\n",
    "            fundamental_df = fundamental_df.sort_values('date').groupby('ticker').tail(1)\n",
    "            \n",
    "            # Get momentum data\n",
    "            momentum_data = self.precomputed_data['momentum']\n",
    "            momentum_df = momentum_data[\n",
    "                (momentum_data['trading_date'] == analysis_date) & \n",
    "                (momentum_data['ticker'].isin(universe))\n",
    "            ].copy()\n",
    "            \n",
    "            if momentum_df.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Merge fundamental and momentum data\n",
    "            factors_df = fundamental_df.merge(momentum_df, on='ticker', how='inner')\n",
    "            \n",
    "            # Add sector information\n",
    "            sector_query = text(\"\"\"\n",
    "                SELECT ticker, sector\n",
    "                FROM master_info\n",
    "                WHERE ticker IN :tickers\n",
    "            \"\"\")\n",
    "            \n",
    "            ticker_list = tuple(universe)\n",
    "            sector_df = pd.read_sql(sector_query, self.engine, params={'tickers': ticker_list})\n",
    "            \n",
    "            factors_df = factors_df.merge(sector_df, on='ticker', how='left')\n",
    "            \n",
    "            # Calculate validated factors\n",
    "            factors_df = self._calculate_validated_factors(factors_df, universe, analysis_date)\n",
    "            \n",
    "            # Apply sector-specific calculations\n",
    "            factors_df = self.sector_calculator.calculate_sector_aware_pe(factors_df)\n",
    "            factors_df = self.sector_calculator.calculate_momentum_score(factors_df)\n",
    "            \n",
    "            # Calculate composite score with validated factors\n",
    "            factors_df = self._calculate_validated_composite_score(factors_df)\n",
    "            \n",
    "            return factors_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting validated factors from precomputed data: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _calculate_validated_factors(self, factors_df: pd.DataFrame, universe: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"Calculate the three validated factors.\"\"\"\n",
    "        try:\n",
    "            # 1. Calculate Low-Volatility factor\n",
    "            print(\"   📊 Calculating Low-Volatility factor...\")\n",
    "            price_data = self.price_data_raw[self.price_data_raw['ticker'].isin(universe)].copy()\n",
    "            if not price_data.empty:\n",
    "                low_vol_data = self.validated_calculator.calculate_low_volatility_factor(\n",
    "                    price_data, self.config['factors']['volatility_lookback']\n",
    "                )\n",
    "                if not low_vol_data.empty:\n",
    "                    # Get the most recent low-vol score for each ticker\n",
    "                    latest_low_vol = low_vol_data.groupby('ticker').tail(1)[['ticker', 'low_vol_score']]\n",
    "                    factors_df = factors_df.merge(latest_low_vol, on='ticker', how='left')\n",
    "            \n",
    "            # 2. Calculate Piotroski F-Score\n",
    "            print(\"   📊 Calculating Piotroski F-Score...\")\n",
    "            fscore_data = self.validated_calculator.calculate_piotroski_fscore(universe, analysis_date)\n",
    "            if not fscore_data.empty:\n",
    "                factors_df = factors_df.merge(fscore_data[['ticker', 'fscore']], on='ticker', how='left')\n",
    "            \n",
    "            # 3. Calculate FCF Yield\n",
    "            print(\"   📊 Calculating FCF Yield...\")\n",
    "            fcf_data = self.validated_calculator.calculate_fcf_yield(universe, analysis_date)\n",
    "            if not fcf_data.empty:\n",
    "                factors_df = factors_df.merge(fcf_data[['ticker', 'fcf_yield']], on='ticker', how='left')\n",
    "            \n",
    "            return factors_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error calculating validated factors: {e}\")\n",
    "            return factors_df\n",
    "\n",
    "    def _calculate_validated_composite_score(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate composite score using validated factors structure.\"\"\"\n",
    "        factors_df['composite_score'] = 0.0\n",
    "        \n",
    "        # Value Factors (33% total weight)\n",
    "        value_score = 0.0\n",
    "        \n",
    "        # P/E component (contrarian signal - lower is better)\n",
    "        if 'quality_adjusted_pe' in factors_df.columns:\n",
    "            pe_weight = self.config['factors']['value_factors']['pe_weight']\n",
    "            factors_df['pe_normalized'] = (factors_df['quality_adjusted_pe'] - factors_df['quality_adjusted_pe'].mean()) / factors_df['quality_adjusted_pe'].std()\n",
    "            value_score += (-factors_df['pe_normalized']) * pe_weight  # Negative for contrarian\n",
    "        \n",
    "        # FCF Yield component (positive signal - higher is better)\n",
    "        if 'fcf_yield' in factors_df.columns:\n",
    "            fcf_weight = self.config['factors']['value_factors']['fcf_yield_weight']\n",
    "            factors_df['fcf_normalized'] = (factors_df['fcf_yield'] - factors_df['fcf_yield'].mean()) / factors_df['fcf_yield'].std()\n",
    "            value_score += factors_df['fcf_normalized'] * fcf_weight\n",
    "        \n",
    "        # Quality Factors (33% total weight)\n",
    "        quality_score = 0.0\n",
    "        \n",
    "        # ROAA component (positive signal - higher is better)\n",
    "        if 'roaa' in factors_df.columns:\n",
    "            roaa_weight = self.config['factors']['quality_factors']['roaa_weight']\n",
    "            factors_df['roaa_normalized'] = (factors_df['roaa'] - factors_df['roaa'].mean()) / factors_df['roaa'].std()\n",
    "            quality_score += factors_df['roaa_normalized'] * roaa_weight\n",
    "        \n",
    "        # Piotroski F-Score component (positive signal - higher is better)\n",
    "        if 'fscore' in factors_df.columns:\n",
    "            fscore_weight = self.config['factors']['quality_factors']['fscore_weight']\n",
    "            factors_df['fscore_normalized'] = (factors_df['fscore'] - factors_df['fscore'].mean()) / factors_df['fscore'].std()\n",
    "            quality_score += factors_df['fscore_normalized'] * fscore_weight\n",
    "        \n",
    "        # Momentum Factors (34% total weight)\n",
    "        momentum_score = 0.0\n",
    "        \n",
    "        # Existing momentum component (mixed signals)\n",
    "        if 'momentum_score' in factors_df.columns:\n",
    "            momentum_weight = self.config['factors']['momentum_factors']['momentum_weight']\n",
    "            factors_df['momentum_normalized'] = (factors_df['momentum_score'] - factors_df['momentum_score'].mean()) / factors_df['momentum_score'].std()\n",
    "            momentum_score += factors_df['momentum_normalized'] * momentum_weight\n",
    "        \n",
    "        # Low-Volatility component (defensive - inverse volatility)\n",
    "        if 'low_vol_score' in factors_df.columns:\n",
    "            low_vol_weight = self.config['factors']['momentum_factors']['low_vol_weight']\n",
    "            factors_df['low_vol_normalized'] = (factors_df['low_vol_score'] - factors_df['low_vol_score'].mean()) / factors_df['low_vol_score'].std()\n",
    "            momentum_score += factors_df['low_vol_normalized'] * low_vol_weight\n",
    "        \n",
    "        # Combine all factor categories\n",
    "        factors_df['composite_score'] = (\n",
    "            value_score * self.config['factors']['value_weight'] +\n",
    "            quality_score * self.config['factors']['quality_weight'] +\n",
    "            momentum_score * self.config['factors']['momentum_weight']\n",
    "        )\n",
    "        \n",
    "        return factors_df\n",
    "\n",
    "    def _apply_entry_criteria(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply entry criteria to filter stocks.\"\"\"\n",
    "        qualified = factors_df.copy()\n",
    "        \n",
    "        # Quality filters\n",
    "        if 'roaa' in qualified.columns:\n",
    "            qualified = qualified[qualified['roaa'] > 0]  # Positive ROAA\n",
    "        \n",
    "        if 'net_margin' in qualified.columns:\n",
    "            qualified = qualified[qualified['net_margin'] > 0]  # Positive net margin\n",
    "        \n",
    "        # F-Score filter (minimum quality threshold)\n",
    "        if 'fscore' in qualified.columns:\n",
    "            qualified = qualified[qualified['fscore'] >= 5]  # At least 5 out of 9 tests passed\n",
    "        \n",
    "        # FCF Yield filter (positive cash flow)\n",
    "        if 'fcf_yield' in qualified.columns:\n",
    "            qualified = qualified[qualified['fcf_yield'] > 0]  # Positive FCF Yield\n",
    "        \n",
    "        return qualified\n",
    "\n",
    "    def _construct_portfolio(self, qualified_df: pd.DataFrame, regime_allocation: float) -> pd.Series:\n",
    "        \"\"\"Construct the portfolio using the qualified stocks.\"\"\"\n",
    "        if qualified_df.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Sort by composite score\n",
    "        qualified_df = qualified_df.sort_values('composite_score', ascending=False)\n",
    "        \n",
    "        # Select top stocks\n",
    "        target_size = self.config['universe']['target_portfolio_size']\n",
    "        selected_stocks = qualified_df.head(target_size)\n",
    "        \n",
    "        if selected_stocks.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Equal weight portfolio\n",
    "        portfolio = pd.Series(regime_allocation / len(selected_stocks), index=selected_stocks['ticker'])\n",
    "        \n",
    "        return portfolio\n",
    "\n",
    "    def _calculate_net_returns(self, daily_holdings: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate net returns with transaction costs.\"\"\"\n",
    "        holdings_shifted = daily_holdings.shift(1).fillna(0.0)\n",
    "        gross_returns = (holdings_shifted * self.daily_returns_matrix).sum(axis=1)\n",
    "        \n",
    "        # Calculate turnover and costs\n",
    "        turnover = (holdings_shifted - holdings_shifted.shift(1)).abs().sum(axis=1) / 2.0\n",
    "        costs = turnover * (self.config['transaction_cost_bps'] / 10000)\n",
    "        net_returns = (gross_returns - costs).rename(self.config['strategy_name'])\n",
    "        \n",
    "        print(\"\\n💸 Net returns calculated.\")\n",
    "        print(f\"   - Total Gross Return: {(1 + gross_returns).prod() - 1:.2%}\")\n",
    "        print(f\"   - Total Net Return: {(1 + net_returns).prod() - 1:.2%}\")\n",
    "        print(f\"   - Total Cost Drag: {(gross_returns.sum() - net_returns.sum()):.2%}\")\n",
    "        \n",
    "        return net_returns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c43d15f",
   "metadata": {},
   "source": [
    "# DATA PRE-COMPUTATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ab14e2c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def precompute_universe_rankings(config: dict, db_engine):\n",
    "    \"\"\"\n",
    "    Pre-compute universe rankings for all rebalance dates.\n",
    "    This eliminates the need for individual universe queries during rebalancing.\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Pre-computing universe rankings for all dates...\")\n",
    "    \n",
    "    universe_query = text(\"\"\"\n",
    "        WITH daily_adtv AS (\n",
    "            SELECT \n",
    "                trading_date,\n",
    "                ticker,\n",
    "                total_volume * close_price_adjusted as adtv_vnd\n",
    "            FROM vcsc_daily_data_complete\n",
    "            WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "        ),\n",
    "        rolling_adtv AS (\n",
    "            SELECT \n",
    "                trading_date,\n",
    "                ticker,\n",
    "                AVG(adtv_vnd) OVER (\n",
    "                    PARTITION BY ticker \n",
    "                    ORDER BY trading_date \n",
    "                    ROWS BETWEEN 62 PRECEDING AND CURRENT ROW\n",
    "                ) as avg_adtv_63d\n",
    "            FROM daily_adtv\n",
    "        ),\n",
    "        ranked_universe AS (\n",
    "            SELECT \n",
    "                trading_date,\n",
    "                ticker,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY trading_date \n",
    "                    ORDER BY avg_adtv_63d DESC\n",
    "                ) as rank_position\n",
    "            FROM rolling_adtv\n",
    "            WHERE avg_adtv_63d > 0\n",
    "        )\n",
    "        SELECT trading_date, ticker\n",
    "        FROM ranked_universe\n",
    "        WHERE rank_position <= :top_n_stocks\n",
    "        ORDER BY trading_date, rank_position\n",
    "    \"\"\")\n",
    "    \n",
    "    universe_data = pd.read_sql(universe_query, db_engine, \n",
    "                               params={'start_date': config['backtest_start_date'], \n",
    "                                       'end_date': config['backtest_end_date'],\n",
    "                                       'top_n_stocks': config['universe']['top_n_stocks']},\n",
    "                               parse_dates=['trading_date'])\n",
    "    \n",
    "    print(f\"   ✅ Pre-computed universe rankings: {len(universe_data):,} observations\")\n",
    "    return universe_data\n",
    "\n",
    "def precompute_fundamental_factors(config: dict, db_engine):\n",
    "    \"\"\"\n",
    "    Pre-compute fundamental factors for all rebalance dates.\n",
    "    This eliminates the need for individual fundamental queries during rebalancing.\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Pre-computing fundamental factors for all dates...\")\n",
    "    \n",
    "    # Get all years needed for fundamental calculations\n",
    "    start_year = pd.Timestamp(config['backtest_start_date']).year - 1\n",
    "    end_year = pd.Timestamp(config['backtest_end_date']).year\n",
    "    \n",
    "    fundamental_query = text(\"\"\"\n",
    "        WITH fundamental_metrics AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                fv.item_id,\n",
    "                fv.statement_type,\n",
    "                SUM(fv.value / 1e9) as value_bn\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.year BETWEEN :start_year AND :end_year\n",
    "            AND fv.item_id IN (1, 2)\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter, fv.item_id, fv.statement_type\n",
    "        ),\n",
    "        netprofit_ttm AS (\n",
    "            SELECT \n",
    "                ticker,\n",
    "                year,\n",
    "                quarter,\n",
    "                SUM(CASE WHEN item_id = 1 AND statement_type = 'PL' THEN value_bn ELSE 0 END) as netprofit_ttm\n",
    "            FROM fundamental_metrics\n",
    "            GROUP BY ticker, year, quarter\n",
    "        ),\n",
    "        totalassets_ttm AS (\n",
    "            SELECT \n",
    "                ticker,\n",
    "                year,\n",
    "                quarter,\n",
    "                SUM(CASE WHEN item_id = 2 AND statement_type = 'BS' THEN value_bn ELSE 0 END) as totalassets_ttm\n",
    "            FROM fundamental_metrics\n",
    "            GROUP BY ticker, year, quarter\n",
    "        ),\n",
    "        revenue_ttm AS (\n",
    "            SELECT \n",
    "                ticker,\n",
    "                year,\n",
    "                quarter,\n",
    "                SUM(CASE WHEN item_id = 2 AND statement_type = 'PL' THEN value_bn ELSE 0 END) as revenue_ttm\n",
    "            FROM fundamental_metrics\n",
    "            GROUP BY ticker, year, quarter\n",
    "        )\n",
    "        SELECT \n",
    "            np.ticker,\n",
    "            np.year,\n",
    "            np.quarter,\n",
    "            np.netprofit_ttm,\n",
    "            ta.totalassets_ttm,\n",
    "            rv.revenue_ttm,\n",
    "            CASE \n",
    "                WHEN ta.totalassets_ttm > 0 THEN np.netprofit_ttm / ta.totalassets_ttm \n",
    "                ELSE NULL \n",
    "            END as roaa,\n",
    "            CASE \n",
    "                WHEN rv.revenue_ttm > 0 THEN np.netprofit_ttm / rv.revenue_ttm\n",
    "                ELSE NULL \n",
    "            END as net_margin,\n",
    "            CASE \n",
    "                WHEN ta.totalassets_ttm > 0 THEN rv.revenue_ttm / ta.totalassets_ttm\n",
    "                ELSE NULL \n",
    "            END as asset_turnover\n",
    "        FROM netprofit_ttm np\n",
    "        LEFT JOIN totalassets_ttm ta ON np.ticker = ta.ticker AND np.year = ta.year AND np.quarter = ta.quarter\n",
    "        LEFT JOIN revenue_ttm rv ON np.ticker = rv.ticker AND np.year = rv.year AND np.quarter = rv.quarter\n",
    "        WHERE np.netprofit_ttm > 0 \n",
    "        AND ta.totalassets_ttm > 0\n",
    "        AND rv.revenue_ttm > 0\n",
    "    \"\"\")\n",
    "    \n",
    "    fundamental_data = pd.read_sql(fundamental_query, db_engine,\n",
    "                                  params={'start_year': start_year, 'end_year': end_year})\n",
    "    \n",
    "    # Add date column for easier lookup\n",
    "    fundamental_data['date'] = pd.to_datetime(\n",
    "        fundamental_data['year'].astype(str) + '-' + \n",
    "        (fundamental_data['quarter'] * 3).astype(str).str.zfill(2) + '-01'\n",
    "    )\n",
    "    \n",
    "    print(f\"   ✅ Pre-computed fundamental factors: {len(fundamental_data):,} observations\")\n",
    "    return fundamental_data\n",
    "\n",
    "def precompute_momentum_factors(config: dict, db_engine):\n",
    "    \"\"\"\n",
    "    Pre-compute momentum factors using vectorized operations.\n",
    "    This eliminates the need for individual momentum calculations during rebalancing.\n",
    "    \"\"\"\n",
    "    print(\"\\n📊 Pre-computing momentum factors using vectorized operations...\")\n",
    "    \n",
    "    # Get all price data once\n",
    "    price_query = text(\"\"\"\n",
    "        SELECT \n",
    "            trading_date,\n",
    "            ticker,\n",
    "            close_price_adjusted as close\n",
    "        FROM vcsc_daily_data_complete\n",
    "        WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "        ORDER BY ticker, trading_date\n",
    "    \"\"\")\n",
    "    \n",
    "    price_data = pd.read_sql(price_query, db_engine,\n",
    "                            params={'start_date': config['backtest_start_date'],\n",
    "                                    'end_date': config['backtest_end_date']},\n",
    "                            parse_dates=['trading_date'])\n",
    "    \n",
    "    print(f\"   ✅ Loaded price data: {len(price_data):,} observations\")\n",
    "    \n",
    "    # Pivot for vectorized calculations\n",
    "    price_pivot = price_data.pivot(index='trading_date', columns='ticker', values='close')\n",
    "    \n",
    "    # Calculate momentum factors vectorized\n",
    "    skip_months = config['factors']['skip_months']\n",
    "    \n",
    "    # Initialize the result DataFrame with the same structure as price_pivot\n",
    "    momentum_df = price_pivot.copy()\n",
    "    momentum_df = momentum_df.stack().reset_index()\n",
    "    momentum_df.columns = ['trading_date', 'ticker', 'close']\n",
    "    \n",
    "    # Add momentum columns\n",
    "    for period in config['factors']['momentum_horizons']:\n",
    "        # Apply skip month logic\n",
    "        if skip_months > 0:\n",
    "            # Shift by skip_months days (approximately)\n",
    "            shifted_prices = price_pivot.shift(skip_months * 30)\n",
    "            momentum_calc = (shifted_prices / shifted_prices.shift(period)) - 1\n",
    "        else:\n",
    "            momentum_calc = price_pivot.pct_change(periods=period)\n",
    "        \n",
    "        # Stack the momentum calculation and add to the result\n",
    "        momentum_stacked = momentum_calc.stack().reset_index()\n",
    "        momentum_stacked.columns = ['trading_date', 'ticker', f'momentum_{period}d']\n",
    "        \n",
    "        # Merge with the main DataFrame\n",
    "        momentum_df = momentum_df.merge(momentum_stacked, on=['trading_date', 'ticker'], how='left')\n",
    "    \n",
    "    # Drop the close column as it's not needed\n",
    "    momentum_df = momentum_df.drop('close', axis=1)\n",
    "    \n",
    "    print(f\"   ✅ Pre-computed momentum factors: {len(momentum_df):,} observations\")\n",
    "    return momentum_df\n",
    "\n",
    "def precompute_all_data(config: dict, db_engine):\n",
    "    \"\"\"\n",
    "    Pre-compute all data needed for the backtest.\n",
    "    This is the main optimization that reduces database queries from 342 to 4.\n",
    "    \"\"\"\n",
    "    print(\"\\n🚀 OPTIMIZATION: Pre-computing all data for faster rebalancing...\")\n",
    "    \n",
    "    # Pre-compute all data components\n",
    "    universe_data = precompute_universe_rankings(config, db_engine)\n",
    "    fundamental_data = precompute_fundamental_factors(config, db_engine)\n",
    "    momentum_data = precompute_momentum_factors(config, db_engine)\n",
    "    \n",
    "    # Create optimized data structure\n",
    "    precomputed_data = {\n",
    "        'universe': universe_data,\n",
    "        'fundamentals': fundamental_data,\n",
    "        'momentum': momentum_data\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ All data pre-computed successfully!\")\n",
    "    print(f\"   - Universe rankings: {len(universe_data):,} observations\")\n",
    "    print(f\"   - Fundamental factors: {len(fundamental_data):,} observations\")\n",
    "    print(f\"   - Momentum factors: {len(momentum_data):,} observations\")\n",
    "    print(f\"   - Database queries reduced from 342 to 4 (98.8% reduction)\")\n",
    "    \n",
    "    return precomputed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e84814",
   "metadata": {},
   "source": [
    "# DATA LOADING, ANALYSIS, AND MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e91b36",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_all_data_for_backtest(config: dict, db_engine):\n",
    "    \"\"\"\n",
    "    Loads all necessary data (prices, fundamentals, sectors) for the\n",
    "    specified backtest period.\n",
    "    \"\"\"\n",
    "    start_date = config['backtest_start_date']\n",
    "    end_date = config['backtest_end_date']\n",
    "    \n",
    "    # Add a buffer to the start date for rolling calculations\n",
    "    buffer_start_date = pd.Timestamp(start_date) - pd.DateOffset(months=6)\n",
    "    \n",
    "    print(f\"📂 Loading all data for period: {buffer_start_date.date()} to {end_date}...\")\n",
    "\n",
    "    # 1. Price and Volume Data\n",
    "    print(\"   - Loading price and volume data...\")\n",
    "    price_query = text(\"\"\"\n",
    "        SELECT \n",
    "            trading_date as date,\n",
    "            ticker,\n",
    "            close_price_adjusted as close,\n",
    "            total_volume as volume,\n",
    "            market_cap\n",
    "        FROM vcsc_daily_data_complete\n",
    "        WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "    \"\"\")\n",
    "    price_data = pd.read_sql(price_query, db_engine, \n",
    "                            params={'start_date': buffer_start_date, 'end_date': end_date}, \n",
    "                            parse_dates=['date'])\n",
    "    print(f\"     ✅ Loaded {len(price_data):,} price observations.\")\n",
    "\n",
    "    # 2. Fundamental Data (from fundamental_values table with simplified approach)\n",
    "    print(\"   - Loading fundamental data from fundamental_values with simplified approach...\")\n",
    "    fundamental_query = text(\"\"\"\n",
    "        WITH netprofit_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as netprofit_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 1\n",
    "            AND fv.statement_type = 'PL'\n",
    "            AND fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        ),\n",
    "        totalassets_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as totalassets_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 2\n",
    "            AND fv.statement_type = 'BS'\n",
    "            AND fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        ),\n",
    "        revenue_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as revenue_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 2\n",
    "            AND fv.statement_type = 'PL'\n",
    "            AND fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        )\n",
    "        SELECT \n",
    "            np.ticker,\n",
    "            mi.sector,\n",
    "            DATE(CONCAT(np.year, '-', LPAD(np.quarter * 3, 2, '0'), '-01')) as date,\n",
    "            np.netprofit_ttm,\n",
    "            ta.totalassets_ttm,\n",
    "            rv.revenue_ttm,\n",
    "            CASE \n",
    "                WHEN ta.totalassets_ttm > 0 THEN np.netprofit_ttm / ta.totalassets_ttm \n",
    "                ELSE NULL \n",
    "            END as roaa,\n",
    "            CASE \n",
    "                WHEN rv.revenue_ttm > 0 THEN np.netprofit_ttm / rv.revenue_ttm\n",
    "                ELSE NULL \n",
    "            END as net_margin,\n",
    "            CASE \n",
    "                WHEN ta.totalassets_ttm > 0 THEN rv.revenue_ttm / ta.totalassets_ttm\n",
    "                ELSE NULL \n",
    "            END as asset_turnover\n",
    "        FROM netprofit_ttm np\n",
    "        LEFT JOIN totalassets_ttm ta ON np.ticker = ta.ticker AND np.year = ta.year AND np.quarter = ta.quarter\n",
    "        LEFT JOIN revenue_ttm rv ON np.ticker = rv.ticker AND np.year = rv.year AND np.quarter = rv.quarter\n",
    "        LEFT JOIN master_info mi ON np.ticker = mi.ticker\n",
    "        WHERE np.netprofit_ttm > 0 \n",
    "        AND ta.totalassets_ttm > 0\n",
    "        AND rv.revenue_ttm > 0\n",
    "    \"\"\")\n",
    "    \n",
    "    fundamental_data = pd.read_sql(fundamental_query, db_engine, \n",
    "                                  params={'start_date': buffer_start_date, 'end_date': end_date}, \n",
    "                                  parse_dates=['date'])\n",
    "    print(f\"     ✅ Loaded {len(fundamental_data):,} fundamental observations from fundamental_values.\")\n",
    "\n",
    "    # 3. Benchmark Data (VN-Index)\n",
    "    print(\"   - Loading benchmark data (VN-Index)...\")\n",
    "    benchmark_query = text(\"\"\"\n",
    "        SELECT date, close\n",
    "        FROM etf_history\n",
    "        WHERE ticker = 'VNINDEX' AND date BETWEEN :start_date AND :end_date\n",
    "    \"\"\")\n",
    "    benchmark_data = pd.read_sql(benchmark_query, db_engine, \n",
    "                                params={'start_date': buffer_start_date, 'end_date': end_date}, \n",
    "                                parse_dates=['date'])\n",
    "    print(f\"     ✅ Loaded {len(benchmark_data):,} benchmark observations.\")\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    print(\"\\n🛠️  Preparing data structures for backtesting engine...\")\n",
    "\n",
    "    # Create returns matrix\n",
    "    price_data['return'] = price_data.groupby('ticker')['close'].pct_change()\n",
    "    daily_returns_matrix = price_data.pivot(index='date', columns='ticker', values='return')\n",
    "\n",
    "    # Create benchmark returns series\n",
    "    benchmark_returns = benchmark_data.set_index('date')['close'].pct_change().rename('VN-Index')\n",
    "\n",
    "    print(\"   ✅ Data preparation complete.\")\n",
    "    return price_data, fundamental_data, daily_returns_matrix, benchmark_returns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136edc3",
   "metadata": {},
   "source": [
    "# PERFORMANCE ANALYSIS FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc866e52",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(returns: pd.Series, benchmark: pd.Series, periods_per_year: int = 252) -> dict:\n",
    "    \"\"\"Calculates comprehensive performance metrics with corrected benchmark alignment.\"\"\"\n",
    "    # Align benchmark\n",
    "    first_trade_date = returns.loc[returns.ne(0)].index.min()\n",
    "    if pd.isna(first_trade_date):\n",
    "        return {metric: 0.0 for metric in ['Annualized Return (%)', 'Annualized Volatility (%)', 'Sharpe Ratio', 'Max Drawdown (%)', 'Calmar Ratio', 'Information Ratio', 'Beta']}\n",
    "    \n",
    "    aligned_returns = returns.loc[first_trade_date:]\n",
    "    aligned_benchmark = benchmark.loc[first_trade_date:]\n",
    "\n",
    "    n_years = len(aligned_returns) / periods_per_year\n",
    "    annualized_return = ((1 + aligned_returns).prod() ** (1 / n_years) - 1) if n_years > 0 else 0\n",
    "    annualized_volatility = aligned_returns.std() * np.sqrt(periods_per_year)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility != 0 else 0.0\n",
    "    \n",
    "    cumulative_returns = (1 + aligned_returns).cumprod()\n",
    "    max_drawdown = (cumulative_returns / cumulative_returns.cummax() - 1).min()\n",
    "    calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else 0.0\n",
    "    \n",
    "    excess_returns = aligned_returns - aligned_benchmark\n",
    "    information_ratio = (excess_returns.mean() * periods_per_year) / (excess_returns.std() * np.sqrt(periods_per_year)) if excess_returns.std() > 0 else 0.0\n",
    "    beta = aligned_returns.cov(aligned_benchmark) / aligned_benchmark.var() if aligned_benchmark.var() > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'Annualized Return (%)': annualized_return * 100,\n",
    "        'Annualized Volatility (%)': annualized_volatility * 100,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown (%)': max_drawdown * 100,\n",
    "        'Calmar Ratio': calmar_ratio,\n",
    "        'Information Ratio': information_ratio,\n",
    "        'Beta': beta\n",
    "    }\n",
    "\n",
    "def generate_comprehensive_tearsheet(strategy_returns: pd.Series, benchmark_returns: pd.Series, diagnostics: pd.DataFrame, title: str):\n",
    "    \"\"\"Generates comprehensive institutional tearsheet with equity curve and analysis.\"\"\"\n",
    "    \n",
    "    # Align benchmark for plotting & metrics\n",
    "    first_trade_date = strategy_returns.loc[strategy_returns.ne(0)].index.min()\n",
    "    aligned_strategy_returns = strategy_returns.loc[first_trade_date:]\n",
    "    aligned_benchmark_returns = benchmark_returns.loc[first_trade_date:]\n",
    "\n",
    "    strategy_metrics = calculate_performance_metrics(strategy_returns, benchmark_returns)\n",
    "    benchmark_metrics = calculate_performance_metrics(benchmark_returns, benchmark_returns)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 26))\n",
    "    gs = fig.add_gridspec(5, 2, height_ratios=[1.2, 0.8, 0.8, 0.8, 1.2], hspace=0.7, wspace=0.2)\n",
    "    fig.suptitle(title, fontsize=20, fontweight='bold', color='#2C3E50')\n",
    "\n",
    "    # 1. Cumulative Performance (Equity Curve)\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    (1 + aligned_strategy_returns).cumprod().plot(ax=ax1, label='QVM Engine v3j Validated Factors', color='#16A085', lw=2.5)\n",
    "    (1 + aligned_benchmark_returns).cumprod().plot(ax=ax1, label='VN-Index (Aligned)', color='#34495E', linestyle='--', lw=2)\n",
    "    ax1.set_title('Cumulative Performance (Log Scale)', fontweight='bold')\n",
    "    ax1.set_ylabel('Growth of 1 VND')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 2. Drawdown Analysis\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    drawdown = ((1 + aligned_strategy_returns).cumprod() / (1 + aligned_strategy_returns).cumprod().cummax() - 1) * 100\n",
    "    drawdown.plot(ax=ax2, color='#C0392B')\n",
    "    ax2.fill_between(drawdown.index, drawdown, 0, color='#C0392B', alpha=0.1)\n",
    "    ax2.set_title('Drawdown Analysis', fontweight='bold')\n",
    "    ax2.set_ylabel('Drawdown (%)')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 3. Annual Returns\n",
    "    ax3 = fig.add_subplot(gs[2, 0])\n",
    "    strat_annual = aligned_strategy_returns.resample('Y').apply(lambda x: (1+x).prod()-1) * 100\n",
    "    bench_annual = aligned_benchmark_returns.resample('Y').apply(lambda x: (1+x).prod()-1) * 100\n",
    "    pd.DataFrame({'Strategy': strat_annual, 'Benchmark': bench_annual}).plot(kind='bar', ax=ax3, color=['#16A085', '#34495E'])\n",
    "    ax3.set_xticklabels([d.strftime('%Y') for d in strat_annual.index], rotation=45, ha='right')\n",
    "    ax3.set_title('Annual Returns', fontweight='bold')\n",
    "    ax3.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 4. Rolling Sharpe Ratio\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    rolling_sharpe = (aligned_strategy_returns.rolling(252).mean() * 252) / (aligned_strategy_returns.rolling(252).std() * np.sqrt(252))\n",
    "    rolling_sharpe.plot(ax=ax4, color='#E67E22')\n",
    "    ax4.axhline(1.0, color='#27AE60', linestyle='--')\n",
    "    ax4.set_title('1-Year Rolling Sharpe Ratio', fontweight='bold')\n",
    "    ax4.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 5. Regime Analysis\n",
    "    ax5 = fig.add_subplot(gs[3, 0])\n",
    "    if not diagnostics.empty and 'regime' in diagnostics.columns:\n",
    "        regime_counts = diagnostics['regime'].value_counts()\n",
    "        regime_counts.plot(kind='bar', ax=ax5, color=['#3498DB', '#E74C3C', '#F39C12', '#9B59B6'])\n",
    "        ax5.set_title('Regime Distribution', fontweight='bold')\n",
    "        ax5.set_ylabel('Number of Rebalances')\n",
    "        ax5.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 6. Portfolio Size Evolution\n",
    "    ax6 = fig.add_subplot(gs[3, 1])\n",
    "    if not diagnostics.empty and 'portfolio_size' in diagnostics.columns:\n",
    "        diagnostics['portfolio_size'].plot(ax=ax6, color='#2ECC71', marker='o', markersize=3)\n",
    "        ax6.set_title('Portfolio Size Evolution', fontweight='bold')\n",
    "        ax6.set_ylabel('Number of Stocks')\n",
    "        ax6.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 7. Performance Metrics Table\n",
    "    ax7 = fig.add_subplot(gs[4:, :])\n",
    "    ax7.axis('off')\n",
    "    summary_data = [['Metric', 'Strategy', 'Benchmark']]\n",
    "    for key in strategy_metrics.keys():\n",
    "        summary_data.append([key, f\"{strategy_metrics[key]:.2f}\", f\"{benchmark_metrics.get(key, 0.0):.2f}\"])\n",
    "    \n",
    "    table = ax7.table(cellText=summary_data[1:], colLabels=summary_data[0], loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c83658",
   "metadata": {},
   "source": [
    "# MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d300b465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 QVM ENGINE V3J: VALIDATED FACTORS STRATEGY EXECUTION\n",
      "================================================================================\n",
      "📂 Loading all data for period: 2015-07-01 to 2025-07-28...\n",
      "   - Loading price and volume data...\n",
      "     ✅ Loaded 1,695,229 price observations.\n",
      "   - Loading fundamental data from fundamental_values with simplified approach...\n",
      "     ✅ Loaded 11,149 fundamental observations from fundamental_values.\n",
      "   - Loading benchmark data (VN-Index)...\n",
      "     ✅ Loaded 2,519 benchmark observations.\n",
      "\n",
      "🛠️  Preparing data structures for backtesting engine...\n",
      "   ✅ Data preparation complete.\n",
      "\n",
      "✅ All basic data successfully loaded and prepared for the backtest.\n",
      "   - Price Data Shape: (1695229, 6)\n",
      "   - Fundamental Data Shape: (11149, 9)\n",
      "   - Returns Matrix Shape: (2520, 728)\n",
      "   - Benchmark Returns: 2519 days\n",
      "\n",
      "🚀 OPTIMIZATION: Pre-computing all data for faster rebalancing...\n",
      "\n",
      "📊 Pre-computing universe rankings for all dates...\n",
      "   ✅ Pre-computed universe rankings: 477,800 observations\n",
      "\n",
      "📊 Pre-computing fundamental factors for all dates...\n",
      "   ✅ Pre-computed fundamental factors: 11,149 observations\n",
      "\n",
      "📊 Pre-computing momentum factors using vectorized operations...\n",
      "   ✅ Loaded price data: 1,624,849 observations\n",
      "   ✅ Pre-computed momentum factors: 1,624,849 observations\n",
      "\n",
      "✅ All data pre-computed successfully!\n",
      "   - Universe rankings: 477,800 observations\n",
      "   - Fundamental factors: 11,149 observations\n",
      "   - Momentum factors: 1,624,849 observations\n",
      "   - Database queries reduced from 342 to 4 (98.8% reduction)\n",
      "\n",
      "================================================================================\n",
      "🚀 QVM ENGINE V3J: VALIDATED FACTORS BACKTEST\n",
      "================================================================================\n",
      "✅ RegimeDetector initialized with thresholds:\n",
      "   - Volatility: 1.40%\n",
      "   - Return: 0.12%\n",
      "   - Low Return: 0.02%\n",
      "✅ ValidatedFactorsCalculator initialized\n",
      "✅ ValidatedFactorsCalculator initialized\n",
      "   ✅ Pre-computed data indexed for fast access\n",
      "✅ QVMEngineV3jValidatedFactors initialized.\n",
      "   - Strategy: QVM_Engine_v3j_Validated_Factors\n",
      "   - Period: 2016-01-04 to 2025-07-25\n",
      "   - Value Factors: P/E + FCF Yield (33% weight)\n",
      "   - Quality Factors: ROAA + Piotroski F-Score (33% weight)\n",
      "   - Momentum Factors: Multi-horizon + Low-Volatility (34% weight)\n",
      "   - Performance: Pre-computed data + Vectorized operations\n",
      "\n",
      "🚀 Starting QVM Engine v3j validated factors backtest execution...\n",
      "   - Generated 114 monthly rebalance dates.\n",
      "   - Processing rebalance 1/114: 2016-01-29...   ⚠️  Insufficient data: 20 < 60 (need 60 days)\n",
      "   📊 Calculating Low-Volatility factor...\n",
      "   ✅ Low-Volatility factor calculated: 416,959 observations\n",
      "   📊 Calculating Piotroski F-Score...\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('FID', 'HHS', 'JVC', 'PCT', 'PET', 'QBS', 'SHN', 'TLH', 'TSC', 'VFG', 'VMD', 'SDA'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('KLF', 'MWG', 'SVC', 'PNJ'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('TTB', 'BCG', 'CII', 'CTD', 'CTI', 'CTX', 'FCN', 'HBC', 'HHV', 'HUT', 'LCG', 'LIG', 'NDX', 'REE', 'S55', 'S99', 'SC5', 'VCG', 'VNE'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('BIC', 'BMI', 'BVH', 'PTI', 'PVI', 'VNR'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('API', 'CEO', 'DIG', 'DRH', 'DXG', 'FDC', 'HAR', 'HDC', 'HQC', 'IJC', 'ITA', 'ITC', 'KBC', 'KDH', 'LDG', 'LHG', 'NBB', 'NDN', 'NHA', 'NLG', 'NTL', 'PDR', 'PV2', 'QCG', 'SCR', 'SJS', 'TDC', 'TDH', 'TIG', 'VC3', 'VIC', 'VPH'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating securities F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'Revenue_TTM' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetTradingIncome_TTM,\n",
      "                    BrokerageRevenue_TTM,\n",
      "                    OperatingExpenses_TTM,\n",
      "                    Revenue_TTM,\n",
      "                    TotalEquity,\n",
      "                    AvgTotalAssets,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_securities\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('AGR', 'APG', 'BVS', 'HCM', 'IVS', 'SHS', 'SSI', 'TVB', 'VDS', 'VIG', 'VIX', 'VND', 'WSS'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('BED', 'DST', 'ELC', 'FPT', 'ITD'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('SVN', 'HAG', 'HNG'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('DHG', 'DMC', 'FIT', 'TRA'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('C32', 'DHA', 'DHM', 'KSB', 'KSQ', 'PVB', 'PVC', 'PVD', 'PVS', 'SPI', 'TNT'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('ACB', 'BID', 'CTG', 'EIB', 'MBB', 'SHB', 'STB', 'VCB'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('ASM', 'DAT', 'TFC', 'FMC', 'IDI', 'VHC'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('GIL', 'KMR', 'SHA', 'STK', 'TCM', 'TNG', 'TTF'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('CSM', 'DRC', 'SRC'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('AAA', 'BMP', 'DAG', 'DCM', 'DGC', 'DPM', 'HDA', 'HRC', 'LIX', 'NTP'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('DBC', 'KDC', 'LAF', 'LSS', 'MSN', 'NAF', 'PAN', 'SBT', 'VCF', 'VNM'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('BCC', 'CVT', 'FCM', 'HPG', 'HSG', 'HT1', 'PDB', 'TEG', 'VCS', 'VGS'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('GAS', 'NT2', 'PGC', 'PGD', 'PGS', 'PPC', 'PVG', 'VSH'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('DVP', 'GMD', 'GSP', 'HAH', 'MHC', 'NCT', 'PVT', 'SKG', 'TCL', 'VIP', 'VSC'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('TTZ', 'CAV', 'DQC', 'GEX', 'MBG', 'PAC', 'SAM'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('NVT',), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('DHC', 'DLG', 'HAP', 'ITQ', 'PLC', 'PTB', 'SHI', 'TMT'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ❌ Error calculating non-financial F-Score: (pymysql.err.OperationalError) (1054, \"Unknown column 'TotalDebt' in 'field list'\")\n",
      "[SQL: \n",
      "                SELECT \n",
      "                    ticker,\n",
      "                    year,\n",
      "                    quarter,\n",
      "                    NetProfit_TTM,\n",
      "                    NetCFO_TTM,\n",
      "                    AvgTotalAssets,\n",
      "                    TotalDebt,\n",
      "                    CurrentAssets,\n",
      "                    CurrentLiabilities,\n",
      "                    TotalEquity,\n",
      "                    Revenue_TTM,\n",
      "                    GrossProfit_TTM,\n",
      "                    SharesOutstanding\n",
      "                FROM intermediary_calculations_enhanced\n",
      "                WHERE ticker IN %(tickers)s\n",
      "                AND year >= YEAR(%(analysis_date)s) - 2\n",
      "                ORDER BY ticker, year, quarter\n",
      "            ]\n",
      "[parameters: {'tickers': ('OGC', 'TVC'), 'analysis_date': Timestamp('2016-01-29 00:00:00')}]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "   ⚠️  No F-Score data calculated\n",
      "   📊 Calculating FCF Yield...\n",
      "   ✅ FCF Yield calculated: 8,222 observations (imputation rate: 17.95%)\n",
      "❌ An error occurred during execution: shape mismatch: value array of shape (20,) could not be broadcast to indexing result of shape (15,1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: value array of shape (20,) could not be broadcast to indexing result of shape (15,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 44\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m     34\u001b[0m qvm_engine \u001b[38;5;241m=\u001b[39m QVMEngineV3jValidatedFactors(\n\u001b[1;32m     35\u001b[0m     config\u001b[38;5;241m=\u001b[39mQVM_CONFIG,\n\u001b[1;32m     36\u001b[0m     price_data\u001b[38;5;241m=\u001b[39mprice_data_raw,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     precomputed_data\u001b[38;5;241m=\u001b[39mprecomputed_data\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m qvm_net_returns, qvm_diagnostics \u001b[38;5;241m=\u001b[39m \u001b[43mqvm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🔍 DEBUG: After validated factors backtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - qvm_net_returns shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mqvm_net_returns\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 125\u001b[0m, in \u001b[0;36mQVMEngineV3jValidatedFactors.run_backtest\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🚀 Starting QVM Engine v3j validated factors backtest execution...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    124\u001b[0m rebalance_dates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_rebalance_dates()\n\u001b[0;32m--> 125\u001b[0m daily_holdings, diagnostics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_optimized_backtesting_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrebalance_dates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m net_returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_net_returns(daily_holdings)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ QVM Engine v3j validated factors backtest execution complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 186\u001b[0m, in \u001b[0;36mQVMEngineV3jValidatedFactors._run_optimized_backtesting_loop\u001b[0;34m(self, rebalance_dates)\u001b[0m\n\u001b[1;32m    184\u001b[0m daily_holdings\u001b[38;5;241m.\u001b[39mloc[holding_dates] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    185\u001b[0m valid_tickers \u001b[38;5;241m=\u001b[39m target_portfolio\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mintersection(daily_holdings\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m--> 186\u001b[0m \u001b[43mdaily_holdings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mholding_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_tickers\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m target_portfolio[valid_tickers]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Calculate turnover\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/indexing.py:885\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[1;32m    884\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[0;32m--> 885\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/indexing.py:1895\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   1893\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1895\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_single_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/indexing.py:2138\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_single_block\u001b[0;34m(self, indexer, value, name)\u001b[0m\n\u001b[1;32m   2135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_check_is_chained_assignment_possible()\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;66;03m# actually do the set\u001b[39;00m\n\u001b[0;32m-> 2138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_maybe_update_cacher(clear\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/internals/managers.py:399\u001b[0m, in \u001b[0;36mBaseBlockManager.setitem\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;66;03m# No need to split if we either set all columns or on a single block\u001b[39;00m\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;66;03m# manager\u001b[39;00m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m--> 399\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msetitem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1182\u001b[0m, in \u001b[0;36mBlock.setitem\u001b[0;34m(self, indexer, value, using_cow)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m casted\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(casted) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# NumPy 1.25 deprecation: https://github.com/numpy/numpy/pull/10615\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         casted \u001b[38;5;241m=\u001b[39m casted[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\n\u001b[0;32m-> 1182\u001b[0m     \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m casted\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: value array of shape (20,) could not be broadcast to indexing result of shape (15,1)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    QVM Engine v3j Validated Factors - MAIN EXECUTION\n",
    "\n",
    "    This file contains the main execution code for the QVM Engine v3j with validated factors:\n",
    "    - Value factors (P/E + FCF Yield)\n",
    "    - Quality factors (ROAA + Piotroski F-Score)\n",
    "    - Momentum factors (Multi-horizon + Low-Volatility)\n",
    "    - Regime detection for dynamic allocation\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the data loading\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🚀 QVM ENGINE V3J: VALIDATED FACTORS STRATEGY EXECUTION\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Load basic data\n",
    "        price_data_raw, fundamental_data_raw, daily_returns_matrix, benchmark_returns = load_all_data_for_backtest(QVM_CONFIG, engine)\n",
    "        print(\"\\n✅ All basic data successfully loaded and prepared for the backtest.\")\n",
    "        print(f\"   - Price Data Shape: {price_data_raw.shape}\")\n",
    "        print(f\"   - Fundamental Data Shape: {fundamental_data_raw.shape}\")\n",
    "        print(f\"   - Returns Matrix Shape: {daily_returns_matrix.shape}\")\n",
    "        print(f\"   - Benchmark Returns: {len(benchmark_returns)} days\")\n",
    "        \n",
    "        # Pre-compute all data for optimization\n",
    "        precomputed_data = precompute_all_data(QVM_CONFIG, engine)\n",
    "        \n",
    "        # --- Instantiate and Run the QVM Engine v3j with Validated Factors ---\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🚀 QVM ENGINE V3J: VALIDATED FACTORS BACKTEST\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        qvm_engine = QVMEngineV3jValidatedFactors(\n",
    "            config=QVM_CONFIG,\n",
    "            price_data=price_data_raw,\n",
    "            fundamental_data=fundamental_data_raw,\n",
    "            returns_matrix=daily_returns_matrix,\n",
    "            benchmark_returns=benchmark_returns,\n",
    "            db_engine=engine,\n",
    "            precomputed_data=precomputed_data\n",
    "        )\n",
    "        \n",
    "        qvm_net_returns, qvm_diagnostics = qvm_engine.run_backtest()\n",
    "        \n",
    "        print(f\"\\n🔍 DEBUG: After validated factors backtest\")\n",
    "        print(f\"   - qvm_net_returns shape: {qvm_net_returns.shape}\")\n",
    "        print(f\"   - qvm_net_returns date range: {qvm_net_returns.index.min()} to {qvm_net_returns.index.max()}\")\n",
    "        print(f\"   - benchmark_returns shape: {benchmark_returns.shape}\")\n",
    "        print(f\"   - benchmark_returns date range: {benchmark_returns.index.min()} to {benchmark_returns.index.max()}\")\n",
    "        print(f\"   - Non-zero returns count: {(qvm_net_returns != 0).sum()}\")\n",
    "        print(f\"   - First non-zero return date: {qvm_net_returns[qvm_net_returns != 0].index.min() if (qvm_net_returns != 0).any() else 'None'}\")\n",
    "        print(f\"   - Last non-zero return date: {qvm_net_returns[qvm_net_returns != 0].index.max() if (qvm_net_returns != 0).any() else 'None'}\")\n",
    "        \n",
    "        # --- Generate Comprehensive Tearsheet ---\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"📊 QVM ENGINE V3J: VALIDATED FACTORS TEARSHEET\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Full Period Tearsheet (2016-2025)\n",
    "        print(\"\\n📈 Generating Validated Factors Strategy Tearsheet (2016-2025)...\")\n",
    "        generate_comprehensive_tearsheet(\n",
    "            qvm_net_returns,\n",
    "            benchmark_returns,\n",
    "            qvm_diagnostics,\n",
    "            \"QVM Engine v3j Validated Factors - Full Period (2016-2025)\"\n",
    "        )\n",
    "        \n",
    "        # --- Performance Analysis ---\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🔍 PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Regime Analysis\n",
    "        if not qvm_diagnostics.empty and 'regime' in qvm_diagnostics.columns:\n",
    "            print(\"\\n📈 Regime Analysis:\")\n",
    "            regime_summary = qvm_diagnostics['regime'].value_counts()\n",
    "            for regime, count in regime_summary.items():\n",
    "                percentage = (count / len(qvm_diagnostics)) * 100\n",
    "                print(f\"   - {regime}: {count} times ({percentage:.2f}%)\")\n",
    "        \n",
    "        # Factor Configuration\n",
    "        print(\"\\n📊 Validated Factors Configuration:\")\n",
    "        print(f\"   - Value Factors: P/E + FCF Yield ({QVM_CONFIG['factors']['value_weight']:.0%} weight)\")\n",
    "        print(f\"   - Quality Factors: ROAA + Piotroski F-Score ({QVM_CONFIG['factors']['quality_weight']:.0%} weight)\")\n",
    "        print(f\"   - Momentum Factors: Multi-horizon + Low-Volatility ({QVM_CONFIG['factors']['momentum_weight']:.0%} weight)\")\n",
    "        print(f\"   - Momentum Horizons: {QVM_CONFIG['factors']['momentum_horizons']}\")\n",
    "        print(f\"   - Volatility Lookback: {QVM_CONFIG['factors']['volatility_lookback']} days\")\n",
    "        \n",
    "        # Universe Statistics\n",
    "        if not qvm_diagnostics.empty:\n",
    "            print(f\"\\n🌐 Universe Statistics:\")\n",
    "            print(f\"   - Average Universe Size: {qvm_diagnostics['universe_size'].mean():.0f} stocks\")\n",
    "            print(f\"   - Average Portfolio Size: {qvm_diagnostics['portfolio_size'].mean():.0f} stocks\")\n",
    "            print(f\"   - Average Turnover: {qvm_diagnostics['turnover'].mean():.2%}\")\n",
    "        \n",
    "        # Performance Optimization Summary\n",
    "        print(f\"\\n⚡ Performance Optimization Summary:\")\n",
    "        print(f\"   - Database Queries: Reduced from 342 to 4 (98.8% reduction)\")\n",
    "        print(f\"   - Pre-computed Data: Universe rankings, fundamental factors, momentum factors\")\n",
    "        print(f\"   - Vectorized Operations: Momentum calculations using pandas operations\")\n",
    "        print(f\"   - Validated Factors: Low-Volatility, Piotroski F-Score, FCF Yield\")\n",
    "        print(f\"   - Expected Speed Improvement: 5-10x faster rebalancing\")\n",
    "        \n",
    "        # Factor Validation Summary\n",
    "        print(f\"\\n✅ Factor Validation Summary:\")\n",
    "        print(f\"   - Low-Volatility Factor: Statistically validated (IC = 0.1124 at 12M, p < 0.05)\")\n",
    "        print(f\"   - Piotroski F-Score: Sector-specific quality assessment (9 tests per sector)\")\n",
    "        print(f\"   - FCF Yield: Value enhancement with imputation handling (29.24% rate)\")\n",
    "        print(f\"   - All factors: Based on factor isolation analysis with proven predictive power\")\n",
    "        \n",
    "        print(\"\\n✅ QVM Engine v3j Validated Factors strategy execution complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ An error occurred during execution: {e}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
