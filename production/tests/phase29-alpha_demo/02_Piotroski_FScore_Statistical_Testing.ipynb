{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48bf138",
   "metadata": {},
   "source": [
    "# Piotroski F-Score Factor Statistical Significance Testing\n",
    "\n",
    "**Objective:** Test the statistical significance of the Piotroski F-Score factor as a quality enhancement in the QVM v2.1 Alpha strategy.\n",
    "\n",
    "**Factor Description:** \n",
    "- Sector-specific F-Score implementations (Non-Financial: 9 tests, Banking: 6 tests, Securities: 5 tests)\n",
    "- Normalized scores (Raw_Score/Max_Possible_Score) to prevent sector bias\n",
    "- Quality enhancement to prevent value traps\n",
    "\n",
    "**Testing Period:** 2018-2025 (excluding 2016-2017 OOS period)\n",
    "**Target Metrics:** Information Coefficient (IC), Factor Returns, Rank Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbf5c0",
   "metadata": {},
   "source": [
    "# IMPORTS AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "424d4287",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Piotroski F-Score Factor Testing Started: 2025-08-04 00:17:02.662399\n",
      "QVM Engine v2 Enhanced - F-Score Statistical Analysis\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the necessary paths to import modules\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '..', '..', 'engine'))\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '..', '..', 'universe'))\n",
    "\n",
    "from qvm_engine_v2_enhanced import QVMEngineV2Enhanced\n",
    "from constructors import get_liquid_universe\n",
    "\n",
    "print(f\"Piotroski F-Score Factor Testing Started: {datetime.now()}\")\n",
    "print(\"QVM Engine v2 Enhanced - F-Score Statistical Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e7074a",
   "metadata": {},
   "source": [
    "# STATISTICAL FUNCTIONS (NUMPY-BASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f4b39ef4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def spearman_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate Spearman's rank correlation coefficient using numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    - x, y: arrays of values\n",
    "    \n",
    "    Returns:\n",
    "    - float: Spearman's rho\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate ranks\n",
    "    x_ranks = pd.Series(x).rank()\n",
    "    y_ranks = pd.Series(y).rank()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    \n",
    "    # Pearson correlation of ranks\n",
    "    x_mean = x_ranks.mean()\n",
    "    y_mean = y_ranks.mean()\n",
    "    \n",
    "    numerator = np.sum((x_ranks - x_mean) * (y_ranks - y_mean))\n",
    "    denominator = np.sqrt(np.sum((x_ranks - x_mean)**2) * np.sum((y_ranks - y_mean)**2))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def t_test_one_sample(data, mu=0):\n",
    "    \"\"\"\n",
    "    Perform one-sample t-test using numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: array of values\n",
    "    - mu: hypothesized mean (default 0)\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: (t_statistic, p_value)\n",
    "    \"\"\"\n",
    "    if len(data) < 2:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    sample_mean = np.mean(data)\n",
    "    sample_std = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    n = len(data)\n",
    "    \n",
    "    if sample_std == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    t_stat = (sample_mean - mu) / (sample_std / np.sqrt(n))\n",
    "    \n",
    "    # Approximate p-value using normal distribution for large samples\n",
    "    # For small samples, this is an approximation\n",
    "    if n > 30:\n",
    "        # Use normal approximation\n",
    "        p_value = 2 * (1 - 0.5 * (1 + np.math.erf(abs(t_stat) / np.sqrt(2))))\n",
    "    else:\n",
    "        # For small samples, use a simplified approximation\n",
    "        # This is not exact but gives reasonable results\n",
    "        p_value = 2 * (1 - 0.5 * (1 + np.math.erf(abs(t_stat) / np.sqrt(2))))\n",
    "    \n",
    "    return t_stat, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3b5f18",
   "metadata": {},
   "source": [
    "# DATABASE CONNECTION AND ENGINE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5864a9c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:17:02,798 - EnhancedCanonicalQVMEngine - INFO - Initializing Enhanced Canonical QVM Engine\n",
      "2025-08-04 00:17:02,798 - EnhancedCanonicalQVMEngine - INFO - Initializing Enhanced Canonical QVM Engine\n",
      "2025-08-04 00:17:02,798 - EnhancedCanonicalQVMEngine - INFO - Initializing Enhanced Canonical QVM Engine\n",
      "2025-08-04 00:17:02,798 - EnhancedCanonicalQVMEngine - INFO - Initializing Enhanced Canonical QVM Engine\n",
      "2025-08-04 00:17:02,959 - EnhancedCanonicalQVMEngine - INFO - Enhanced configurations loaded successfully\n",
      "2025-08-04 00:17:02,959 - EnhancedCanonicalQVMEngine - INFO - Enhanced configurations loaded successfully\n",
      "2025-08-04 00:17:02,959 - EnhancedCanonicalQVMEngine - INFO - Enhanced configurations loaded successfully\n",
      "2025-08-04 00:17:02,959 - EnhancedCanonicalQVMEngine - INFO - Enhanced configurations loaded successfully\n",
      "2025-08-04 00:17:03,021 - EnhancedCanonicalQVMEngine - INFO - Database connection established successfully\n",
      "2025-08-04 00:17:03,021 - EnhancedCanonicalQVMEngine - INFO - Database connection established successfully\n",
      "2025-08-04 00:17:03,021 - EnhancedCanonicalQVMEngine - INFO - Database connection established successfully\n",
      "2025-08-04 00:17:03,021 - EnhancedCanonicalQVMEngine - INFO - Database connection established successfully\n",
      "2025-08-04 00:17:03,022 - EnhancedCanonicalQVMEngine - INFO - Enhanced components initialized successfully\n",
      "2025-08-04 00:17:03,022 - EnhancedCanonicalQVMEngine - INFO - Enhanced components initialized successfully\n",
      "2025-08-04 00:17:03,022 - EnhancedCanonicalQVMEngine - INFO - Enhanced components initialized successfully\n",
      "2025-08-04 00:17:03,022 - EnhancedCanonicalQVMEngine - INFO - Enhanced components initialized successfully\n",
      "2025-08-04 00:17:03,026 - EnhancedCanonicalQVMEngine - INFO - Enhanced Canonical QVM Engine initialized successfully\n",
      "2025-08-04 00:17:03,026 - EnhancedCanonicalQVMEngine - INFO - Enhanced Canonical QVM Engine initialized successfully\n",
      "2025-08-04 00:17:03,026 - EnhancedCanonicalQVMEngine - INFO - Enhanced Canonical QVM Engine initialized successfully\n",
      "2025-08-04 00:17:03,026 - EnhancedCanonicalQVMEngine - INFO - Enhanced Canonical QVM Engine initialized successfully\n",
      "2025-08-04 00:17:03,030 - EnhancedCanonicalQVMEngine - INFO - QVM Weights: Quality 40.0%, Value 30.0%, Momentum 30.0%\n",
      "2025-08-04 00:17:03,030 - EnhancedCanonicalQVMEngine - INFO - QVM Weights: Quality 40.0%, Value 30.0%, Momentum 30.0%\n",
      "2025-08-04 00:17:03,030 - EnhancedCanonicalQVMEngine - INFO - QVM Weights: Quality 40.0%, Value 30.0%, Momentum 30.0%\n",
      "2025-08-04 00:17:03,030 - EnhancedCanonicalQVMEngine - INFO - QVM Weights: Quality 40.0%, Value 30.0%, Momentum 30.0%\n",
      "2025-08-04 00:17:03,032 - EnhancedCanonicalQVMEngine - INFO - Enhanced Features: Multi-tier Quality, Enhanced EV/EBITDA, Sector-specific weights, Working capital efficiency\n",
      "2025-08-04 00:17:03,032 - EnhancedCanonicalQVMEngine - INFO - Enhanced Features: Multi-tier Quality, Enhanced EV/EBITDA, Sector-specific weights, Working capital efficiency\n",
      "2025-08-04 00:17:03,032 - EnhancedCanonicalQVMEngine - INFO - Enhanced Features: Multi-tier Quality, Enhanced EV/EBITDA, Sector-specific weights, Working capital efficiency\n",
      "2025-08-04 00:17:03,032 - EnhancedCanonicalQVMEngine - INFO - Enhanced Features: Multi-tier Quality, Enhanced EV/EBITDA, Sector-specific weights, Working capital efficiency\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ QVM Engine v2 Enhanced initialized successfully\n",
      "   - Engine class: QVMEngineV2Enhanced\n",
      "   - Database connection: ✅ Connected\n"
     ]
    }
   ],
   "source": [
    "# Initialize the QVM engine\n",
    "engine = QVMEngineV2Enhanced()\n",
    "\n",
    "print(\"✅ QVM Engine v2 Enhanced initialized successfully\")\n",
    "print(f\"   - Engine class: {engine.__class__.__name__}\")\n",
    "print(f\"   - Database connection: {'✅ Connected' if hasattr(engine, 'engine') and engine.engine else '❌ Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec1cdc",
   "metadata": {},
   "source": [
    "# DATABASE SCHEMA CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8340117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking database schema for required columns...\n",
      "============================================================\n",
      "Error checking database schema: %i format: a real number is required, not dict\n"
     ]
    }
   ],
   "source": [
    "# Check what columns are available in the database\n",
    "print(\"Checking database schema for required columns...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Check what tables are available\n",
    "    tables_query = \"SHOW TABLES LIKE '%intermediary%'\"\n",
    "    tables_result = pd.read_sql(tables_query, engine.engine)\n",
    "    \n",
    "    print(f\"Available intermediary tables:\")\n",
    "    for table in tables_result.iloc[:, 0]:\n",
    "        print(f\"  - {table}\")\n",
    "    \n",
    "    # Check columns in vcsc_daily_data_complete table\n",
    "    schema_query = \"DESCRIBE vcsc_daily_data_complete\"\n",
    "    schema_result = pd.read_sql(schema_query, engine.engine)\n",
    "    \n",
    "    print(f\"\\nAvailable columns in vcsc_daily_data_complete table:\")\n",
    "    print(f\"Total columns: {len(schema_result)}\")\n",
    "    \n",
    "    # Check for required columns for F-Score calculation\n",
    "    required_columns = {\n",
    "        'non_financial': ['roa', 'cfo', 'total_assets', 'total_equity', 'current_assets', \n",
    "                         'current_liabilities', 'gross_profit', 'revenue', 'total_shares'],\n",
    "        'banking': ['roa', 'nim', 'total_assets', 'total_equity', 'net_interest_income',\n",
    "                   'total_interest_expense', 'non_performing_loans', 'total_loans'],\n",
    "        'securities': ['roa', 'brokerage_income', 'total_revenue', 'total_assets', 'trading_volume']\n",
    "    }\n",
    "    \n",
    "    available_columns = schema_result['Field'].tolist()\n",
    "    \n",
    "    for sector, columns in required_columns.items():\n",
    "        print(f\"\\n{sector.upper().replace('_', ' ')} SECTOR REQUIRED COLUMNS:\")\n",
    "        missing_columns = []\n",
    "        available_count = 0\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in available_columns:\n",
    "                print(f\"  ✅ {col}\")\n",
    "                available_count += 1\n",
    "            else:\n",
    "                print(f\"  ❌ {col} - MISSING\")\n",
    "                missing_columns.append(col)\n",
    "        \n",
    "        print(f\"  Available: {available_count}/{len(columns)} columns\")\n",
    "        if missing_columns:\n",
    "            print(f\"  Missing columns: {missing_columns}\")\n",
    "    \n",
    "    # Show sample of available columns\n",
    "    print(f\"\\nSample of available columns in vcsc_daily_data_complete:\")\n",
    "    for i, col in enumerate(available_columns[:20]):\n",
    "        print(f\"  {i+1:2d}. {col}\")\n",
    "    if len(available_columns) > 20:\n",
    "        print(f\"  ... and {len(available_columns) - 20} more columns\")\n",
    "        \n",
    "    # Check if intermediary tables exist and have the required data\n",
    "    print(f\"\\nChecking intermediary tables for financial data...\")\n",
    "    intermediary_tables = ['intermediary_calculations_enhanced', 'intermediary_calculations_banking', 'intermediary_calculations_securities']\n",
    "    \n",
    "    for table in intermediary_tables:\n",
    "        try:\n",
    "            check_query = f\"SELECT COUNT(*) as count FROM {table} LIMIT 1\"\n",
    "            check_result = pd.read_sql(check_query, engine.engine)\n",
    "            print(f\"  ✅ {table}: {check_result.iloc[0]['count']} records available\")\n",
    "            \n",
    "            # Check columns in this table\n",
    "            table_schema_query = f\"DESCRIBE {table}\"\n",
    "            table_schema = pd.read_sql(table_schema_query, engine.engine)\n",
    "            print(f\"    Columns: {len(table_schema)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ {table}: {str(e)[:50]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error checking database schema: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19601d2a",
   "metadata": {},
   "source": [
    "# UNIVERSE CONSTRUCTION BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee0b6a5e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Period: 2018-01-01 to 2025-08-02\n",
      "Number of analysis dates: 91\n",
      "Testing with comprehensive sector-specific tickers:\n",
      "  Banking: 21 tickers\n",
      "  Securities: 26 tickers\n",
      "  Non-Financial: 60 tickers\n",
      "\n",
      "Attempting to get sector tickers from database...\n",
      "✅ Using 21 banking tickers from database\n",
      "✅ Using 26 securities tickers from database\n",
      "✅ Using 667 non-financial tickers from database\n"
     ]
    }
   ],
   "source": [
    "# Set up test parameters\n",
    "start_date = datetime(2018, 1, 1)\n",
    "end_date = datetime(2025, 8, 2)\n",
    "analysis_dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "print(f\"Analysis Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Number of analysis dates: {len(analysis_dates)}\")\n",
    "\n",
    "# Define comprehensive sector-specific tickers from the codebase\n",
    "BANKING_TICKERS = [\n",
    "    'VCB', 'TCB', 'BID', 'CTG', 'VPB', 'TPB', 'MBB', 'STB', 'HDB', 'ACB', \n",
    "    'SHB', 'EIB', 'MSB', 'OCB', 'LPB', 'KLB', 'NVB', 'PGB', 'VIB', 'NAB', 'BAB'\n",
    "]\n",
    "\n",
    "SECURITIES_TICKERS = [\n",
    "    'SSI', 'VCI', 'VND', 'HCM', 'BSI', 'SHS', 'MBS', 'FTS', 'VIG', 'TVS',\n",
    "    'AGR', 'VDS', 'PSI', 'APS', 'IVS', 'BVS', 'CTS', 'DSC', 'EVS', 'ORS',\n",
    "    'TCI', 'VFS', 'WSS', 'ASP', 'VIX', 'CSI'\n",
    "]\n",
    "\n",
    "# Non-Financial tickers (representative sample from major sectors)\n",
    "NON_FINANCIAL_TICKERS = [\n",
    "    # Real Estate\n",
    "    'VIC', 'VHM', 'NLG', 'DXG', 'KDH', 'NVL', 'PDR', 'CEO', 'FLC', 'HQC',\n",
    "    # Food & Beverage\n",
    "    'VNM', 'SAB', 'MSN', 'MCH', 'KDC', 'BHN', 'TAC', 'VCF', 'VAF', 'HAG',\n",
    "    # Construction Materials\n",
    "    'HPG', 'HSG', 'NKG', 'GVR', 'TMS', 'VGS', 'VCS', 'VCA', 'VCM', 'VCI',\n",
    "    # Technology\n",
    "    'FPT', 'CMG', 'ELC', 'VNG', 'VGI', 'VHC', 'VHT', 'VIC', 'VJC', 'VKD',\n",
    "    # Retail\n",
    "    'MWG', 'PNJ', 'DGW', 'FPT', 'VJC', 'VKD', 'VKG', 'VKH', 'VKI', 'VKJ',\n",
    "    # Utilities\n",
    "    'POW', 'GAS', 'REE', 'DPM', 'DGC', 'TCH', 'VRE', 'VJC', 'HVN', 'ACV'\n",
    "]\n",
    "\n",
    "print(f\"Testing with comprehensive sector-specific tickers:\")\n",
    "print(f\"  Banking: {len(BANKING_TICKERS)} tickers\")\n",
    "print(f\"  Securities: {len(SECURITIES_TICKERS)} tickers\")\n",
    "print(f\"  Non-Financial: {len(NON_FINANCIAL_TICKERS)} tickers\")\n",
    "\n",
    "# Function to get tickers by sector from database (alternative approach)\n",
    "def get_tickers_by_sector_from_db(engine, sector_name):\n",
    "    \"\"\"\n",
    "    Get tickers for a specific sector from the database.\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: QVMEngineV2Enhanced instance\n",
    "    - sector_name: 'Banking', 'Securities', or 'Non-Financial'\n",
    "    \n",
    "    Returns:\n",
    "    - list: ticker symbols for the sector\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if sector_name == 'Banking':\n",
    "            query = \"\"\"\n",
    "            SELECT ticker FROM master_info \n",
    "            WHERE sector = 'Banks' AND ticker IS NOT NULL\n",
    "            \"\"\"\n",
    "        elif sector_name == 'Securities':\n",
    "            query = \"\"\"\n",
    "            SELECT ticker FROM master_info \n",
    "            WHERE sector = 'Securities' AND ticker IS NOT NULL\n",
    "            \"\"\"\n",
    "        else:  # Non-Financial\n",
    "            query = \"\"\"\n",
    "            SELECT ticker FROM master_info \n",
    "            WHERE sector NOT IN ('Banks', 'Securities', 'Insurance', 'Other Financial')\n",
    "            AND ticker IS NOT NULL\n",
    "            \"\"\"\n",
    "        \n",
    "        result = pd.read_sql(query, engine.engine)\n",
    "        return result['ticker'].tolist()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get {sector_name} tickers from database: {e}\")\n",
    "        # Return hardcoded lists as fallback\n",
    "        if sector_name == 'Banking':\n",
    "            return BANKING_TICKERS\n",
    "        elif sector_name == 'Securities':\n",
    "            return SECURITIES_TICKERS\n",
    "        else:\n",
    "            return NON_FINANCIAL_TICKERS\n",
    "\n",
    "# Try to get tickers from database first, fallback to hardcoded lists\n",
    "print(\"\\nAttempting to get sector tickers from database...\")\n",
    "try:\n",
    "    db_banking_tickers = get_tickers_by_sector_from_db(engine, 'Banking')\n",
    "    db_securities_tickers = get_tickers_by_sector_from_db(engine, 'Securities')\n",
    "    db_non_financial_tickers = get_tickers_by_sector_from_db(engine, 'Non-Financial')\n",
    "    \n",
    "    if db_banking_tickers:\n",
    "        BANKING_TICKERS = db_banking_tickers\n",
    "        print(f\"✅ Using {len(BANKING_TICKERS)} banking tickers from database\")\n",
    "    if db_securities_tickers:\n",
    "        SECURITIES_TICKERS = db_securities_tickers\n",
    "        print(f\"✅ Using {len(SECURITIES_TICKERS)} securities tickers from database\")\n",
    "    if db_non_financial_tickers:\n",
    "        NON_FINANCIAL_TICKERS = db_non_financial_tickers\n",
    "        print(f\"✅ Using {len(NON_FINANCIAL_TICKERS)} non-financial tickers from database\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Using hardcoded ticker lists: {e}\")\n",
    "    print(f\"  Banking: {len(BANKING_TICKERS)} tickers\")\n",
    "    print(f\"  Securities: {len(SECURITIES_TICKERS)} tickers\")\n",
    "    print(f\"  Non-Financial: {len(NON_FINANCIAL_TICKERS)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318be275",
   "metadata": {},
   "source": [
    "# PIOTROSKI F-SCORE FACTOR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a0d6187",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_f_score_non_financial(engine, analysis_date, universe_tickers):\n",
    "    \"\"\"\n",
    "    Calculate Piotroski F-Score for non-financial companies (9 tests).\n",
    "    \n",
    "    Tests:\n",
    "    1. ROA > 0\n",
    "    2. CFO > 0  \n",
    "    3. Change in ROA > 0\n",
    "    4. Accruals < CFO\n",
    "    5. Change in Leverage < 0\n",
    "    6. Change in Current Ratio > 0\n",
    "    7. No Share Issuance\n",
    "    8. Change in Gross Margin > 0\n",
    "    9. Change in Asset Turnover > 0\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {ticker: normalized_f_score}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f_scores = {}\n",
    "        \n",
    "        # Get financial data from intermediary table\n",
    "        ticker_str = \"', '\".join(universe_tickers)\n",
    "        \n",
    "        # Get current year and quarter\n",
    "        current_year = analysis_date.year\n",
    "        current_quarter = (analysis_date.month - 1) // 3 + 1\n",
    "        \n",
    "        # Query for current financial metrics from intermediary table\n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            NetProfit_TTM,\n",
    "            NetCFO_TTM,\n",
    "            AvgTotalAssets,\n",
    "            AvgTotalEquity,\n",
    "            AvgCurrentAssets,\n",
    "            AvgCurrentLiabilities,\n",
    "            GrossProfit_TTM,\n",
    "            Revenue_TTM,\n",
    "            SharesOutstanding\n",
    "        FROM intermediary_calculations_enhanced\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND year = {current_year}\n",
    "          AND quarter = {current_quarter}\n",
    "        \"\"\"\n",
    "        \n",
    "        current_data = pd.read_sql(query, engine.engine)\n",
    "        \n",
    "        if current_data.empty:\n",
    "            return f_scores\n",
    "        \n",
    "        # Get previous year data for comparisons\n",
    "        prev_year = current_year - 1\n",
    "        prev_query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            NetProfit_TTM,\n",
    "            NetCFO_TTM,\n",
    "            AvgTotalAssets,\n",
    "            AvgTotalEquity,\n",
    "            AvgCurrentAssets,\n",
    "            AvgCurrentLiabilities,\n",
    "            GrossProfit_TTM,\n",
    "            Revenue_TTM,\n",
    "            SharesOutstanding\n",
    "        FROM intermediary_calculations_enhanced\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND year = {prev_year}\n",
    "          AND quarter = {current_quarter}\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_data = pd.read_sql(prev_query, engine.engine)\n",
    "        \n",
    "        # Merge current and previous data\n",
    "        merged_data = current_data.merge(prev_data, on='ticker', suffixes=('_curr', '_prev'))\n",
    "        \n",
    "        for _, row in merged_data.iterrows():\n",
    "            ticker = row['ticker']\n",
    "            score = 0\n",
    "            max_score = 9  # 9 tests for non-financial\n",
    "            \n",
    "            # Calculate ROA (Net Profit / Average Total Assets)\n",
    "            curr_roa = row['NetProfit_TTM_curr'] / row['AvgTotalAssets_curr'] if row['AvgTotalAssets_curr'] > 0 else 0\n",
    "            prev_roa = row['NetProfit_TTM_prev'] / row['AvgTotalAssets_prev'] if row['AvgTotalAssets_prev'] > 0 else 0\n",
    "            \n",
    "            # Test 1: ROA > 0\n",
    "            if curr_roa > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 2: CFO > 0\n",
    "            if row['NetCFO_TTM_curr'] > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 3: Change in ROA > 0\n",
    "            if curr_roa > prev_roa:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 4: Accruals < CFO (simplified)\n",
    "            if row['NetCFO_TTM_curr'] > 0:  # Simplified test\n",
    "                score += 1\n",
    "            \n",
    "            # Test 5: Change in Leverage < 0\n",
    "            curr_leverage = row['AvgTotalAssets_curr'] / row['AvgTotalEquity_curr'] if row['AvgTotalEquity_curr'] > 0 else 0\n",
    "            prev_leverage = row['AvgTotalAssets_prev'] / row['AvgTotalEquity_prev'] if row['AvgTotalEquity_prev'] > 0 else 0\n",
    "            if curr_leverage < prev_leverage:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 6: Change in Current Ratio > 0\n",
    "            curr_ratio = row['AvgCurrentAssets_curr'] / row['AvgCurrentLiabilities_curr'] if row['AvgCurrentLiabilities_curr'] > 0 else 0\n",
    "            prev_ratio = row['AvgCurrentAssets_prev'] / row['AvgCurrentLiabilities_prev'] if row['AvgCurrentLiabilities_prev'] > 0 else 0\n",
    "            if curr_ratio > prev_ratio:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 7: No Share Issuance\n",
    "            curr_shares = row['SharesOutstanding_curr'] if pd.notna(row['SharesOutstanding_curr']) else 0\n",
    "            prev_shares = row['SharesOutstanding_prev'] if pd.notna(row['SharesOutstanding_prev']) else 0\n",
    "            if curr_shares <= prev_shares:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 8: Change in Gross Margin > 0\n",
    "            curr_gm = row['GrossProfit_TTM_curr'] / row['Revenue_TTM_curr'] if row['Revenue_TTM_curr'] > 0 else 0\n",
    "            prev_gm = row['GrossProfit_TTM_prev'] / row['Revenue_TTM_prev'] if row['Revenue_TTM_prev'] > 0 else 0\n",
    "            if curr_gm > prev_gm:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 9: Change in Asset Turnover > 0\n",
    "            curr_at = row['Revenue_TTM_curr'] / row['AvgTotalAssets_curr'] if row['AvgTotalAssets_curr'] > 0 else 0\n",
    "            prev_at = row['Revenue_TTM_prev'] / row['AvgTotalAssets_prev'] if row['AvgTotalAssets_prev'] > 0 else 0\n",
    "            if curr_at > prev_at:\n",
    "                score += 1\n",
    "            \n",
    "            # Normalize score to 0-1 range\n",
    "            normalized_score = score / max_score\n",
    "            f_scores[ticker] = normalized_score\n",
    "        \n",
    "        return f_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate non-financial F-Score for {analysis_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def calculate_f_score_banking(engine, analysis_date, universe_tickers):\n",
    "    \"\"\"\n",
    "    Calculate Piotroski F-Score for banking companies (6 tests).\n",
    "    \n",
    "    Tests:\n",
    "    1. ROA > 0\n",
    "    2. NIM > 0\n",
    "    3. Change in ROA > 0\n",
    "    4. Change in Leverage < 0\n",
    "    5. Change in Efficiency Ratio > 0\n",
    "    6. Change in Asset Quality > 0\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {ticker: normalized_f_score}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f_scores = {}\n",
    "        \n",
    "        # Get banking-specific financial data from intermediary table\n",
    "        ticker_str = \"', '\".join(universe_tickers)\n",
    "        \n",
    "        # Get current year and quarter\n",
    "        current_year = analysis_date.year\n",
    "        current_quarter = (analysis_date.month - 1) // 3 + 1\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            NetProfit_TTM,\n",
    "            AvgTotalAssets,\n",
    "            AvgTotalEquity,\n",
    "            InterestExpense_TTM,\n",
    "            NIM,\n",
    "            OperatingProfit_TTM\n",
    "        FROM intermediary_calculations_banking\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND year = {current_year}\n",
    "          AND quarter = {current_quarter}\n",
    "        \"\"\"\n",
    "        \n",
    "        current_data = pd.read_sql(query, engine.engine)\n",
    "        \n",
    "        if current_data.empty:\n",
    "            return f_scores\n",
    "        \n",
    "        # Get previous year data\n",
    "        prev_year = current_year - 1\n",
    "        prev_query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            NetProfit_TTM,\n",
    "            AvgTotalAssets,\n",
    "            AvgTotalEquity,\n",
    "            InterestExpense_TTM,\n",
    "            NIM,\n",
    "            OperatingProfit_TTM\n",
    "        FROM intermediary_calculations_banking\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND year = {prev_year}\n",
    "          AND quarter = {current_quarter}\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_data = pd.read_sql(prev_query, engine.engine)\n",
    "        \n",
    "        # Merge data\n",
    "        merged_data = current_data.merge(prev_data, on='ticker', suffixes=('_curr', '_prev'))\n",
    "        \n",
    "        for _, row in merged_data.iterrows():\n",
    "            ticker = row['ticker']\n",
    "            score = 0\n",
    "            max_score = 6  # 6 tests for banking\n",
    "            \n",
    "            # Calculate ROA (Net Profit / Average Total Assets)\n",
    "            curr_roa = row['NetProfit_TTM_curr'] / row['AvgTotalAssets_curr'] if row['AvgTotalAssets_curr'] > 0 else 0\n",
    "            prev_roa = row['NetProfit_TTM_prev'] / row['AvgTotalAssets_prev'] if row['AvgTotalAssets_prev'] > 0 else 0\n",
    "            \n",
    "            # Test 1: ROA > 0\n",
    "            if curr_roa > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 2: NIM > 0\n",
    "            if row['NIM_curr'] > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 3: Change in ROA > 0\n",
    "            if curr_roa > prev_roa:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 4: Change in Leverage < 0\n",
    "            curr_leverage = row['AvgTotalAssets_curr'] / row['AvgTotalEquity_curr'] if row['AvgTotalEquity_curr'] > 0 else 0\n",
    "            prev_leverage = row['AvgTotalAssets_prev'] / row['AvgTotalEquity_prev'] if row['AvgTotalEquity_prev'] > 0 else 0\n",
    "            if curr_leverage < prev_leverage:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 5: Change in Efficiency Ratio > 0 (simplified)\n",
    "            curr_expense = row['InterestExpense_TTM_curr'] if pd.notna(row['InterestExpense_TTM_curr']) else 0\n",
    "            prev_expense = row['InterestExpense_TTM_prev'] if pd.notna(row['InterestExpense_TTM_prev']) else 0\n",
    "            if curr_expense < prev_expense:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 6: Change in Asset Quality > 0 (using Operating Profit as proxy)\n",
    "            if row['OperatingProfit_TTM_curr'] > row['OperatingProfit_TTM_prev']:\n",
    "                score += 1\n",
    "            \n",
    "            # Normalize score\n",
    "            normalized_score = score / max_score\n",
    "            f_scores[ticker] = normalized_score\n",
    "        \n",
    "        return f_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate banking F-Score for {analysis_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def calculate_f_score_securities(engine, analysis_date, universe_tickers):\n",
    "    \"\"\"\n",
    "    Calculate Piotroski F-Score for securities companies (5 tests).\n",
    "    \n",
    "    Tests:\n",
    "    1. ROA > 0\n",
    "    2. Brokerage Ratio > 0\n",
    "    3. Change in ROA > 0\n",
    "    4. Change in Efficiency > 0\n",
    "    5. Change in Trading Volume > 0\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {ticker: normalized_f_score}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f_scores = {}\n",
    "        \n",
    "        # Get securities-specific financial data from intermediary table\n",
    "        ticker_str = \"', '\".join(universe_tickers)\n",
    "        \n",
    "        # Get current year and quarter\n",
    "        current_year = analysis_date.year\n",
    "        current_quarter = (analysis_date.month - 1) // 3 + 1\n",
    "        \n",
    "        query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            NetProfit_TTM,\n",
    "            TotalOperatingRevenue_TTM,\n",
    "            AvgTotalAssets,\n",
    "            BrokerageRevenue_TTM,\n",
    "            NetTradingIncome_TTM\n",
    "        FROM intermediary_calculations_securities\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND year = {current_year}\n",
    "          AND quarter = {current_quarter}\n",
    "        \"\"\"\n",
    "        \n",
    "        current_data = pd.read_sql(query, engine.engine)\n",
    "        \n",
    "        if current_data.empty:\n",
    "            return f_scores\n",
    "        \n",
    "        # Get previous year data\n",
    "        prev_year = current_year - 1\n",
    "        prev_query = f\"\"\"\n",
    "        SELECT \n",
    "            ticker,\n",
    "            NetProfit_TTM,\n",
    "            TotalOperatingRevenue_TTM,\n",
    "            AvgTotalAssets,\n",
    "            BrokerageRevenue_TTM,\n",
    "            NetTradingIncome_TTM\n",
    "        FROM intermediary_calculations_securities\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND year = {prev_year}\n",
    "          AND quarter = {current_quarter}\n",
    "        \"\"\"\n",
    "        \n",
    "        prev_data = pd.read_sql(prev_query, engine.engine)\n",
    "        \n",
    "        # Merge data\n",
    "        merged_data = current_data.merge(prev_data, on='ticker', suffixes=('_curr', '_prev'))\n",
    "        \n",
    "        for _, row in merged_data.iterrows():\n",
    "            ticker = row['ticker']\n",
    "            score = 0\n",
    "            max_score = 5  # 5 tests for securities\n",
    "            \n",
    "            # Calculate ROA (Net Profit / Average Total Assets)\n",
    "            curr_roa = row['NetProfit_TTM_curr'] / row['AvgTotalAssets_curr'] if row['AvgTotalAssets_curr'] > 0 else 0\n",
    "            prev_roa = row['NetProfit_TTM_prev'] / row['AvgTotalAssets_prev'] if row['AvgTotalAssets_prev'] > 0 else 0\n",
    "            \n",
    "            # Test 1: ROA > 0\n",
    "            if curr_roa > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 2: Brokerage Ratio > 0\n",
    "            brokerage_ratio = row['BrokerageRevenue_TTM_curr'] / row['TotalOperatingRevenue_TTM_curr'] if row['TotalOperatingRevenue_TTM_curr'] > 0 else 0\n",
    "            if brokerage_ratio > 0:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 3: Change in ROA > 0\n",
    "            if curr_roa > prev_roa:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 4: Change in Efficiency > 0\n",
    "            curr_efficiency = row['TotalOperatingRevenue_TTM_curr'] / row['AvgTotalAssets_curr'] if row['AvgTotalAssets_curr'] > 0 else 0\n",
    "            prev_efficiency = row['TotalOperatingRevenue_TTM_prev'] / row['AvgTotalAssets_prev'] if row['AvgTotalAssets_prev'] > 0 else 0\n",
    "            if curr_efficiency > prev_efficiency:\n",
    "                score += 1\n",
    "            \n",
    "            # Test 5: Change in Trading Volume > 0 (using Net Trading Income as proxy)\n",
    "            if row['NetTradingIncome_TTM_curr'] > row['NetTradingIncome_TTM_prev']:\n",
    "                score += 1\n",
    "            \n",
    "            # Normalize score\n",
    "            normalized_score = score / max_score\n",
    "            f_scores[ticker] = normalized_score\n",
    "        \n",
    "        return f_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate securities F-Score for {analysis_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea2f582",
   "metadata": {},
   "source": [
    "# HISTORICAL FACTOR GENERATION BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c0878d74",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating historical F-Score data...\n",
      "Processing 2018-01-31... NF:552 B:14 S:26 \n",
      "Processing 2018-02-28... NF:552 B:14 S:26 \n",
      "Processing 2018-03-31... NF:552 B:14 S:26 \n",
      "Processing 2018-04-30... NF:562 B:15 S:26 \n",
      "Processing 2018-05-31... NF:562 B:15 S:26 \n",
      "Processing 2018-06-30... NF:562 B:15 S:26 \n",
      "Processing 2018-07-31... NF:582 B:19 S:26 \n",
      "Processing 2018-08-31... NF:582 B:19 S:26 \n",
      "Processing 2018-09-30... NF:582 B:19 S:26 \n",
      "Processing 2018-10-31... NF:582 B:19 S:26 \n",
      "Processing 2018-11-30... NF:582 B:19 S:26 \n",
      "Processing 2018-12-31... NF:582 B:19 S:26 \n",
      "Processing 2019-01-31... NF:594 B:19 S:26 \n",
      "Processing 2019-02-28... NF:594 B:19 S:26 \n",
      "Processing 2019-03-31... NF:594 B:19 S:26 \n",
      "Processing 2019-04-30... NF:607 B:19 S:26 \n",
      "Processing 2019-05-31... NF:607 B:19 S:26 \n",
      "Processing 2019-06-30... NF:607 B:19 S:26 \n",
      "Processing 2019-07-31... NF:624 B:21 S:26 \n",
      "Processing 2019-08-31... NF:624 B:21 S:26 \n",
      "Processing 2019-09-30... NF:624 B:21 S:26 \n",
      "Processing 2019-10-31... NF:624 B:21 S:26 \n",
      "Processing 2019-11-30... NF:624 B:21 S:26 \n",
      "Processing 2019-12-31... NF:624 B:21 S:26 \n",
      "Processing 2020-01-31... NF:628 B:21 S:26 \n",
      "Processing 2020-02-29... NF:628 B:21 S:26 \n",
      "Processing 2020-03-31... NF:628 B:21 S:26 \n",
      "Processing 2020-04-30... NF:630 B:21 S:26 \n",
      "Processing 2020-05-31... NF:630 B:21 S:26 \n",
      "Processing 2020-06-30... NF:630 B:21 S:26 \n",
      "Processing 2020-07-31... NF:635 B:21 S:26 \n",
      "Processing 2020-08-31... NF:635 B:21 S:26 \n",
      "Processing 2020-09-30... NF:635 B:21 S:26 \n",
      "Processing 2020-10-31... NF:635 B:21 S:26 \n",
      "Processing 2020-11-30... NF:635 B:21 S:26 \n",
      "Processing 2020-12-31... NF:635 B:21 S:26 \n",
      "Processing 2021-01-31... NF:639 B:21 S:26 \n",
      "Processing 2021-02-28... NF:639 B:21 S:26 \n",
      "Processing 2021-03-31... NF:639 B:21 S:26 \n",
      "Processing 2021-04-30... NF:643 B:21 S:26 \n",
      "Processing 2021-05-31... NF:643 B:21 S:26 \n",
      "Processing 2021-06-30... NF:643 B:21 S:26 \n",
      "Processing 2021-07-31... NF:644 B:21 S:26 \n",
      "Processing 2021-08-31... NF:644 B:21 S:26 \n",
      "Processing 2021-09-30... NF:644 B:21 S:26 \n",
      "Processing 2021-10-31... NF:644 B:21 S:26 \n",
      "Processing 2021-11-30... NF:644 B:21 S:26 \n",
      "Processing 2021-12-31... NF:644 B:21 S:26 \n",
      "Processing 2022-01-31... NF:648 B:21 S:26 \n",
      "Processing 2022-02-28... NF:648 B:21 S:26 \n",
      "Processing 2022-03-31... NF:648 B:21 S:26 \n",
      "Processing 2022-04-30... NF:651 B:21 S:26 \n",
      "Processing 2022-05-31... NF:651 B:21 S:26 \n",
      "Processing 2022-06-30... NF:651 B:21 S:26 \n",
      "Processing 2022-07-31... NF:655 B:21 S:26 \n",
      "Processing 2022-08-31... NF:655 B:21 S:26 \n",
      "Processing 2022-09-30... NF:655 B:21 S:26 \n",
      "Processing 2022-10-31... NF:655 B:21 S:26 \n",
      "Processing 2022-11-30... NF:655 B:21 S:26 \n",
      "Processing 2022-12-31... NF:655 B:21 S:26 \n",
      "Processing 2023-01-31... NF:658 B:21 S:26 \n",
      "Processing 2023-02-28... NF:658 B:21 S:26 \n",
      "Processing 2023-03-31... NF:658 B:21 S:26 \n",
      "Processing 2023-04-30... NF:660 B:21 S:26 \n",
      "Processing 2023-05-31... NF:660 B:21 S:26 \n",
      "Processing 2023-06-30... NF:660 B:21 S:26 \n",
      "Processing 2023-07-31... NF:665 B:21 S:26 \n",
      "Processing 2023-08-31... NF:665 B:21 S:26 \n",
      "Processing 2023-09-30... NF:665 B:21 S:26 \n",
      "Processing 2023-10-31... NF:662 B:21 S:26 \n",
      "Processing 2023-11-30... NF:662 B:21 S:26 \n",
      "Processing 2023-12-31... NF:662 B:21 S:26 \n",
      "Processing 2024-01-31... NF:659 B:21 S:26 \n",
      "Processing 2024-02-29... NF:659 B:21 S:26 \n",
      "Processing 2024-03-31... NF:659 B:21 S:26 \n",
      "Processing 2024-04-30... NF:654 B:21 S:26 \n",
      "Processing 2024-05-31... NF:654 B:21 S:26 \n",
      "Processing 2024-06-30... NF:654 B:21 S:26 \n",
      "Processing 2024-07-31... NF:652 B:21 S:26 \n",
      "Processing 2024-08-31... NF:652 B:21 S:26 \n",
      "Processing 2024-09-30... NF:652 B:21 S:26 \n",
      "Processing 2024-10-31... NF:650 B:21 S:26 \n",
      "Processing 2024-11-30... NF:650 B:21 S:26 \n",
      "Processing 2024-12-31... NF:650 B:21 S:26 \n",
      "Processing 2025-01-31... NF:637 B:21 S:26 \n",
      "Processing 2025-02-28... NF:637 B:21 S:26 \n",
      "Processing 2025-03-31... NF:637 B:21 S:26 \n",
      "Processing 2025-04-30... \n",
      "Processing 2025-05-31... \n",
      "Processing 2025-06-30... \n",
      "Processing 2025-07-31... \n",
      "\n",
      "✅ Historical F-Score data generated:\n",
      "  Non-Financial: 87 dates\n",
      "  Banking: 87 dates\n",
      "  Securities: 87 dates\n"
     ]
    }
   ],
   "source": [
    "# Generate historical F-Score data by sector\n",
    "historical_f_score = {\n",
    "    'non_financial': {},\n",
    "    'banking': {},\n",
    "    'securities': {}\n",
    "}\n",
    "\n",
    "print(\"Generating historical F-Score data...\")\n",
    "\n",
    "for date in analysis_dates:\n",
    "    print(f\"Processing {date.strftime('%Y-%m-%d')}...\", end=' ')\n",
    "    \n",
    "    # Non-Financial F-Score\n",
    "    nf_scores = calculate_f_score_non_financial(engine, date, NON_FINANCIAL_TICKERS)\n",
    "    if nf_scores:\n",
    "        historical_f_score['non_financial'][date] = nf_scores\n",
    "        print(f\"NF:{len(nf_scores)}\", end=' ')\n",
    "    \n",
    "    # Banking F-Score\n",
    "    banking_scores = calculate_f_score_banking(engine, date, BANKING_TICKERS)\n",
    "    if banking_scores:\n",
    "        historical_f_score['banking'][date] = banking_scores\n",
    "        print(f\"B:{len(banking_scores)}\", end=' ')\n",
    "    \n",
    "    # Securities F-Score\n",
    "    securities_scores = calculate_f_score_securities(engine, date, SECURITIES_TICKERS)\n",
    "    if securities_scores:\n",
    "        historical_f_score['securities'][date] = securities_scores\n",
    "        print(f\"S:{len(securities_scores)}\", end=' ')\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"\\n✅ Historical F-Score data generated:\")\n",
    "print(f\"  Non-Financial: {len(historical_f_score['non_financial'])} dates\")\n",
    "print(f\"  Banking: {len(historical_f_score['banking'])} dates\")\n",
    "print(f\"  Securities: {len(historical_f_score['securities'])} dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de28b44",
   "metadata": {},
   "source": [
    "# FORWARD RETURNS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7118b50f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_forward_returns(engine, analysis_date, universe_tickers, forward_periods=[1, 3, 6, 12]):\n",
    "    \"\"\"\n",
    "    Calculate forward returns for statistical testing.\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: QVMEngineV2Enhanced instance\n",
    "    - analysis_date: datetime for analysis\n",
    "    - universe_tickers: list of ticker symbols\n",
    "    - forward_periods: list of months for forward returns\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {ticker: {period: return}}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        forward_returns = {}\n",
    "        \n",
    "        # Get price data for forward return calculation\n",
    "        ticker_str = \"', '\".join(universe_tickers)\n",
    "        max_forward = max(forward_periods)\n",
    "        end_date = analysis_date + pd.DateOffset(months=max_forward)\n",
    "        \n",
    "        price_query = f\"\"\"\n",
    "        SELECT \n",
    "            date,\n",
    "            ticker,\n",
    "            close as adj_close\n",
    "        FROM equity_history\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND date BETWEEN '{analysis_date.date()}' AND '{end_date.date()}'\n",
    "        ORDER BY ticker, date\n",
    "        \"\"\"\n",
    "        \n",
    "        price_data = pd.read_sql(price_query, engine.engine, parse_dates=['date'])\n",
    "        \n",
    "        if price_data.empty:\n",
    "            return forward_returns\n",
    "        \n",
    "        # Calculate forward returns for each period\n",
    "        for ticker in universe_tickers:\n",
    "            ticker_data = price_data[price_data['ticker'] == ticker].sort_values('date')\n",
    "            if ticker_data.empty:\n",
    "                continue\n",
    "                \n",
    "            start_price = ticker_data.iloc[0]['adj_close']\n",
    "            forward_returns[ticker] = {}\n",
    "            \n",
    "            for period in forward_periods:\n",
    "                # Find price at period months later\n",
    "                period_date = analysis_date + pd.DateOffset(months=period)\n",
    "                period_data = ticker_data[ticker_data['date'] >= period_date]\n",
    "                \n",
    "                if not period_data.empty:\n",
    "                    end_price = period_data.iloc[0]['adj_close']\n",
    "                    forward_return = (end_price - start_price) / start_price\n",
    "                    forward_returns[ticker][period] = forward_return\n",
    "        \n",
    "        return forward_returns\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate forward returns for {analysis_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf4290",
   "metadata": {},
   "source": [
    "# STATISTICAL SIGNIFICANCE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d74e6015",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_information_coefficient(factor_scores, forward_returns, period):\n",
    "    \"\"\"\n",
    "    Calculate Information Coefficient (IC) for a given forward period.\n",
    "    \n",
    "    Parameters:\n",
    "    - factor_scores: dict of {ticker: score}\n",
    "    - forward_returns: dict of {ticker: {period: return}}\n",
    "    - period: forward period in months\n",
    "    \n",
    "    Returns:\n",
    "    - float: Information Coefficient\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    returns = []\n",
    "    \n",
    "    for ticker in factor_scores:\n",
    "        if ticker in forward_returns and period in forward_returns[ticker]:\n",
    "            scores.append(factor_scores[ticker])\n",
    "            returns.append(forward_returns[ticker][period])\n",
    "    \n",
    "    if len(scores) < 3:  # Need at least 3 observations\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate rank correlation (Spearman's rho)\n",
    "    ic = spearman_correlation(scores, returns)\n",
    "    return ic\n",
    "\n",
    "def calculate_factor_returns(factor_scores, forward_returns, period, n_quintiles=5):\n",
    "    \"\"\"\n",
    "    Calculate factor returns using quintile analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - factor_scores: dict of {ticker: score}\n",
    "    - forward_returns: dict of {ticker: {period: return}}\n",
    "    - period: forward period in months\n",
    "    - n_quintiles: number of quintiles for analysis\n",
    "    \n",
    "    Returns:\n",
    "    - dict: quintile returns and spread\n",
    "    \"\"\"\n",
    "    # Create DataFrame for analysis\n",
    "    data = []\n",
    "    for ticker in factor_scores:\n",
    "        if ticker in forward_returns and period in forward_returns[ticker]:\n",
    "            data.append({\n",
    "                'ticker': ticker,\n",
    "                'factor_score': factor_scores[ticker],\n",
    "                'forward_return': forward_returns[ticker][period]\n",
    "            })\n",
    "    \n",
    "    if len(data) < n_quintiles:\n",
    "        return {}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create quintiles\n",
    "    df['quintile'] = pd.qcut(df['factor_score'], n_quintiles, labels=False)\n",
    "    \n",
    "    # Calculate returns by quintile\n",
    "    quintile_returns = df.groupby('quintile')['forward_return'].mean()\n",
    "    \n",
    "    # Calculate spread (Q5 - Q1)\n",
    "    spread = quintile_returns.iloc[-1] - quintile_returns.iloc[0]\n",
    "    \n",
    "    return {\n",
    "        'quintile_returns': quintile_returns,\n",
    "        'spread': spread,\n",
    "        'high_low_spread': spread\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698613b",
   "metadata": {},
   "source": [
    "# COMPREHENSIVE STATISTICAL ANALYSIS BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c6767b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating forward returns by sector...\n",
      "✅ Forward returns calculated:\n",
      "  Non-Financial: 87 dates\n",
      "  Banking: 87 dates\n",
      "  Securities: 87 dates\n"
     ]
    }
   ],
   "source": [
    "# Calculate forward returns for all sectors\n",
    "print(\"Calculating forward returns by sector...\")\n",
    "historical_forward_returns = {\n",
    "    'non_financial': {},\n",
    "    'banking': {},\n",
    "    'securities': {}\n",
    "}\n",
    "\n",
    "# Non-Financial forward returns\n",
    "for date in list(historical_f_score['non_financial'].keys()):\n",
    "    forward_returns = calculate_forward_returns(engine, date, NON_FINANCIAL_TICKERS, [1, 3, 6, 12])\n",
    "    if forward_returns:\n",
    "        historical_forward_returns['non_financial'][date] = forward_returns\n",
    "\n",
    "# Banking forward returns\n",
    "for date in list(historical_f_score['banking'].keys()):\n",
    "    forward_returns = calculate_forward_returns(engine, date, BANKING_TICKERS, [1, 3, 6, 12])\n",
    "    if forward_returns:\n",
    "        historical_forward_returns['banking'][date] = forward_returns\n",
    "\n",
    "# Securities forward returns\n",
    "for date in list(historical_f_score['securities'].keys()):\n",
    "    forward_returns = calculate_forward_returns(engine, date, SECURITIES_TICKERS, [1, 3, 6, 12])\n",
    "    if forward_returns:\n",
    "        historical_forward_returns['securities'][date] = forward_returns\n",
    "\n",
    "print(f\"✅ Forward returns calculated:\")\n",
    "print(f\"  Non-Financial: {len(historical_forward_returns['non_financial'])} dates\")\n",
    "print(f\"  Banking: {len(historical_forward_returns['banking'])} dates\")\n",
    "print(f\"  Securities: {len(historical_forward_returns['securities'])} dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73165f34",
   "metadata": {},
   "source": [
    "# INFORMATION COEFFICIENT ANALYSIS BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "53b21ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Coefficient Analysis Results by Sector:\n",
      "================================================================================\n",
      "\n",
      "NON FINANCIAL SECTOR:\n",
      "----------------------------------------\n",
      "  1M Forward Period:\n",
      "    Mean IC: 0.0515\n",
      "    Std IC:  0.0601\n",
      "    t-stat:  7.9835\n",
      "    p-value: 0.0000\n",
      "    N:       87\n",
      "    Significant: ✅\n",
      "  3M Forward Period:\n",
      "    Mean IC: 0.0848\n",
      "    Std IC:  0.0729\n",
      "    t-stat:  10.8519\n",
      "    p-value: 0.0000\n",
      "    N:       87\n",
      "    Significant: ✅\n",
      "  6M Forward Period:\n",
      "    Mean IC: 0.0920\n",
      "    Std IC:  0.0729\n",
      "    t-stat:  11.5618\n",
      "    p-value: 0.0000\n",
      "    N:       84\n",
      "    Significant: ✅\n",
      "  12M Forward Period:\n",
      "    Mean IC: 0.0785\n",
      "    Std IC:  0.0790\n",
      "    t-stat:  7.0301\n",
      "    p-value: 0.0000\n",
      "    N:       50\n",
      "    Significant: ✅\n",
      "\n",
      "BANKING SECTOR:\n",
      "----------------------------------------\n",
      "  1M Forward Period:\n",
      "    Mean IC: 0.0391\n",
      "    Std IC:  0.2470\n",
      "    t-stat:  1.4766\n",
      "    p-value: 0.1421\n",
      "    N:       87\n",
      "    Significant: ❌\n",
      "  3M Forward Period:\n",
      "    Mean IC: 0.0344\n",
      "    Std IC:  0.2285\n",
      "    t-stat:  1.4023\n",
      "    p-value: 0.1632\n",
      "    N:       87\n",
      "    Significant: ❌\n",
      "  6M Forward Period:\n",
      "    Mean IC: 0.0389\n",
      "    Std IC:  0.2478\n",
      "    t-stat:  1.4385\n",
      "    p-value: 0.1527\n",
      "    N:       84\n",
      "    Significant: ❌\n",
      "  12M Forward Period:\n",
      "    Mean IC: 0.0334\n",
      "    Std IC:  0.2291\n",
      "    t-stat:  1.0324\n",
      "    p-value: 0.3068\n",
      "    N:       50\n",
      "    Significant: ❌\n",
      "\n",
      "SECURITIES SECTOR:\n",
      "----------------------------------------\n",
      "  1M Forward Period:\n",
      "    Mean IC: 0.0247\n",
      "    Std IC:  0.2338\n",
      "    t-stat:  0.9856\n",
      "    p-value: 0.3271\n",
      "    N:       87\n",
      "    Significant: ❌\n",
      "  3M Forward Period:\n",
      "    Mean IC: 0.0375\n",
      "    Std IC:  0.2252\n",
      "    t-stat:  1.5533\n",
      "    p-value: 0.1225\n",
      "    N:       87\n",
      "    Significant: ❌\n",
      "  6M Forward Period:\n",
      "    Mean IC: -0.0002\n",
      "    Std IC:  0.2059\n",
      "    t-stat:  -0.0069\n",
      "    p-value: 0.9945\n",
      "    N:       84\n",
      "    Significant: ❌\n",
      "  12M Forward Period:\n",
      "    Mean IC: -0.0491\n",
      "    Std IC:  0.2038\n",
      "    t-stat:  -1.7051\n",
      "    p-value: 0.0914\n",
      "    N:       50\n",
      "    Significant: ❌\n"
     ]
    }
   ],
   "source": [
    "# Calculate IC for different sectors and forward periods\n",
    "forward_periods = [1, 3, 6, 12]\n",
    "sectors = ['non_financial', 'banking', 'securities']\n",
    "ic_results = {sector: {period: [] for period in forward_periods} for sector in sectors}\n",
    "\n",
    "for sector in sectors:\n",
    "    for date in historical_f_score[sector]:\n",
    "        if date in historical_forward_returns[sector]:\n",
    "            for period in forward_periods:\n",
    "                ic = calculate_information_coefficient(\n",
    "                    historical_f_score[sector][date], \n",
    "                    historical_forward_returns[sector][date], \n",
    "                    period\n",
    "                )\n",
    "                if not np.isnan(ic):\n",
    "                    ic_results[sector][period].append(ic)\n",
    "\n",
    "# Calculate IC statistics by sector\n",
    "ic_stats = {}\n",
    "for sector in sectors:\n",
    "    ic_stats[sector] = {}\n",
    "    for period in forward_periods:\n",
    "        if ic_results[sector][period]:\n",
    "            ic_values = ic_results[sector][period]\n",
    "            ic_stats[sector][period] = {\n",
    "                'mean': np.mean(ic_values),\n",
    "                'std': np.std(ic_values),\n",
    "                't_stat': np.mean(ic_values) / (np.std(ic_values) / np.sqrt(len(ic_values))),\n",
    "                'p_value': t_test_one_sample(ic_values, 0)[1],\n",
    "                'count': len(ic_values)\n",
    "            }\n",
    "\n",
    "print(\"Information Coefficient Analysis Results by Sector:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sector in sectors:\n",
    "    print(f\"\\n{sector.upper().replace('_', ' ')} SECTOR:\")\n",
    "    print(\"-\" * 40)\n",
    "    for period in forward_periods:\n",
    "        if period in ic_stats[sector]:\n",
    "            stats = ic_stats[sector][period]\n",
    "            print(f\"  {period}M Forward Period:\")\n",
    "            print(f\"    Mean IC: {stats['mean']:.4f}\")\n",
    "            print(f\"    Std IC:  {stats['std']:.4f}\")\n",
    "            print(f\"    t-stat:  {stats['t_stat']:.4f}\")\n",
    "            print(f\"    p-value: {stats['p_value']:.4f}\")\n",
    "            print(f\"    N:       {stats['count']}\")\n",
    "            print(f\"    Significant: {'✅' if stats['p_value'] < 0.05 else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f772ba",
   "metadata": {},
   "source": [
    "# FACTOR RETURNS ANALYSIS BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "feff6fe3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Bin edges must be unique: array([0.11111111, 0.44444444, 0.55555556, 0.55555556, 0.77777778,\n       1.        ]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m historical_f_score[sector]:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01min\u001b[39;00m historical_forward_returns[sector]:\n\u001b[0;32m---> 12\u001b[0m         returns \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_factor_returns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhistorical_f_score\u001b[49m\u001b[43m[\u001b[49m\u001b[43msector\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhistorical_forward_returns\u001b[49m\u001b[43m[\u001b[49m\u001b[43msector\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperiod\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m returns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m returns:\n\u001b[1;32m     18\u001b[0m             period_returns\u001b[38;5;241m.\u001b[39mappend(returns[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[50], line 57\u001b[0m, in \u001b[0;36mcalculate_factor_returns\u001b[0;34m(factor_scores, forward_returns, period, n_quintiles)\u001b[0m\n\u001b[1;32m     54\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Create quintiles\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquintile\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfactor_score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_quintiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Calculate returns by quintile\u001b[39;00m\n\u001b[1;32m     60\u001b[0m quintile_returns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquintile\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforward_return\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/reshape/tile.py:379\u001b[0m, in \u001b[0;36mqcut\u001b[0;34m(x, q, labels, retbins, precision, duplicates)\u001b[0m\n\u001b[1;32m    376\u001b[0m x_np \u001b[38;5;241m=\u001b[39m x_np[\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(x_np)]\n\u001b[1;32m    377\u001b[0m bins \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mquantile(x_np, quantiles)\n\u001b[0;32m--> 379\u001b[0m fac, bins \u001b[38;5;241m=\u001b[39m \u001b[43m_bins_to_cuts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_lowest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mduplicates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mduplicates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _postprocess_for_cut(fac, bins, retbins, dtype, original)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310_env/lib/python3.10/site-packages/pandas/core/reshape/tile.py:421\u001b[0m, in \u001b[0;36m_bins_to_cuts\u001b[0;34m(x, bins, right, labels, precision, include_lowest, dtype, duplicates, ordered)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_bins) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(bins) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m duplicates \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 421\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBin edges must be unique: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(bins)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can drop duplicate edges by setting the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduplicates\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m kwarg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m     bins \u001b[38;5;241m=\u001b[39m unique_bins\n\u001b[1;32m    427\u001b[0m side: Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m right \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Bin edges must be unique: array([0.11111111, 0.44444444, 0.55555556, 0.55555556, 0.77777778,\n       1.        ]).\nYou can drop duplicate edges by setting the 'duplicates' kwarg"
     ]
    }
   ],
   "source": [
    "# Calculate factor returns for different sectors and periods\n",
    "factor_returns_results = {}\n",
    "\n",
    "for sector in sectors:\n",
    "    factor_returns_results[sector] = {}\n",
    "    \n",
    "    for period in forward_periods:\n",
    "        period_returns = []\n",
    "        \n",
    "        for date in historical_f_score[sector]:\n",
    "            if date in historical_forward_returns[sector]:\n",
    "                returns = calculate_factor_returns(\n",
    "                    historical_f_score[sector][date],\n",
    "                    historical_forward_returns[sector][date],\n",
    "                    period\n",
    "                )\n",
    "                if returns and 'spread' in returns:\n",
    "                    period_returns.append(returns['spread'])\n",
    "        \n",
    "        if period_returns:\n",
    "            factor_returns_results[sector][period] = {\n",
    "                'mean_return': np.mean(period_returns),\n",
    "                'std_return': np.std(period_returns),\n",
    "                't_stat': np.mean(period_returns) / (np.std(period_returns) / np.sqrt(len(period_returns))),\n",
    "                'p_value': t_test_one_sample(period_returns, 0)[1],\n",
    "                'count': len(period_returns),\n",
    "                'returns': period_returns\n",
    "            }\n",
    "\n",
    "print(\"Factor Returns Analysis Results by Sector:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for sector in sectors:\n",
    "    print(f\"\\n{sector.upper().replace('_', ' ')} SECTOR:\")\n",
    "    print(\"-\" * 40)\n",
    "    for period in forward_periods:\n",
    "        if period in factor_returns_results[sector]:\n",
    "            results = factor_returns_results[sector][period]\n",
    "            print(f\"  {period}M Forward Period:\")\n",
    "            print(f\"    Mean Spread: {results['mean_return']:.4f}\")\n",
    "            print(f\"    Std Spread:  {results['std_return']:.4f}\")\n",
    "            print(f\"    t-stat:      {results['t_stat']:.4f}\")\n",
    "            print(f\"    p-value:     {results['p_value']:.4f}\")\n",
    "            print(f\"    N:           {results['count']}\")\n",
    "            print(f\"    Significant: {'✅' if results['p_value'] < 0.05 else '❌'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fcd4df",
   "metadata": {},
   "source": [
    "# VISUALIZATION OF RESULTS BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check if we have data to plot\n",
    "print(\"DEBUG: Checking data availability for visualization...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for sector in sectors:\n",
    "    print(f\"\\n{sector.upper().replace('_', ' ')} SECTOR:\")\n",
    "    print(f\"  Historical F-Score dates: {len(historical_f_score[sector])}\")\n",
    "    print(f\"  Historical Forward Returns dates: {len(historical_forward_returns[sector])}\")\n",
    "    \n",
    "    for period in [1, 3, 6, 12]:\n",
    "        ic_count = len(ic_results[sector][period]) if ic_results[sector][period] else 0\n",
    "        returns_count = len(factor_returns_results[sector][period]['returns']) if period in factor_returns_results[sector] else 0\n",
    "        print(f\"    {period}M Forward: IC data points = {ic_count}, Returns data points = {returns_count}\")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "fig.suptitle('Piotroski F-Score Factor Statistical Analysis by Sector', fontsize=16, fontweight='bold')\n",
    "\n",
    "sector_names = {\n",
    "    'non_financial': 'Non-Financial',\n",
    "    'banking': 'Banking', \n",
    "    'securities': 'Securities'\n",
    "}\n",
    "\n",
    "for i, sector in enumerate(sectors):\n",
    "    # Plot 1: IC Distribution\n",
    "    ax1 = axes[i, 0]\n",
    "    has_ic_data = False\n",
    "    for period in [1, 3, 6, 12]:\n",
    "        if ic_results[sector][period] and len(ic_results[sector][period]) > 0:\n",
    "            ax1.hist(ic_results[sector][period], alpha=0.6, label=f'{period}M', bins=15)\n",
    "            has_ic_data = True\n",
    "    \n",
    "    if has_ic_data:\n",
    "        ax1.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "        ax1.set_xlabel('Information Coefficient')\n",
    "        ax1.set_ylabel('Frequency')\n",
    "        ax1.set_title(f'{sector_names[sector]} - IC Distribution')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax1.text(0.5, 0.5, f'No IC data available\\nfor {sector_names[sector]}', \n",
    "                ha='center', va='center', transform=ax1.transAxes, fontsize=12)\n",
    "        ax1.set_title(f'{sector_names[sector]} - IC Distribution (No Data)')\n",
    "    \n",
    "    # Plot 2: Factor Returns Summary\n",
    "    ax2 = axes[i, 1]\n",
    "    periods = [p for p in [1, 3, 6, 12] if p in factor_returns_results[sector] and factor_returns_results[sector][p]['returns']]\n",
    "    \n",
    "    if periods:\n",
    "        means = [factor_returns_results[sector][p]['mean_return'] for p in periods]\n",
    "        stds = [factor_returns_results[sector][p]['std_return'] for p in periods]\n",
    "        colors = ['green' if factor_returns_results[sector][p]['p_value'] < 0.05 else 'red' for p in periods]\n",
    "        \n",
    "        bars = ax2.bar([str(p) + 'M' for p in periods], means, yerr=stds, capsize=5, color=colors, alpha=0.7)\n",
    "        ax2.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax2.set_xlabel('Forward Period')\n",
    "        ax2.set_ylabel('Mean Factor Return Spread')\n",
    "        ax2.set_title(f'{sector_names[sector]} - Factor Returns')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add significance annotations\n",
    "        for j, (period, results) in enumerate(factor_returns_results[sector].items()):\n",
    "            if results['p_value'] < 0.05:\n",
    "                ax2.text(j, means[j] + stds[j] + 0.001, '*', ha='center', va='bottom', fontsize=16, color='green')\n",
    "    else:\n",
    "        ax2.text(0.5, 0.5, f'No factor returns data\\navailable for {sector_names[sector]}', \n",
    "                ha='center', va='center', transform=ax2.transAxes, fontsize=12)\n",
    "        ax2.set_title(f'{sector_names[sector]} - Factor Returns (No Data)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c4d9fb",
   "metadata": {},
   "source": [
    "# DATA AVAILABILITY SUMMARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c54aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"DATA AVAILABILITY SUMMARY FOR VISUALIZATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_data_points = 0\n",
    "for sector in sectors:\n",
    "    print(f\"\\n{sector.upper().replace('_', ' ')} SECTOR:\")\n",
    "    sector_data_points = 0\n",
    "    \n",
    "    for period in [1, 3, 6, 12]:\n",
    "        ic_count = len(ic_results[sector][period]) if ic_results[sector][period] else 0\n",
    "        returns_count = len(factor_returns_results[sector][period]['returns']) if period in factor_returns_results[sector] else 0\n",
    "        sector_data_points += ic_count + returns_count\n",
    "        \n",
    "        print(f\"  {period}M Forward Period:\")\n",
    "        print(f\"    IC data points: {ic_count}\")\n",
    "        print(f\"    Returns data points: {returns_count}\")\n",
    "    \n",
    "    print(f\"  Total data points: {sector_data_points}\")\n",
    "    total_data_points += sector_data_points\n",
    "\n",
    "print(f\"\\nOVERALL SUMMARY:\")\n",
    "print(f\"  Total data points across all sectors: {total_data_points}\")\n",
    "if total_data_points == 0:\n",
    "    print(\"  ⚠️  NO DATA AVAILABLE - This is likely due to:\")\n",
    "    print(\"     - Database schema issues (missing columns like 'roa', 'cfo', etc.)\")\n",
    "    print(\"     - No financial data available for the specified dates\")\n",
    "    print(\"     - Database connection issues\")\n",
    "    print(\"  🔧 RECOMMENDATION: Check database schema and data availability\")\n",
    "else:\n",
    "    print(\"  ✅ Data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f07cba",
   "metadata": {},
   "source": [
    "# SUMMARY AND CONCLUSIONS BY SECTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af3eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 100)\n",
    "print(\"PIOTROSKI F-SCORE FACTOR STATISTICAL SIGNIFICANCE SUMMARY BY SECTOR\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "print(\"\\n📊 KEY FINDINGS BY SECTOR:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for sector in sectors:\n",
    "    print(f\"\\n{sector.upper().replace('_', ' ')} SECTOR:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # IC Analysis Summary\n",
    "    print(\"\\n1. INFORMATION COEFFICIENT ANALYSIS:\")\n",
    "    significant_ic_count = 0\n",
    "    for period in [1, 3, 6, 12]:\n",
    "        if period in ic_stats[sector]:\n",
    "            stats = ic_stats[sector][period]\n",
    "            significance = \"✅ STATISTICALLY SIGNIFICANT\" if stats['p_value'] < 0.05 else \"❌ NOT SIGNIFICANT\"\n",
    "            print(f\"   {period}M Forward: IC = {stats['mean']:.4f} (p = {stats['p_value']:.4f}) - {significance}\")\n",
    "            if stats['p_value'] < 0.05:\n",
    "                significant_ic_count += 1\n",
    "    \n",
    "    # Factor Returns Summary\n",
    "    print(\"\\n2. FACTOR RETURNS ANALYSIS:\")\n",
    "    significant_returns_count = 0\n",
    "    for period in [1, 3, 6, 12]:\n",
    "        if period in factor_returns_results[sector]:\n",
    "            results = factor_returns_results[sector][period]\n",
    "            significance = \"✅ STATISTICALLY SIGNIFICANT\" if results['p_value'] < 0.05 else \"❌ NOT SIGNIFICANT\"\n",
    "            print(f\"   {period}M Forward: Spread = {results['mean_return']:.4f} (p = {results['p_value']:.4f}) - {significance}\")\n",
    "            if results['p_value'] < 0.05:\n",
    "                significant_returns_count += 1\n",
    "    \n",
    "    # Sector Assessment\n",
    "    print(f\"\\n3. SECTOR ASSESSMENT:\")\n",
    "    print(f\"   - IC Significance: {significant_ic_count}/4 periods significant\")\n",
    "    print(f\"   - Returns Significance: {significant_returns_count}/4 periods significant\")\n",
    "    \n",
    "    if significant_ic_count >= 2 and significant_returns_count >= 2:\n",
    "        print(f\"   🎯 CONCLUSION: {sector_names[sector]} F-Score shows strong statistical significance\")\n",
    "        print(f\"   ✅ RECOMMENDATION: Include in QVM v2.1 Alpha strategy\")\n",
    "    else:\n",
    "        print(f\"   ⚠️ CONCLUSION: {sector_names[sector]} F-Score shows mixed statistical significance\")\n",
    "        print(f\"   🔍 RECOMMENDATION: Further analysis needed before inclusion\")\n",
    "\n",
    "# Overall Assessment\n",
    "print(f\"\\n\" + \"=\" * 100)\n",
    "print(\"OVERALL ASSESSMENT:\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "total_significant_sectors = 0\n",
    "for sector in sectors:\n",
    "    significant_ic = sum(1 for period in [1, 3, 6, 12] if period in ic_stats[sector] and ic_stats[sector][period]['p_value'] < 0.05)\n",
    "    significant_returns = sum(1 for period in [1, 3, 6, 12] if period in factor_returns_results[sector] and factor_returns_results[sector][period]['p_value'] < 0.05)\n",
    "    \n",
    "    if significant_ic >= 2 and significant_returns >= 2:\n",
    "        total_significant_sectors += 1\n",
    "\n",
    "print(f\"   - Sectors with strong significance: {total_significant_sectors}/{len(sectors)}\")\n",
    "print(f\"   - Overall F-Score effectiveness: {'✅ HIGH' if total_significant_sectors >= 2 else '⚠️ MIXED' if total_significant_sectors >= 1 else '❌ LOW'}\")\n",
    "\n",
    "if total_significant_sectors >= 2:\n",
    "    print(\"   🎯 FINAL CONCLUSION: Piotroski F-Score factor shows strong statistical significance across sectors\")\n",
    "    print(\"   ✅ FINAL RECOMMENDATION: Include in QVM v2.1 Alpha strategy with sector-specific implementation\")\n",
    "else:\n",
    "    print(\"   ⚠️ FINAL CONCLUSION: Piotroski F-Score factor shows limited statistical significance\")\n",
    "    print(\"   🔍 FINAL RECOMMENDATION: Further refinement needed before inclusion\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 100) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
