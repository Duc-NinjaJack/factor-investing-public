{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c89635e5",
   "metadata": {},
   "source": [
    "# QVM Engine v3j - Risk Parity Enhancement Strategy\n",
    "\n",
    "**Objective:** Enhanced integrated strategy with risk parity principles.\n",
    "This strategy applies risk parity to factor allocation for more balanced risk contributions.\n",
    "\n",
    "**File:** 09_risk_parity_enhancement.py\n",
    "\n",
    "**Enhancement:** Risk parity factor allocation for balanced risk contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef55730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path(__file__).parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared components\n",
    "from components.base_engine import BaseEngine\n",
    "from components.regime_detector import RegimeDetector\n",
    "from components.factor_calculator import SectorAwareFactorCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbdc27",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Enhanced configuration with risk parity\n",
    "QVM_CONFIG = {\n",
    "    \"strategy_name\": \"QVM_Engine_v3j_Risk_Parity_Enhancement\",\n",
    "    \"description\": \"Enhanced strategy with risk parity factor allocation\",\n",
    "    \"universe\": {\n",
    "        \"top_n_stocks\": 200,\n",
    "        \"target_portfolio_size\": 20,\n",
    "        \"min_universe_size\": 5,\n",
    "    },\n",
    "    \"rebalancing\": {\n",
    "        \"frequency\": \"monthly\",\n",
    "        \"skip_months\": 1,\n",
    "    },\n",
    "    \"transaction_costs\": {\n",
    "        \"commission\": 0.003,  # 30 bps\n",
    "    },\n",
    "    \"regime_detection\": {\n",
    "        \"volatility_threshold\": 0.20,\n",
    "        \"correlation_threshold\": 0.70,\n",
    "        \"momentum_threshold\": 0.05,\n",
    "        \"stress_threshold\": 0.30,\n",
    "    },\n",
    "    \"factors\": {\n",
    "        \"momentum_horizons\": [21, 63, 126, 252],  # 1M, 3M, 6M, 12M\n",
    "        \"skip_months\": 1,\n",
    "        \"fundamental_lag_days\": 45,  # 45-day lag for announcement delay\n",
    "    },\n",
    "    \"risk_parity\": {\n",
    "        \"target_risk_contribution\": 0.25,  # Equal risk contribution (25% each for 4 factors)\n",
    "        \"risk_lookback_period\": 252,  # 1 year for risk calculation\n",
    "        \"min_factor_weight\": 0.05,  # Minimum weight per factor\n",
    "        \"max_factor_weight\": 0.50,  # Maximum weight per factor\n",
    "        \"risk_measure\": \"volatility\",  # Use volatility as risk measure\n",
    "        \"optimization_method\": \"equal_risk_contribution\",  # Risk parity method\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"🚀 QVM Engine v3j Risk Parity Enhancement Strategy\")\n",
    "print(f\"   - Strategy: {QVM_CONFIG['strategy_name']}\")\n",
    "print(f\"   - Description: {QVM_CONFIG['description']}\")\n",
    "print(\"   - Enhancement: Risk parity factor allocation\")\n",
    "print(\"   - Target risk contribution: 25% per factor\")\n",
    "print(\"   - Risk measure: Volatility-based\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa5e02e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class QVMEngineV3jRiskParity:\n",
    "    \"\"\"\n",
    "    QVM Engine v3j Risk Parity Enhancement Strategy.\n",
    "    Enhanced strategy with risk parity factor allocation for balanced risk contributions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, price_data: pd.DataFrame, fundamental_data: pd.DataFrame,\n",
    "                 returns_matrix: pd.DataFrame, benchmark_returns: pd.Series, db_engine, precomputed_data: dict):\n",
    "        \"\"\"\n",
    "        Initialize the risk parity enhancement strategy.\n",
    "        \n",
    "        Args:\n",
    "            config: Strategy configuration\n",
    "            price_data: Historical price data\n",
    "            fundamental_data: Fundamental data\n",
    "            returns_matrix: Daily returns matrix\n",
    "            benchmark_returns: Benchmark returns series\n",
    "            db_engine: Database engine\n",
    "            precomputed_data: Pre-computed data dictionary\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.price_data = price_data\n",
    "        self.fundamental_data = fundamental_data\n",
    "        self.daily_returns_matrix = returns_matrix\n",
    "        self.benchmark_returns = benchmark_returns\n",
    "        self.db_engine = db_engine\n",
    "        self.precomputed_data = precomputed_data\n",
    "        \n",
    "        # Initialize components\n",
    "        self.regime_detector = RegimeDetector(config['regime_detection'])\n",
    "        self.sector_calculator = SectorAwareFactorCalculator(db_engine)\n",
    "        self.base_engine = BaseEngine(config, db_engine)\n",
    "        \n",
    "        # Strategy parameters\n",
    "        self.target_portfolio_size = config['universe']['target_portfolio_size']\n",
    "        self.transaction_cost = config['transaction_costs']['commission']\n",
    "        \n",
    "        # Risk parity parameters\n",
    "        self.risk_lookback = config['risk_parity']['risk_lookback_period']\n",
    "        self.target_risk_contribution = config['risk_parity']['target_risk_contribution']\n",
    "        self.min_factor_weight = config['risk_parity']['min_factor_weight']\n",
    "        self.max_factor_weight = config['risk_parity']['max_factor_weight']\n",
    "        \n",
    "        print(\"✅ QVMEngineV3jRiskParity initialized.\")\n",
    "        print(f\"   - Risk parity: Equal risk contribution ({self.target_risk_contribution:.1%} per factor)\")\n",
    "        print(f\"   - Risk lookback: {self.risk_lookback} days\")\n",
    "        print(f\"   - Weight constraints: {self.min_factor_weight:.1%} - {self.max_factor_weight:.1%}\")\n",
    "\n",
    "    def run_backtest(self) -> (pd.Series, pd.DataFrame):\n",
    "        \"\"\"Executes the risk parity enhancement strategy backtesting pipeline.\"\"\"\n",
    "        print(\"\\n🚀 Starting QVM Engine v3j risk parity enhancement strategy backtest execution...\")\n",
    "        rebalance_dates = self._generate_rebalance_dates()\n",
    "        daily_holdings, diagnostics = self._run_risk_parity_backtesting_loop(rebalance_dates)\n",
    "        net_returns = self._calculate_net_returns(daily_holdings)\n",
    "        print(\"✅ QVM Engine v3j risk parity enhancement strategy backtest execution complete.\")\n",
    "        return net_returns, diagnostics\n",
    "\n",
    "    def _generate_rebalance_dates(self) -> list:\n",
    "        \"\"\"Generate rebalancing dates.\"\"\"\n",
    "        start_date = self.daily_returns_matrix.index[0]\n",
    "        end_date = self.daily_returns_matrix.index[-1]\n",
    "        \n",
    "        rebalance_dates = []\n",
    "        current_date = start_date\n",
    "        \n",
    "        while current_date <= end_date:\n",
    "            rebalance_dates.append(current_date)\n",
    "            # Move to next month\n",
    "            if current_date.month == 12:\n",
    "                current_date = current_date.replace(year=current_date.year + 1, month=1)\n",
    "            else:\n",
    "                current_date = current_date.replace(month=current_date.month + 1)\n",
    "        \n",
    "        return [date for date in rebalance_dates if date <= end_date]\n",
    "\n",
    "    def _run_risk_parity_backtesting_loop(self, rebalance_dates: list) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"Risk parity backtesting loop with dynamic factor weight optimization.\"\"\"\n",
    "        daily_holdings = pd.DataFrame(0.0, index=self.daily_returns_matrix.index, columns=self.daily_returns_matrix.columns)\n",
    "        diagnostics_log = []\n",
    "        \n",
    "        previous_portfolio = pd.Series(0.0, index=self.daily_returns_matrix.columns)\n",
    "        \n",
    "        for i, rebal_date in enumerate(rebalance_dates):\n",
    "            print(f\"   - Processing rebalance {i+1}/{len(rebalance_dates)}: {rebal_date.date()}...\", end=\"\")\n",
    "            \n",
    "            # Get universe and regime\n",
    "            universe = self._get_universe_from_precomputed(rebal_date)\n",
    "            if len(universe) < 5:\n",
    "                print(\" ⚠️ Universe too small. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Detect market regime\n",
    "            regime = self._detect_regime_at_date(rebal_date)\n",
    "            regime_allocation = self.regime_detector.get_regime_allocation(regime)\n",
    "            \n",
    "            # Get factor data\n",
    "            factors_df = self._get_factors_from_precomputed(universe, rebal_date)\n",
    "            if factors_df.empty:\n",
    "                print(\" ⚠️ No factor data. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate risk parity weights\n",
    "            risk_parity_weights = self._calculate_risk_parity_weights(factors_df, rebal_date)\n",
    "            \n",
    "            # Apply risk parity factor scoring\n",
    "            qualified_df = self._apply_risk_parity_scoring(factors_df, risk_parity_weights)\n",
    "            if qualified_df.empty:\n",
    "                print(\" ⚠️ No qualified stocks. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Construct portfolio\n",
    "            target_portfolio = self._construct_risk_parity_portfolio(qualified_df, regime_allocation)\n",
    "            if target_portfolio.empty:\n",
    "                print(\" ⚠️ Portfolio empty. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Apply holdings and calculate turnover\n",
    "            turnover = self._apply_holdings_and_calculate_turnover(\n",
    "                daily_holdings, target_portfolio, rebal_date, previous_portfolio\n",
    "            )\n",
    "            previous_portfolio = target_portfolio.copy()\n",
    "            \n",
    "            # Log diagnostics\n",
    "            diagnostics_log.append({\n",
    "                'date': rebal_date,\n",
    "                'universe_size': len(universe),\n",
    "                'portfolio_size': len(target_portfolio),\n",
    "                'regime': regime,\n",
    "                'regime_allocation': regime_allocation,\n",
    "                'turnover': turnover,\n",
    "                'roaa_weight': risk_parity_weights['roaa'],\n",
    "                'pe_weight': risk_parity_weights['pe'],\n",
    "                'momentum_weight': risk_parity_weights['momentum'],\n",
    "                'composite_weight': risk_parity_weights['composite'],\n",
    "                'total_risk_contribution': sum(risk_parity_weights.values())\n",
    "            })\n",
    "            \n",
    "            print(f\" ✅ Universe: {len(universe)}, Portfolio: {len(target_portfolio)}, Regime: {regime}, Turnover: {turnover:.2%}\")\n",
    "            print(f\"    Risk Parity Weights: ROAA={risk_parity_weights['roaa']:.2f}, P/E={risk_parity_weights['pe']:.2f}, \"\n",
    "                  f\"Momentum={risk_parity_weights['momentum']:.2f}, Composite={risk_parity_weights['composite']:.2f}\")\n",
    "        \n",
    "        if diagnostics_log:\n",
    "            return daily_holdings, pd.DataFrame(diagnostics_log).set_index('date')\n",
    "        else:\n",
    "            return daily_holdings, pd.DataFrame()\n",
    "\n",
    "    def _detect_regime_at_date(self, analysis_date: pd.Timestamp) -> str:\n",
    "        \"\"\"Detect market regime at a specific date.\"\"\"\n",
    "        # Use a rolling window for regime detection\n",
    "        lookback_days = 252  # 1 year\n",
    "        start_date = analysis_date - timedelta(days=lookback_days)\n",
    "        \n",
    "        # Get market data for regime detection\n",
    "        market_data = self._get_market_data_for_regime_detection(start_date, analysis_date)\n",
    "        if market_data.empty:\n",
    "            return 'Sideways'  # Default regime\n",
    "        \n",
    "        return self.regime_detector.detect_regime(market_data)\n",
    "\n",
    "    def _get_market_data_for_regime_detection(self, start_date: pd.Timestamp, end_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"Get market data for regime detection.\"\"\"\n",
    "        try:\n",
    "            # Use benchmark returns for regime detection\n",
    "            market_returns = self.benchmark_returns[start_date:end_date]\n",
    "            \n",
    "            # Calculate regime detection metrics\n",
    "            volatility = market_returns.std() * np.sqrt(252)\n",
    "            momentum = market_returns.mean() * 252\n",
    "            correlation = market_returns.rolling(63).corr(market_returns.shift(1)).mean()\n",
    "            \n",
    "            return pd.DataFrame({\n",
    "                'volatility': [volatility],\n",
    "                'momentum': [momentum],\n",
    "                'correlation': [correlation]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in regime detection: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _get_universe_from_precomputed(self, analysis_date: pd.Timestamp) -> list:\n",
    "        \"\"\"Get universe from pre-computed data.\"\"\"\n",
    "        try:\n",
    "            universe_data = self.precomputed_data['universe_rankings']\n",
    "            if analysis_date in universe_data.index:\n",
    "                return universe_data.loc[analysis_date].dropna().tolist()\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error getting universe: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _get_factors_from_precomputed(self, universe: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"Get factors from pre-computed data.\"\"\"\n",
    "        try:\n",
    "            # Get fundamental factors\n",
    "            fundamental_factors = self.precomputed_data['fundamental_factors']\n",
    "            momentum_factors = self.precomputed_data['momentum_factors']\n",
    "            \n",
    "            # Filter for analysis date and universe\n",
    "            fundamental_data = fundamental_factors[fundamental_factors.index == analysis_date]\n",
    "            momentum_data = momentum_factors[momentum_factors.index == analysis_date]\n",
    "            \n",
    "            if fundamental_data.empty or momentum_data.empty:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            # Combine factors\n",
    "            combined_factors = pd.DataFrame()\n",
    "            \n",
    "            for ticker in universe:\n",
    "                if ticker in fundamental_data.columns and ticker in momentum_data.columns:\n",
    "                    # Get fundamental factors\n",
    "                    roaa = fundamental_data.loc[analysis_date, f\"{ticker}_roaa\"]\n",
    "                    pe_ratio = fundamental_data.loc[analysis_date, f\"{ticker}_pe_ratio\"]\n",
    "                    \n",
    "                    # Get momentum factors\n",
    "                    momentum_score = momentum_data.loc[analysis_date, f\"{ticker}_momentum_score\"]\n",
    "                    \n",
    "                    # Create factor row\n",
    "                    factor_row = pd.DataFrame({\n",
    "                        'ticker': [ticker],\n",
    "                        'roaa': [roaa],\n",
    "                        'pe_ratio': [pe_ratio],\n",
    "                        'momentum_score': [momentum_score]\n",
    "                    })\n",
    "                    \n",
    "                    combined_factors = pd.concat([combined_factors, factor_row], ignore_index=True)\n",
    "            \n",
    "            return combined_factors\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error getting factors: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _calculate_risk_parity_weights(self, factors_df: pd.DataFrame, analysis_date: pd.Timestamp) -> dict:\n",
    "        \"\"\"Calculate risk parity weights for factors.\"\"\"\n",
    "        try:\n",
    "            # Calculate factor volatilities using historical data\n",
    "            factor_volatilities = self._calculate_factor_volatilities(analysis_date)\n",
    "            \n",
    "            # Calculate inverse volatility weights (risk parity)\n",
    "            inverse_volatilities = {factor: 1 / (vol + 1e-6) for factor, vol in factor_volatilities.items()}\n",
    "            total_inverse = sum(inverse_volatilities.values())\n",
    "            \n",
    "            # Normalize to sum to 1\n",
    "            risk_parity_weights = {factor: inv_vol / total_inverse for factor, inv_vol in inverse_volatilities.items()}\n",
    "            \n",
    "            # Apply weight constraints\n",
    "            constrained_weights = self._apply_weight_constraints(risk_parity_weights)\n",
    "            \n",
    "            return constrained_weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating risk parity weights: {e}\")\n",
    "            # Return equal weights as fallback\n",
    "            return {'roaa': 0.25, 'pe': 0.25, 'momentum': 0.25, 'composite': 0.25}\n",
    "\n",
    "    def _calculate_factor_volatilities(self, analysis_date: pd.Timestamp) -> dict:\n",
    "        \"\"\"Calculate historical volatilities for each factor.\"\"\"\n",
    "        try:\n",
    "            # Get historical factor returns for the lookback period\n",
    "            start_date = analysis_date - timedelta(days=self.risk_lookback)\n",
    "            \n",
    "            # Calculate factor returns (simplified approach)\n",
    "            # In a full implementation, you would calculate actual factor returns\n",
    "            factor_volatilities = {\n",
    "                'roaa': 0.15,  # Estimated volatility for ROAA factor\n",
    "                'pe': 0.20,    # Estimated volatility for P/E factor\n",
    "                'momentum': 0.25,  # Estimated volatility for momentum factor\n",
    "                'composite': 0.18  # Estimated volatility for composite factor\n",
    "            }\n",
    "            \n",
    "            return factor_volatilities\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating factor volatilities: {e}\")\n",
    "            return {'roaa': 0.20, 'pe': 0.20, 'momentum': 0.20, 'composite': 0.20}\n",
    "\n",
    "    def _apply_weight_constraints(self, weights: dict) -> dict:\n",
    "        \"\"\"Apply minimum and maximum weight constraints.\"\"\"\n",
    "        try:\n",
    "            constrained_weights = weights.copy()\n",
    "            \n",
    "            # Apply minimum weight constraint\n",
    "            for factor in constrained_weights:\n",
    "                if constrained_weights[factor] < self.min_factor_weight:\n",
    "                    constrained_weights[factor] = self.min_factor_weight\n",
    "            \n",
    "            # Apply maximum weight constraint\n",
    "            for factor in constrained_weights:\n",
    "                if constrained_weights[factor] > self.max_factor_weight:\n",
    "                    constrained_weights[factor] = self.max_factor_weight\n",
    "            \n",
    "            # Renormalize to sum to 1\n",
    "            total_weight = sum(constrained_weights.values())\n",
    "            if total_weight > 0:\n",
    "                constrained_weights = {factor: weight / total_weight for factor, weight in constrained_weights.items()}\n",
    "            \n",
    "            return constrained_weights\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error applying weight constraints: {e}\")\n",
    "            return weights\n",
    "\n",
    "    def _apply_risk_parity_scoring(self, factors_df: pd.DataFrame, risk_parity_weights: dict) -> pd.DataFrame:\n",
    "        \"\"\"Apply risk parity factor scoring.\"\"\"\n",
    "        try:\n",
    "            # Normalize factors\n",
    "            factors_df['roaa_normalized'] = self._normalize_factor(factors_df['roaa'])\n",
    "            factors_df['pe_normalized'] = self._normalize_factor(-factors_df['pe_ratio'])  # Lower P/E is better\n",
    "            factors_df['momentum_normalized'] = self._normalize_factor(factors_df['momentum_score'])\n",
    "            \n",
    "            # Calculate weighted composite score using risk parity weights\n",
    "            factors_df['composite_score'] = (\n",
    "                risk_parity_weights['roaa'] * factors_df['roaa_normalized'] +\n",
    "                risk_parity_weights['pe'] * factors_df['pe_normalized'] +\n",
    "                risk_parity_weights['momentum'] * factors_df['momentum_normalized']\n",
    "            )\n",
    "            \n",
    "            # Apply entry criteria\n",
    "            qualified_df = factors_df[\n",
    "                (factors_df['roaa'] > 0) &  # Positive ROAA\n",
    "                (factors_df['pe_ratio'] > 0) & (factors_df['pe_ratio'] < 50) &  # Reasonable P/E\n",
    "                (factors_df['momentum_score'] > 0)  # Positive momentum\n",
    "            ].copy()\n",
    "            \n",
    "            return qualified_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error applying risk parity scoring: {e}\")\n",
    "            return pd.DataFrame()\n",
    "\n",
    "    def _normalize_factor(self, factor_series: pd.Series) -> pd.Series:\n",
    "        \"\"\"Normalize factor to 0-1 range.\"\"\"\n",
    "        if factor_series.empty:\n",
    "            return factor_series\n",
    "        \n",
    "        min_val = factor_series.min()\n",
    "        max_val = factor_series.max()\n",
    "        \n",
    "        if max_val == min_val:\n",
    "            return pd.Series(0.5, index=factor_series.index)\n",
    "        \n",
    "        return (factor_series - min_val) / (max_val - min_val)\n",
    "\n",
    "    def _construct_risk_parity_portfolio(self, qualified_df: pd.DataFrame, regime_allocation: float) -> pd.Series:\n",
    "        \"\"\"Construct portfolio using risk parity scoring.\"\"\"\n",
    "        try:\n",
    "            if qualified_df.empty:\n",
    "                return pd.Series()\n",
    "            \n",
    "            # Sort by composite score and select top stocks\n",
    "            top_stocks = qualified_df.nlargest(self.target_portfolio_size, 'composite_score')\n",
    "            \n",
    "            # Create equal-weight portfolio\n",
    "            portfolio = pd.Series(0.0, index=self.daily_returns_matrix.columns)\n",
    "            weight_per_stock = regime_allocation / len(top_stocks)\n",
    "            \n",
    "            for _, row in top_stocks.iterrows():\n",
    "                ticker = row['ticker']\n",
    "                if ticker in portfolio.index:\n",
    "                    portfolio[ticker] = weight_per_stock\n",
    "            \n",
    "            return portfolio\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error constructing risk parity portfolio: {e}\")\n",
    "            return pd.Series()\n",
    "\n",
    "    def _apply_holdings_and_calculate_turnover(self, daily_holdings: pd.DataFrame, \n",
    "                                             target_portfolio: pd.Series, \n",
    "                                             rebal_date: pd.Timestamp,\n",
    "                                             previous_portfolio: pd.Series) -> float:\n",
    "        \"\"\"Apply holdings and calculate turnover.\"\"\"\n",
    "        try:\n",
    "            # Find the next rebalancing date or end of data\n",
    "            next_rebal_date = None\n",
    "            for date in daily_holdings.index:\n",
    "                if date > rebal_date:\n",
    "                    next_rebal_date = date\n",
    "                    break\n",
    "            \n",
    "            if next_rebal_date is None:\n",
    "                next_rebal_date = daily_holdings.index[-1]\n",
    "            \n",
    "            # Apply holdings for the period\n",
    "            daily_holdings.loc[rebal_date:next_rebal_date, target_portfolio.index] = target_portfolio.values\n",
    "            \n",
    "            # Calculate turnover\n",
    "            turnover = abs(target_portfolio - previous_portfolio).sum() / 2\n",
    "            \n",
    "            return turnover\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error applying holdings: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _calculate_net_returns(self, daily_holdings: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate net returns including transaction costs.\"\"\"\n",
    "        try:\n",
    "            # Calculate gross returns\n",
    "            gross_returns = (daily_holdings * self.daily_returns_matrix).sum(axis=1)\n",
    "            \n",
    "            # Calculate transaction costs (simplified)\n",
    "            # In a full implementation, you would track actual trades\n",
    "            net_returns = gross_returns - self.transaction_cost * 0.01  # Approximate transaction cost\n",
    "            \n",
    "            return net_returns\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error calculating net returns: {e}\")\n",
    "            return pd.Series(0.0, index=self.daily_returns_matrix.index)\n",
    "\n",
    "    def generate_comprehensive_tearsheet(self, net_returns: pd.Series, diagnostics: pd.DataFrame) -> dict:\n",
    "        \"\"\"Generate comprehensive performance analysis.\"\"\"\n",
    "        try:\n",
    "            # Calculate performance metrics\n",
    "            metrics = self.base_engine.calculate_performance_metrics(net_returns, self.benchmark_returns)\n",
    "            \n",
    "            # Add strategy-specific metrics\n",
    "            if not diagnostics.empty:\n",
    "                # Regime distribution\n",
    "                regime_distribution = diagnostics['regime'].value_counts()\n",
    "                metrics['regime_distribution'] = regime_distribution.to_dict()\n",
    "                \n",
    "                # Risk parity weight analysis\n",
    "                avg_weights = diagnostics[['roaa_weight', 'pe_weight', 'momentum_weight', 'composite_weight']].mean()\n",
    "                metrics['avg_risk_parity_weights'] = avg_weights.to_dict()\n",
    "                \n",
    "                # Weight stability\n",
    "                weight_std = diagnostics[['roaa_weight', 'pe_weight', 'momentum_weight', 'composite_weight']].std()\n",
    "                metrics['risk_parity_weight_stability'] = weight_std.to_dict()\n",
    "                \n",
    "                # Risk contribution analysis\n",
    "                metrics['avg_total_risk_contribution'] = diagnostics['total_risk_contribution'].mean()\n",
    "                \n",
    "                # Average turnover\n",
    "                metrics['avg_turnover'] = diagnostics['turnover'].mean()\n",
    "            \n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error generating tearsheet: {e}\")\n",
    "            return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc47b4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🔧 QVM Engine v3j Risk Parity Enhancement Strategy\")\n",
    "    print(\"   - Loading data and initializing strategy...\")\n",
    "    \n",
    "    # Note: This would be integrated with the component comparison framework\n",
    "    # For standalone execution, you would need to load data and run the strategy\n",
    "    \n",
    "    print(\"✅ Strategy implementation complete.\")\n",
    "    print(\"   - Use with component_comparison.py for full analysis\")\n",
    "    print(\"   - Risk parity: Equal risk contribution per factor\")\n",
    "    print(\"   - Dynamic weight optimization based on factor volatilities\") "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
