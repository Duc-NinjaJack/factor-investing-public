{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455852c5",
   "metadata": {},
   "source": [
    "# Low-Volatility Factor Statistical Significance Testing\n",
    "\n",
    "**Objective:** Test the statistical significance of the Low-Volatility factor as a defensive overlay in the QVM v2.1 Alpha strategy.\n",
    "\n",
    "**Factor Description:** \n",
    "- Calculates 252-day rolling volatility for each stock\n",
    "- Converts to low-volatility scores (lower volatility = higher score)\n",
    "- Normalized to 0-1 range for defensive positioning\n",
    "\n",
    "**Testing Period:** 2018-2025 (excluding 2016-2017 OOS period)\n",
    "**Target Metrics:** Information Coefficient (IC), Factor Returns, Rank Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bd1076",
   "metadata": {},
   "source": [
    "# IMPORTS AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55cdddd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add the necessary paths to import modules\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '..', '..', 'engine'))\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), '..', '..', 'universe'))\n",
    "\n",
    "from qvm_engine_v2_enhanced import QVMEngineV2Enhanced\n",
    "from constructors import get_liquid_universe\n",
    "\n",
    "print(f\"Low-Volatility Factor Testing Started: {datetime.now()}\")\n",
    "print(\"QVM Engine v2 Enhanced - Low-Volatility Statistical Analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93aff60",
   "metadata": {},
   "source": [
    "# STATISTICAL FUNCTIONS (NUMPY-BASED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9802baf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def spearman_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate Spearman's rank correlation coefficient using numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    - x, y: arrays of values\n",
    "    \n",
    "    Returns:\n",
    "    - float: Spearman's rho\n",
    "    \"\"\"\n",
    "    if len(x) != len(y):\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate ranks\n",
    "    x_ranks = pd.Series(x).rank()\n",
    "    y_ranks = pd.Series(y).rank()\n",
    "    \n",
    "    # Calculate correlation\n",
    "    n = len(x)\n",
    "    if n < 3:\n",
    "        return np.nan\n",
    "    \n",
    "    # Pearson correlation of ranks\n",
    "    x_mean = x_ranks.mean()\n",
    "    y_mean = y_ranks.mean()\n",
    "    \n",
    "    numerator = np.sum((x_ranks - x_mean) * (y_ranks - y_mean))\n",
    "    denominator = np.sqrt(np.sum((x_ranks - x_mean)**2) * np.sum((y_ranks - y_mean)**2))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    return numerator / denominator\n",
    "\n",
    "def t_test_one_sample(data, mu=0):\n",
    "    \"\"\"\n",
    "    Perform one-sample t-test using numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: array of values\n",
    "    - mu: hypothesized mean (default 0)\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: (t_statistic, p_value)\n",
    "    \"\"\"\n",
    "    if len(data) < 2:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    sample_mean = np.mean(data)\n",
    "    sample_std = np.std(data, ddof=1)  # ddof=1 for sample standard deviation\n",
    "    n = len(data)\n",
    "    \n",
    "    if sample_std == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    t_stat = (sample_mean - mu) / (sample_std / np.sqrt(n))\n",
    "    \n",
    "    # Approximate p-value using normal distribution for large samples\n",
    "    # For small samples, this is an approximation\n",
    "    if n > 30:\n",
    "        # Use normal approximation\n",
    "        p_value = 2 * (1 - 0.5 * (1 + np.math.erf(abs(t_stat) / np.sqrt(2))))\n",
    "    else:\n",
    "        # For small samples, use a simplified approximation\n",
    "        # This is not exact but gives reasonable results\n",
    "        p_value = 2 * (1 - 0.5 * (1 + np.math.erf(abs(t_stat) / np.sqrt(2))))\n",
    "    \n",
    "    return t_stat, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9837b741",
   "metadata": {},
   "source": [
    "# DATABASE CONNECTION AND ENGINE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the QVM engine\n",
    "engine = QVMEngineV2Enhanced()\n",
    "\n",
    "print(\"‚úÖ QVM Engine v2 Enhanced initialized successfully\")\n",
    "print(f\"   - Engine class: {engine.__class__.__name__}\")\n",
    "print(f\"   - Database connection: {'‚úÖ Connected' if hasattr(engine, 'engine') and engine.engine else '‚ùå Failed'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f23df6",
   "metadata": {},
   "source": [
    "# UNIVERSE CONSTRUCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c697ab3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up test parameters\n",
    "start_date = datetime(2018, 1, 1)\n",
    "end_date = datetime(2025, 8, 2)\n",
    "analysis_dates = pd.date_range(start=start_date, end=end_date, freq='M')\n",
    "\n",
    "print(f\"Analysis Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Number of analysis dates: {len(analysis_dates)}\")\n",
    "\n",
    "# Define comprehensive universe of liquid stocks from the codebase\n",
    "UNIVERSE_TICKERS = [\n",
    "    # Banking (21 tickers)\n",
    "    'VCB', 'TCB', 'BID', 'CTG', 'VPB', 'TPB', 'MBB', 'STB', 'HDB', 'ACB', \n",
    "    'SHB', 'EIB', 'MSB', 'OCB', 'LPB', 'KLB', 'NVB', 'PGB', 'VIB', 'NAB', 'BAB',\n",
    "    \n",
    "    # Securities (26 tickers)\n",
    "    'SSI', 'VCI', 'VND', 'HCM', 'BSI', 'SHS', 'MBS', 'FTS', 'VIG', 'TVS',\n",
    "    'AGR', 'VDS', 'PSI', 'APS', 'IVS', 'BVS', 'CTS', 'DSC', 'EVS', 'ORS',\n",
    "    'TCI', 'VFS', 'WSS', 'ASP', 'VIX', 'CSI',\n",
    "    \n",
    "    # Real Estate\n",
    "    'VIC', 'VHM', 'NLG', 'DXG', 'KDH', 'NVL', 'PDR', 'CEO', 'FLC', 'HQC',\n",
    "    \n",
    "    # Food & Beverage\n",
    "    'VNM', 'SAB', 'MSN', 'MCH', 'KDC', 'BHN', 'TAC', 'VCF', 'VAF', 'HAG',\n",
    "    \n",
    "    # Construction Materials\n",
    "    'HPG', 'HSG', 'NKG', 'GVR', 'TMS', 'VGS', 'VCS', 'VCA', 'VCM', 'VCI',\n",
    "    \n",
    "    # Technology\n",
    "    'FPT', 'CMG', 'ELC', 'VNG', 'VGI', 'VHC', 'VHT', 'VIC', 'VJC', 'VKD',\n",
    "    \n",
    "    # Retail\n",
    "    'MWG', 'PNJ', 'DGW', 'FPT', 'VJC', 'VKD', 'VKG', 'VKH', 'VKI', 'VKJ',\n",
    "    \n",
    "    # Utilities\n",
    "    'POW', 'GAS', 'REE', 'DPM', 'DGC', 'TCH', 'VRE', 'VJC', 'HVN', 'ACV'\n",
    "]\n",
    "\n",
    "print(f\"Testing with {len(UNIVERSE_TICKERS)} comprehensive universe tickers\")\n",
    "\n",
    "# Function to get liquid universe from database (alternative approach)\n",
    "def get_liquid_universe_from_db(engine, analysis_date, top_n=200):\n",
    "    \"\"\"\n",
    "    Get liquid universe from database using the existing function.\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: QVMEngineV2Enhanced instance\n",
    "    - analysis_date: datetime for analysis\n",
    "    - top_n: maximum number of tickers to return\n",
    "    \n",
    "    Returns:\n",
    "    - list: ticker symbols for liquid universe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from production.database.utils import get_liquid_universe\n",
    "        \n",
    "        # Convert datetime to string format\n",
    "        date_str = analysis_date.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # Get liquid universe\n",
    "        liquid_df = get_liquid_universe(\n",
    "            analysis_date=date_str,\n",
    "            adtv_threshold=10.0,\n",
    "            lookback_days=63,\n",
    "            top_n=top_n,\n",
    "            min_trading_coverage=0.6,\n",
    "            engine=engine.engine\n",
    "        )\n",
    "        \n",
    "        if not liquid_df.empty:\n",
    "            return liquid_df['ticker'].tolist()\n",
    "        else:\n",
    "            return UNIVERSE_TICKERS\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get liquid universe from database: {e}\")\n",
    "        return UNIVERSE_TICKERS\n",
    "\n",
    "# Try to get liquid universe from database first, fallback to hardcoded list\n",
    "print(\"\\nAttempting to get liquid universe from database...\")\n",
    "try:\n",
    "    db_universe_tickers = get_liquid_universe_from_db(engine, start_date, top_n=200)\n",
    "    if db_universe_tickers:\n",
    "        UNIVERSE_TICKERS = db_universe_tickers\n",
    "        print(f\"‚úÖ Using {len(UNIVERSE_TICKERS)} tickers from liquid universe database\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Using hardcoded ticker list: {len(UNIVERSE_TICKERS)} tickers\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Using hardcoded ticker list: {e}\")\n",
    "    print(f\"  Universe: {len(UNIVERSE_TICKERS)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8c240f",
   "metadata": {},
   "source": [
    "# LOW-VOLATILITY FACTOR CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578aa85",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_low_volatility_factor(engine, analysis_date, universe_tickers):\n",
    "    \"\"\"\n",
    "    Calculate low-volatility factor scores for defensive overlay.\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: QVMEngineV2Enhanced instance\n",
    "    - analysis_date: datetime for analysis\n",
    "    - universe_tickers: list of ticker symbols\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {ticker: low_vol_score}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        low_vol_scores = {}\n",
    "        \n",
    "        # Get price data for volatility calculation\n",
    "        ticker_str = \"', '\".join(universe_tickers)\n",
    "        start_date = analysis_date - pd.DateOffset(months=12)  # 12 months for volatility calculation\n",
    "        \n",
    "        price_query = f\"\"\"\n",
    "        SELECT \n",
    "            date,\n",
    "            ticker,\n",
    "            close as adj_close\n",
    "        FROM equity_history\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND date BETWEEN '{start_date.date()}' AND '{analysis_date.date()}'\n",
    "        ORDER BY ticker, date\n",
    "        \"\"\"\n",
    "        \n",
    "        price_data = pd.read_sql(price_query, engine.engine, parse_dates=['date'])\n",
    "        \n",
    "        if price_data.empty:\n",
    "            print(f\"No price data available for {analysis_date.strftime('%Y-%m-%d')}\")\n",
    "            return low_vol_scores\n",
    "        \n",
    "        # Calculate daily returns\n",
    "        price_data['return'] = price_data.groupby('ticker')['adj_close'].pct_change()\n",
    "        \n",
    "        # Calculate rolling volatility (252-day annualized)\n",
    "        volatility_data = price_data.groupby('ticker')['return'].rolling(\n",
    "            window=252, min_periods=126\n",
    "        ).std().reset_index()\n",
    "        \n",
    "        # Annualize volatility\n",
    "        volatility_data['volatility_annualized'] = volatility_data['return'] * np.sqrt(252)\n",
    "        \n",
    "        # Get latest volatility for each ticker\n",
    "        latest_volatility = volatility_data.groupby('ticker')['volatility_annualized'].last()\n",
    "        \n",
    "        # Convert to low-volatility scores (lower volatility = higher score)\n",
    "        if not latest_volatility.empty:\n",
    "            max_vol = latest_volatility.max()\n",
    "            min_vol = latest_volatility.min()\n",
    "            \n",
    "            if max_vol > min_vol:\n",
    "                # Normalize to 0-1 range (lower volatility = higher score)\n",
    "                low_vol_scores = {\n",
    "                    ticker: 1.0 - ((vol - min_vol) / (max_vol - min_vol))\n",
    "                    for ticker, vol in latest_volatility.items()\n",
    "                }\n",
    "            else:\n",
    "                # All volatilities are the same, assign equal scores\n",
    "                low_vol_scores = {ticker: 0.5 for ticker in latest_volatility.index}\n",
    "        \n",
    "        return low_vol_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate low-volatility factor for {analysis_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484f5b9c",
   "metadata": {},
   "source": [
    "# HISTORICAL FACTOR GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff78f25",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Generate historical low-volatility scores\n",
    "historical_low_vol = {}\n",
    "\n",
    "for date in analysis_dates:\n",
    "    print(f\"Processing {date.strftime('%Y-%m-%d')}...\", end=' ')\n",
    "    scores = calculate_low_volatility_factor(engine, date, UNIVERSE_TICKERS)\n",
    "    if scores:\n",
    "        historical_low_vol[date] = scores\n",
    "        print(f\"‚úÖ {len(scores)} scores calculated\")\n",
    "    else:\n",
    "        print(\"‚ùå No scores\")\n",
    "\n",
    "print(f\"\\n‚úÖ Historical low-volatility scores generated for {len(historical_low_vol)} dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df6626d",
   "metadata": {},
   "source": [
    "# FORWARD RETURNS CALCULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4a844",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_forward_returns(engine, analysis_date, universe_tickers, forward_periods=[1, 3, 6, 12]):\n",
    "    \"\"\"\n",
    "    Calculate forward returns for statistical testing.\n",
    "    \n",
    "    Parameters:\n",
    "    - engine: QVMEngineV2Enhanced instance\n",
    "    - analysis_date: datetime for analysis\n",
    "    - universe_tickers: list of ticker symbols\n",
    "    - forward_periods: list of months for forward returns\n",
    "    \n",
    "    Returns:\n",
    "    - dict: {ticker: {period: return}}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        forward_returns = {}\n",
    "        \n",
    "        # Get price data for forward return calculation\n",
    "        ticker_str = \"', '\".join(universe_tickers)\n",
    "        max_forward = max(forward_periods)\n",
    "        end_date = analysis_date + pd.DateOffset(months=max_forward)\n",
    "        \n",
    "        price_query = f\"\"\"\n",
    "        SELECT \n",
    "            date,\n",
    "            ticker,\n",
    "            close as adj_close\n",
    "        FROM equity_history\n",
    "        WHERE ticker IN ('{ticker_str}')\n",
    "          AND date BETWEEN '{analysis_date.date()}' AND '{end_date.date()}'\n",
    "        ORDER BY ticker, date\n",
    "        \"\"\"\n",
    "        \n",
    "        price_data = pd.read_sql(price_query, engine.engine, parse_dates=['date'])\n",
    "        \n",
    "        if price_data.empty:\n",
    "            return forward_returns\n",
    "        \n",
    "        # Calculate forward returns for each period\n",
    "        for ticker in universe_tickers:\n",
    "            ticker_data = price_data[price_data['ticker'] == ticker].sort_values('date')\n",
    "            if ticker_data.empty:\n",
    "                continue\n",
    "                \n",
    "            start_price = ticker_data.iloc[0]['adj_close']\n",
    "            forward_returns[ticker] = {}\n",
    "            \n",
    "            for period in forward_periods:\n",
    "                # Find price at period months later\n",
    "                period_date = analysis_date + pd.DateOffset(months=period)\n",
    "                period_data = ticker_data[ticker_data['date'] >= period_date]\n",
    "                \n",
    "                if not period_data.empty:\n",
    "                    end_price = period_data.iloc[0]['adj_close']\n",
    "                    forward_return = (end_price - start_price) / start_price\n",
    "                    forward_returns[ticker][period] = forward_return\n",
    "        \n",
    "        return forward_returns\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to calculate forward returns for {analysis_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62a1e5",
   "metadata": {},
   "source": [
    "# STATISTICAL SIGNIFICANCE TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bc335",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_information_coefficient(factor_scores, forward_returns, period):\n",
    "    \"\"\"\n",
    "    Calculate Information Coefficient (IC) for a given forward period.\n",
    "    \n",
    "    Parameters:\n",
    "    - factor_scores: dict of {ticker: score}\n",
    "    - forward_returns: dict of {ticker: {period: return}}\n",
    "    - period: forward period in months\n",
    "    \n",
    "    Returns:\n",
    "    - float: Information Coefficient\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    returns = []\n",
    "    \n",
    "    for ticker in factor_scores:\n",
    "        if ticker in forward_returns and period in forward_returns[ticker]:\n",
    "            scores.append(factor_scores[ticker])\n",
    "            returns.append(forward_returns[ticker][period])\n",
    "    \n",
    "    if len(scores) < 3:  # Need at least 3 observations\n",
    "        return np.nan\n",
    "    \n",
    "    # Calculate rank correlation (Spearman's rho)\n",
    "    ic = spearman_correlation(scores, returns)\n",
    "    return ic\n",
    "\n",
    "def calculate_factor_returns(factor_scores, forward_returns, period, n_quintiles=5):\n",
    "    \"\"\"\n",
    "    Calculate factor returns using quintile analysis.\n",
    "    \n",
    "    Parameters:\n",
    "    - factor_scores: dict of {ticker: score}\n",
    "    - forward_returns: dict of {ticker: {period: return}}\n",
    "    - period: forward period in months\n",
    "    - n_quintiles: number of quintiles for analysis\n",
    "    \n",
    "    Returns:\n",
    "    - dict: quintile returns and spread\n",
    "    \"\"\"\n",
    "    # Create DataFrame for analysis\n",
    "    data = []\n",
    "    for ticker in factor_scores:\n",
    "        if ticker in forward_returns and period in forward_returns[ticker]:\n",
    "            data.append({\n",
    "                'ticker': ticker,\n",
    "                'factor_score': factor_scores[ticker],\n",
    "                'forward_return': forward_returns[ticker][period]\n",
    "            })\n",
    "    \n",
    "    if len(data) < n_quintiles:\n",
    "        return {}\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Create quintiles\n",
    "    try:\n",
    "        df['quintile'] = pd.qcut(df['factor_score'], n_quintiles, labels=False, duplicates='drop')\n",
    "    except ValueError as e:\n",
    "        # If we still can't create quintiles due to insufficient unique values,\n",
    "        # try with fewer quintiles\n",
    "        unique_values = df['factor_score'].nunique()\n",
    "        if unique_values < 2:\n",
    "            return {}\n",
    "        \n",
    "        # Use the maximum number of quintiles possible\n",
    "        max_quintiles = min(unique_values, n_quintiles)\n",
    "        if max_quintiles < 2:\n",
    "            return {}\n",
    "            \n",
    "        df['quintile'] = pd.qcut(df['factor_score'], max_quintiles, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Calculate returns by quintile\n",
    "    quintile_returns = df.groupby('quintile')['forward_return'].mean()\n",
    "    \n",
    "    # Calculate spread (Q5 - Q1)\n",
    "    spread = quintile_returns.iloc[-1] - quintile_returns.iloc[0]\n",
    "    \n",
    "    return {\n",
    "        'quintile_returns': quintile_returns,\n",
    "        'spread': spread,\n",
    "        'high_low_spread': spread\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dc355a",
   "metadata": {},
   "source": [
    "# COMPREHENSIVE STATISTICAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d067d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forward returns for all dates\n",
    "print(\"Calculating forward returns...\")\n",
    "historical_forward_returns = {}\n",
    "\n",
    "for date in list(historical_low_vol.keys()):\n",
    "    forward_returns = calculate_forward_returns(engine, date, UNIVERSE_TICKERS, [1, 3, 6, 12])\n",
    "    if forward_returns:\n",
    "        historical_forward_returns[date] = forward_returns\n",
    "\n",
    "print(f\"‚úÖ Forward returns calculated for {len(historical_forward_returns)} dates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d3282",
   "metadata": {},
   "source": [
    "# INFORMATION COEFFICIENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f017096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IC for different forward periods\n",
    "forward_periods = [1, 3, 6, 12]\n",
    "ic_results = {period: [] for period in forward_periods}\n",
    "\n",
    "for date in historical_low_vol:\n",
    "    if date in historical_forward_returns:\n",
    "        for period in forward_periods:\n",
    "            ic = calculate_information_coefficient(\n",
    "                historical_low_vol[date], \n",
    "                historical_forward_returns[date], \n",
    "                period\n",
    "            )\n",
    "            if not np.isnan(ic):\n",
    "                ic_results[period].append(ic)\n",
    "\n",
    "# Calculate IC statistics\n",
    "ic_stats = {}\n",
    "for period in forward_periods:\n",
    "    if ic_results[period]:\n",
    "        ic_values = ic_results[period]\n",
    "        ic_stats[period] = {\n",
    "            'mean': np.mean(ic_values),\n",
    "            'std': np.std(ic_values),\n",
    "            't_stat': np.mean(ic_values) / (np.std(ic_values) / np.sqrt(len(ic_values))),\n",
    "            'p_value': t_test_one_sample(ic_values, 0)[1],\n",
    "            'count': len(ic_values)\n",
    "        }\n",
    "\n",
    "print(\"Information Coefficient Analysis Results:\")\n",
    "print(\"=\" * 60)\n",
    "for period, stats in ic_stats.items():\n",
    "    print(f\"{period}M Forward Period:\")\n",
    "    print(f\"  Mean IC: {stats['mean']:.4f}\")\n",
    "    print(f\"  Std IC:  {stats['std']:.4f}\")\n",
    "    print(f\"  t-stat:  {stats['t_stat']:.4f}\")\n",
    "    print(f\"  p-value: {stats['p_value']:.4f}\")\n",
    "    print(f\"  N:       {stats['count']}\")\n",
    "    print(f\"  Significant: {'‚úÖ' if stats['p_value'] < 0.05 else '‚ùå'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd77af",
   "metadata": {},
   "source": [
    "# FACTOR RETURNS ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a846f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate factor returns for different periods\n",
    "factor_returns_results = {}\n",
    "\n",
    "for period in forward_periods:\n",
    "    period_returns = []\n",
    "    \n",
    "    for date in historical_low_vol:\n",
    "        if date in historical_forward_returns:\n",
    "            returns = calculate_factor_returns(\n",
    "                historical_low_vol[date],\n",
    "                historical_forward_returns[date],\n",
    "                period\n",
    "            )\n",
    "            if returns and 'spread' in returns:\n",
    "                period_returns.append(returns['spread'])\n",
    "    \n",
    "    if period_returns:\n",
    "        factor_returns_results[period] = {\n",
    "            'mean_return': np.mean(period_returns),\n",
    "            'std_return': np.std(period_returns),\n",
    "            't_stat': np.mean(period_returns) / (np.std(period_returns) / np.sqrt(len(period_returns))),\n",
    "            'p_value': t_test_one_sample(period_returns, 0)[1],\n",
    "            'count': len(period_returns),\n",
    "            'returns': period_returns\n",
    "        }\n",
    "\n",
    "print(\"Factor Returns Analysis Results:\")\n",
    "print(\"=\" * 60)\n",
    "for period, results in factor_returns_results.items():\n",
    "    print(f\"{period}M Forward Period:\")\n",
    "    print(f\"  Mean Spread: {results['mean_return']:.4f}\")\n",
    "    print(f\"  Std Spread:  {results['std_return']:.4f}\")\n",
    "    print(f\"  t-stat:      {results['t_stat']:.4f}\")\n",
    "    print(f\"  p-value:     {results['p_value']:.4f}\")\n",
    "    print(f\"  N:           {results['count']}\")\n",
    "    print(f\"  Significant: {'‚úÖ' if results['p_value'] < 0.05 else '‚ùå'}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109b33bf",
   "metadata": {},
   "source": [
    "# VISUALIZATION OF RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f11c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Low-Volatility Factor Statistical Analysis Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: IC Distribution\n",
    "ax1 = axes[0, 0]\n",
    "for period in [1, 3, 6, 12]:\n",
    "    if ic_results[period]:\n",
    "        ax1.hist(ic_results[period], alpha=0.6, label=f'{period}M', bins=20)\n",
    "ax1.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "ax1.set_xlabel('Information Coefficient')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('IC Distribution by Forward Period')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: IC Time Series\n",
    "ax2 = axes[0, 1]\n",
    "for period in [1, 3, 6, 12]:\n",
    "    if ic_results[period]:\n",
    "        dates = list(historical_low_vol.keys())[:len(ic_results[period])]\n",
    "        ax2.plot(dates, ic_results[period], label=f'{period}M', alpha=0.7)\n",
    "ax2.axhline(0, color='red', linestyle='--', alpha=0.7)\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Information Coefficient')\n",
    "ax2.set_title('IC Time Series')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Factor Returns Distribution\n",
    "ax3 = axes[1, 0]\n",
    "for period in [1, 3, 6, 12]:\n",
    "    if period in factor_returns_results:\n",
    "        ax3.hist(factor_returns_results[period]['returns'], alpha=0.6, label=f'{period}M', bins=20)\n",
    "ax3.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "ax3.set_xlabel('Factor Return Spread')\n",
    "ax3.set_ylabel('Frequency')\n",
    "ax3.set_title('Factor Returns Distribution')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Factor Returns Summary\n",
    "ax4 = axes[1, 1]\n",
    "periods = list(factor_returns_results.keys())\n",
    "means = [factor_returns_results[p]['mean_return'] for p in periods]\n",
    "stds = [factor_returns_results[p]['std_return'] for p in periods]\n",
    "colors = ['green' if factor_returns_results[p]['p_value'] < 0.05 else 'red' for p in periods]\n",
    "\n",
    "bars = ax4.bar([str(p) + 'M' for p in periods], means, yerr=stds, capsize=5, color=colors, alpha=0.7)\n",
    "ax4.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "ax4.set_xlabel('Forward Period')\n",
    "ax4.set_ylabel('Mean Factor Return Spread')\n",
    "ax4.set_title('Factor Returns by Forward Period')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add significance annotations\n",
    "for i, (period, results) in enumerate(factor_returns_results.items()):\n",
    "    if results['p_value'] < 0.05:\n",
    "        ax4.text(i, means[i] + stds[i] + 0.001, '*', ha='center', va='bottom', fontsize=16, color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43da32f",
   "metadata": {},
   "source": [
    "# SUMMARY AND CONCLUSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc2d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LOW-VOLATILITY FACTOR STATISTICAL SIGNIFICANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä KEY FINDINGS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# IC Analysis Summary\n",
    "print(\"\\n1. INFORMATION COEFFICIENT ANALYSIS:\")\n",
    "for period in [1, 3, 6, 12]:\n",
    "    if period in ic_stats:\n",
    "        stats = ic_stats[period]\n",
    "        significance = \"‚úÖ STATISTICALLY SIGNIFICANT\" if stats['p_value'] < 0.05 else \"‚ùå NOT SIGNIFICANT\"\n",
    "        print(f\"   {period}M Forward: IC = {stats['mean']:.4f} (p = {stats['p_value']:.4f}) - {significance}\")\n",
    "\n",
    "# Factor Returns Summary\n",
    "print(\"\\n2. FACTOR RETURNS ANALYSIS:\")\n",
    "for period in [1, 3, 6, 12]:\n",
    "    if period in factor_returns_results:\n",
    "        results = factor_returns_results[period]\n",
    "        significance = \"‚úÖ STATISTICALLY SIGNIFICANT\" if results['p_value'] < 0.05 else \"‚ùå NOT SIGNIFICANT\"\n",
    "        print(f\"   {period}M Forward: Spread = {results['mean_return']:.4f} (p = {results['p_value']:.4f}) - {significance}\")\n",
    "\n",
    "# Overall Assessment\n",
    "print(\"\\n3. OVERALL ASSESSMENT:\")\n",
    "significant_ic = sum(1 for period in [1, 3, 6, 12] if period in ic_stats and ic_stats[period]['p_value'] < 0.05)\n",
    "significant_returns = sum(1 for period in [1, 3, 6, 12] if period in factor_returns_results and factor_returns_results[period]['p_value'] < 0.05)\n",
    "\n",
    "print(f\"   - IC Significance: {significant_ic}/4 periods significant\")\n",
    "print(f\"   - Returns Significance: {significant_returns}/4 periods significant\")\n",
    "\n",
    "if significant_ic >= 2 and significant_returns >= 2:\n",
    "    print(\"   üéØ CONCLUSION: Low-Volatility factor shows strong statistical significance\")\n",
    "    print(\"   ‚úÖ RECOMMENDATION: Include in QVM v2.1 Alpha strategy\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è CONCLUSION: Low-Volatility factor shows mixed statistical significance\")\n",
    "    print(\"   üîç RECOMMENDATION: Further analysis needed before inclusion\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80) "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
