{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e23742b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QVM Engine v3j - Optimized Strategy (Simplified Factor Structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0e9528",
   "metadata": {},
   "source": [
    "# QVM Engine v3j - Optimized Strategy Analysis\n",
    "\n",
    "**Objective:** Optimized implementation based on factor integration investigation:\n",
    "- Simplified 3-factor structure (ROAA, P/E, Momentum)\n",
    "- Removed redundant factors (F-Score, FCF Yield, Low-Volatility)\n",
    "- Improved factor integration methodology\n",
    "- Focus on portfolio-level performance, not just factor-level IC\n",
    "\n",
    "**Key Changes:**\n",
    "- Simplified factor structure to reduce complexity\n",
    "- Removed correlated factors to improve diversification\n",
    "- Optimized factor weights based on investigation findings\n",
    "- Improved data quality handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "229db427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 23:03:25,058 - production.database.connection - INFO - Database configuration loaded from /Users/raymond/Documents/Projects/factor-investing-public/config/database.yml\n",
      "2025-08-04 23:03:25,059 - production.database.connection - INFO - DatabaseManager initialized for environment: production\n",
      "2025-08-04 23:03:25,181 - production.database.connection - INFO - SQLAlchemy engine created successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported production modules.\n",
      "   - Project Root set to: /Users/raymond/Documents/Projects/factor-investing-public\n",
      "\n",
      "⚙️  QVM Engine v3j Optimized Configuration Loaded:\n",
      "   - Strategy: QVM_Engine_v3j_Optimized\n",
      "   - Period: 2016-01-01 to 2025-07-28\n",
      "   - Universe: Top 200 stocks by ADTV\n",
      "   - Rebalancing: M frequency\n",
      "   - ROAA (Quality): 35.0%\n",
      "   - P/E (Value): 25.0%\n",
      "   - Momentum: 40.0%\n",
      "   - Performance: Pre-computed data + Vectorized operations\n",
      "🚀 QVM ENGINE V3J OPTIMIZED STRATEGY EXECUTION\n",
      "================================================================================\n",
      "📊 Step 1: Establishing database connection...\n",
      "✅ Database connection established successfully.\n",
      "\n",
      "📊 Step 2: Loading data...\n",
      "📂 Loading all data for period: 2015-07-01 to 2025-07-28...\n",
      "   - Loading price and volume data...\n",
      "   - Loading benchmark data...\n",
      "   - Loading fundamental data...\n",
      "   - Processing data...\n",
      "   ✅ Data loaded successfully:\n",
      "      - Price data: 1695229 records\n",
      "      - Returns matrix: (375, 728)\n",
      "      - Benchmark returns: 375 records\n",
      "      - Common dates: 375\n",
      "\n",
      "📊 Step 3: Precomputing data...\n",
      "🚀 Precomputing all data for optimized strategy...\n",
      "📊 Precomputing universe rankings...\n",
      "   ✅ Universe rankings computed: 267014 records\n",
      "   📊 Applied liquidity filter: ADTV > 10 billion VND\n",
      "📊 Precomputing fundamental factors...\n",
      "   📊 Calculating P/E ratios...\n",
      "   ✅ Fundamental factors computed: 694340 records\n",
      "📊 Precomputing momentum factors...\n",
      "   ✅ Momentum factors computed: 1729422 records\n",
      "✅ All data precomputed successfully!\n",
      "\n",
      "📊 Step 4: Initializing optimized engine...\n",
      "   📊 Precomputed data loaded:\n",
      "      - Universe: 267014 records\n",
      "      - Fundamentals: 694340 records\n",
      "      - Momentum: 1729422 records\n",
      "✅ QVM Engine v3j Optimized initialized\n",
      "   - Simplified 3-factor structure\n",
      "   - Optimized factor weights\n",
      "   - Improved data quality handling\n",
      "\n",
      "📊 Step 5: Running optimized backtest...\n",
      "\n",
      "🚀 Running QVM Engine v3j Optimized Backtest...\n",
      "   📊 Generating monthly rebalancing dates...\n",
      "   ✅ Generated 18 monthly rebalancing dates\n",
      "   🔄 Running optimized backtesting loop...\n",
      "   🔄 Rebalancing 1/18: 2024-01-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 155 stocks\n",
      "   ✅ 135 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 155, Portfolio: 20, Allocation: 100.0%, Turnover: 50.0%\n",
      "   🔄 Rebalancing 2/18: 2024-02-28\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 158 stocks\n",
      "   ✅ 139 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 158, Portfolio: 20, Allocation: 100.0%, Turnover: 17.5%\n",
      "   🔄 Rebalancing 3/18: 2024-03-29\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 166 stocks\n",
      "   ✅ 147 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 166, Portfolio: 20, Allocation: 100.0%, Turnover: 15.0%\n",
      "   🔄 Rebalancing 4/18: 2024-04-26\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 170 stocks\n",
      "   ✅ 151 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 170, Portfolio: 20, Allocation: 100.0%, Turnover: 12.5%\n",
      "   🔄 Rebalancing 5/18: 2024-05-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 176 stocks\n",
      "   ✅ 156 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 176, Portfolio: 20, Allocation: 100.0%, Turnover: 12.5%\n",
      "   🔄 Rebalancing 6/18: 2024-06-28\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 190 stocks\n",
      "   ✅ 169 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 190, Portfolio: 20, Allocation: 100.0%, Turnover: 7.5%\n",
      "   🔄 Rebalancing 7/18: 2024-07-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 183 stocks\n",
      "   ✅ 163 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 183, Portfolio: 20, Allocation: 100.0%, Turnover: 7.5%\n",
      "   🔄 Rebalancing 8/18: 2024-08-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 166 stocks\n",
      "   ✅ 147 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 166, Portfolio: 20, Allocation: 100.0%, Turnover: 2.5%\n",
      "   🔄 Rebalancing 9/18: 2024-09-27\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 151 stocks\n",
      "   ✅ 133 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 151, Portfolio: 20, Allocation: 100.0%, Turnover: 10.0%\n",
      "   🔄 Rebalancing 10/18: 2024-10-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 148 stocks\n",
      "   ✅ 130 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 148, Portfolio: 20, Allocation: 100.0%, Turnover: 2.5%\n",
      "   🔄 Rebalancing 11/18: 2024-11-29\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 146 stocks\n",
      "   ✅ 128 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 146, Portfolio: 20, Allocation: 100.0%, Turnover: 7.5%\n",
      "   🔄 Rebalancing 12/18: 2024-12-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 154 stocks\n",
      "   ✅ 136 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 154, Portfolio: 20, Allocation: 100.0%, Turnover: 10.0%\n",
      "   🔄 Rebalancing 13/18: 2025-01-24\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 154 stocks\n",
      "   ✅ 136 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 154, Portfolio: 20, Allocation: 100.0%, Turnover: 7.5%\n",
      "   🔄 Rebalancing 14/18: 2025-02-27\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 157 stocks\n",
      "   ✅ 140 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 157, Portfolio: 20, Allocation: 100.0%, Turnover: 15.0%\n",
      "   🔄 Rebalancing 15/18: 2025-03-28\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 164 stocks\n",
      "   ✅ 146 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 164, Portfolio: 20, Allocation: 100.0%, Turnover: 20.0%\n",
      "   🔄 Rebalancing 16/18: 2025-04-29\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 163 stocks\n",
      "   ✅ 143 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 163, Portfolio: 20, Allocation: 100.0%, Turnover: 5.0%\n",
      "   🔄 Rebalancing 17/18: 2025-05-30\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 166 stocks\n",
      "   ✅ 146 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 166, Portfolio: 20, Allocation: 100.0%, Turnover: 2.5%\n",
      "   🔄 Rebalancing 18/18: 2025-06-27\n",
      "   ✅ ROAA factor calculated\n",
      "   ✅ P/E factor calculated\n",
      "   ✅ Momentum factor calculated\n",
      "   ✅ Composite scores calculated for 162 stocks\n",
      "   ✅ 142 stocks qualified for portfolio construction\n",
      "   ✅ Universe: 162, Portfolio: 20, Allocation: 100.0%, Turnover: 7.5%\n",
      "\n",
      "💸 Net returns calculated.\n",
      "   - Total Gross Return: 31.23%\n",
      "   - Total Net Return: 30.39%\n",
      "   - Total Cost Drag: 0.64%\n",
      "\n",
      "📊 Step 6: Generating optimized tearsheet...\n",
      "✅ Tearsheet saved as: tearsheet_optimized_20250804_231213.png\n",
      "\n",
      "================================================================================\n",
      "📊 QVM ENGINE V3J: OPTIMIZED STRATEGY RESULTS\n",
      "================================================================================\n",
      "📈 Performance Summary:\n",
      "   - Strategy Annualized Return: 19.52%\n",
      "   - Benchmark Annualized Return: 19.88%\n",
      "   - Strategy Sharpe Ratio: 0.28\n",
      "   - Benchmark Sharpe Ratio: 1.08\n",
      "   - Strategy Max Drawdown: -74.60%\n",
      "   - Benchmark Max Drawdown: -18.11%\n",
      "   - Information Ratio: 0.39\n",
      "   - Beta: 3.37\n",
      "\n",
      "🔧 Optimized Configuration:\n",
      "   - Simplified 3-factor structure (ROAA, P/E, Momentum)\n",
      "   - Removed redundant factors (F-Score, FCF Yield, Low-Volatility)\n",
      "   - Optimized factor weights based on investigation\n",
      "   - Improved data quality handling\n",
      "\n",
      "✅ QVM Engine v3j Optimized strategy execution complete!\n"
     ]
    }
   ],
   "source": [
    "# Core scientific libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Database connectivity\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Environment Setup\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add Project Root to Python Path\n",
    "try:\n",
    "    current_path = Path.cwd()\n",
    "    while not (current_path / 'production').is_dir():\n",
    "        if current_path.parent == current_path:\n",
    "            raise FileNotFoundError(\"Could not find the 'production' directory.\")\n",
    "        current_path = current_path.parent\n",
    "    \n",
    "    project_root = current_path\n",
    "    \n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    \n",
    "    from production.database.connection import get_database_manager\n",
    "    from production.database.mappings.financial_mapping_manager import FinancialMappingManager\n",
    "    print(f\"✅ Successfully imported production modules.\")\n",
    "    print(f\"   - Project Root set to: {project_root}\")\n",
    "\n",
    "except (ImportError, FileNotFoundError) as e:\n",
    "    print(f\"❌ ERROR: Could not import production modules. Please check your directory structure.\")\n",
    "    print(f\"   - Final Path Searched: {project_root}\")\n",
    "    print(f\"   - Error: {e}\")\n",
    "    raise\n",
    "\n",
    "# OPTIMIZED CONFIGURATION\n",
    "QVM_CONFIG = {\n",
    "    # Backtest Parameters\n",
    "    \"strategy_name\": \"QVM_Engine_v3j_Optimized\",\n",
    "    \"backtest_start_date\": \"2016-01-01\",\n",
    "    \"backtest_end_date\": \"2025-07-28\",\n",
    "    \"rebalance_frequency\": \"M\", # Monthly\n",
    "    \"transaction_cost_bps\": 30, # Flat 30bps\n",
    "    \n",
    "    # Universe Construction\n",
    "    \"universe\": {\n",
    "        \"lookback_days\": 63,\n",
    "        \"top_n_stocks\": 200,  # Top 200 stocks by ADTV\n",
    "        \"max_position_size\": 0.05,\n",
    "        \"max_sector_exposure\": 0.30,\n",
    "        \"target_portfolio_size\": 20,\n",
    "    },\n",
    "    \n",
    "    # Optimized Factor Configuration (Simplified 3-Factor Structure)\n",
    "    \"factors\": {\n",
    "        \"roaa_weight\": 0.35,      # Quality factor (increased from 0.30)\n",
    "        \"pe_weight\": 0.25,        # Value factor (decreased from 0.30)\n",
    "        \"momentum_weight\": 0.40,  # Momentum factor (unchanged)\n",
    "        \"momentum_horizons\": [21, 63, 126, 252], # 1M, 3M, 6M, 12M\n",
    "        \"skip_months\": 1,\n",
    "        \"fundamental_lag_days\": 45,  # 45-day lag for announcement delay\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n⚙️  QVM Engine v3j Optimized Configuration Loaded:\")\n",
    "print(f\"   - Strategy: {QVM_CONFIG['strategy_name']}\")\n",
    "print(f\"   - Period: {QVM_CONFIG['backtest_start_date']} to {QVM_CONFIG['backtest_end_date']}\")\n",
    "print(f\"   - Universe: Top {QVM_CONFIG['universe']['top_n_stocks']} stocks by ADTV\")\n",
    "print(f\"   - Rebalancing: {QVM_CONFIG['rebalance_frequency']} frequency\")\n",
    "print(f\"   - ROAA (Quality): {QVM_CONFIG['factors']['roaa_weight']:.1%}\")\n",
    "print(f\"   - P/E (Value): {QVM_CONFIG['factors']['pe_weight']:.1%}\")\n",
    "print(f\"   - Momentum: {QVM_CONFIG['factors']['momentum_weight']:.1%}\")\n",
    "print(f\"   - Performance: Pre-computed data + Vectorized operations\")\n",
    "\n",
    "# DATABASE CONNECTION\n",
    "def create_db_connection():\n",
    "    \"\"\"Create database connection using production configuration.\"\"\"\n",
    "    try:\n",
    "        db_manager = get_database_manager()\n",
    "        engine = db_manager.get_engine()\n",
    "        print(\"✅ Database connection established successfully.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Database connection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# DATA PREPROCESSING FUNCTIONS\n",
    "def precompute_universe_rankings(config: dict, db_engine):\n",
    "    \"\"\"Precompute universe rankings for all dates.\"\"\"\n",
    "    print(\"📊 Precomputing universe rankings...\")\n",
    "    \n",
    "    start_date = pd.Timestamp(config['backtest_start_date']) - pd.DateOffset(days=config['universe']['lookback_days'] + 30)\n",
    "    end_date = config['backtest_end_date']\n",
    "    \n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            trading_date,\n",
    "            ticker,\n",
    "            total_volume,\n",
    "            close_price_adjusted as close,\n",
    "            total_volume * close_price_adjusted as adtv_vnd,\n",
    "            market_cap\n",
    "        FROM vcsc_daily_data_complete\n",
    "        WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "        AND total_volume > 0\n",
    "        ORDER BY trading_date, adtv_vnd DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    universe_data = pd.read_sql(query, db_engine, params={'start_date': start_date, 'end_date': end_date})\n",
    "    \n",
    "    # Calculate rolling ADTV and apply liquidity filter\n",
    "    universe_rankings = []\n",
    "    for date in universe_data['trading_date'].unique():\n",
    "        date_data = universe_data[universe_data['trading_date'] == date]\n",
    "        \n",
    "        # Calculate rolling ADTV (63-day lookback)\n",
    "        lookback_days = config['universe']['lookback_days']\n",
    "        lookback_start = date - pd.Timedelta(days=lookback_days)\n",
    "        lookback_data = universe_data[\n",
    "            (universe_data['trading_date'] >= lookback_start) & \n",
    "            (universe_data['trading_date'] <= date)\n",
    "        ]\n",
    "        \n",
    "        # Calculate average ADTV for each ticker\n",
    "        avg_adtv = lookback_data.groupby('ticker')['adtv_vnd'].mean().reset_index()\n",
    "        \n",
    "        # Apply liquidity filter: ADTV > 10 billion VND\n",
    "        liquidity_threshold = 10_000_000_000  # 10 billion VND\n",
    "        liquid_stocks = avg_adtv[avg_adtv['adtv_vnd'] >= liquidity_threshold]\n",
    "        \n",
    "        if len(liquid_stocks) > 0:\n",
    "            liquid_stocks = liquid_stocks.sort_values('adtv_vnd', ascending=False)\n",
    "            \n",
    "            # Select top N stocks from liquid universe\n",
    "            top_n = config['universe']['top_n_stocks']\n",
    "            top_stocks = liquid_stocks.head(top_n)\n",
    "            top_stocks['trading_date'] = date\n",
    "            \n",
    "            universe_rankings.append(top_stocks)\n",
    "    \n",
    "    universe_df = pd.concat(universe_rankings, ignore_index=True)\n",
    "    \n",
    "    print(f\"   ✅ Universe rankings computed: {len(universe_df)} records\")\n",
    "    print(f\"   📊 Applied liquidity filter: ADTV > 10 billion VND\")\n",
    "    return universe_df\n",
    "\n",
    "def precompute_fundamental_factors(config: dict, db_engine):\n",
    "    \"\"\"Precompute fundamental factors (ROAA, P/E) for all dates.\"\"\"\n",
    "    print(\"📊 Precomputing fundamental factors...\")\n",
    "    \n",
    "    start_date = pd.Timestamp(config['backtest_start_date']) - pd.DateOffset(days=config['factors']['fundamental_lag_days'] + 30)\n",
    "    end_date = config['backtest_end_date']\n",
    "    \n",
    "    # Load fundamental data\n",
    "    fundamental_query = text(\"\"\"\n",
    "        WITH netprofit_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as netprofit_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 1\n",
    "            AND fv.statement_type = 'PL'\n",
    "            AND fv.year >= :start_year\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        ),\n",
    "        totalassets_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as totalassets_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 2\n",
    "            AND fv.statement_type = 'BS'\n",
    "            AND fv.year >= :start_year\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        )\n",
    "        SELECT \n",
    "            np.ticker,\n",
    "            np.year,\n",
    "            np.quarter,\n",
    "            np.netprofit_ttm,\n",
    "            ta.totalassets_ttm,\n",
    "            CASE \n",
    "                WHEN ta.totalassets_ttm > 0 THEN np.netprofit_ttm / ta.totalassets_ttm \n",
    "                ELSE NULL \n",
    "            END as roaa\n",
    "        FROM netprofit_ttm np\n",
    "        LEFT JOIN totalassets_ttm ta ON np.ticker = ta.ticker AND np.year = ta.year AND np.quarter = ta.quarter\n",
    "        WHERE np.netprofit_ttm > 0 \n",
    "        AND ta.totalassets_ttm > 0\n",
    "        ORDER BY np.ticker, np.year, np.quarter\n",
    "    \"\"\")\n",
    "    \n",
    "    fundamental_data = pd.read_sql(fundamental_query, db_engine, params={'start_year': start_date.year})\n",
    "    \n",
    "    # Calculate ROAA\n",
    "    fundamental_data['roaa'] = fundamental_data['netprofit_ttm'] / fundamental_data['totalassets_ttm']\n",
    "    \n",
    "    # Calculate P/E ratio using market cap and net profit\n",
    "    print(\"   📊 Calculating P/E ratios...\")\n",
    "    pe_query = text(\"\"\"\n",
    "        SELECT \n",
    "            fv.ticker,\n",
    "            fv.year,\n",
    "            fv.quarter,\n",
    "            SUM(CASE WHEN fv.item_id = 1 AND fv.statement_type = 'PL' THEN fv.value / 1e9 ELSE 0 END) as netprofit_ttm,\n",
    "            eh.market_cap / 1e9 as market_cap_bn\n",
    "        FROM fundamental_values fv\n",
    "        JOIN equity_history_with_market_cap eh ON fv.ticker = eh.ticker \n",
    "            AND fv.year = YEAR(eh.date) \n",
    "            AND fv.quarter = QUARTER(eh.date)\n",
    "        WHERE fv.year >= :start_year\n",
    "        AND fv.item_id = 1 \n",
    "        AND fv.statement_type = 'PL'\n",
    "        AND eh.market_cap > 0\n",
    "        GROUP BY fv.ticker, fv.year, fv.quarter, eh.market_cap\n",
    "        HAVING netprofit_ttm > 0\n",
    "    \"\"\")\n",
    "    \n",
    "    pe_data = pd.read_sql(pe_query, db_engine, params={'start_year': start_date.year})\n",
    "    \n",
    "    if not pe_data.empty:\n",
    "        # Calculate P/E ratio\n",
    "        pe_data['pe'] = pe_data['market_cap_bn'] / pe_data['netprofit_ttm']\n",
    "        \n",
    "        # Add date column for merging\n",
    "        pe_data['date'] = pd.to_datetime(\n",
    "            pe_data['year'].astype(str) + '-' + \n",
    "            (pe_data['quarter'] * 3).astype(str).str.zfill(2) + '-01'\n",
    "        )\n",
    "        \n",
    "        # Add date column to fundamental data for merging\n",
    "        fundamental_data['date'] = pd.to_datetime(\n",
    "            fundamental_data['year'].astype(str) + '-' + \n",
    "            (fundamental_data['quarter'] * 3).astype(str).str.zfill(2) + '-01'\n",
    "        )\n",
    "        \n",
    "        # Merge P/E data with fundamental data\n",
    "        fundamental_data = fundamental_data.merge(\n",
    "            pe_data[['ticker', 'date', 'pe']], \n",
    "            on=['ticker', 'date'], \n",
    "            how='left'\n",
    "        )\n",
    "    else:\n",
    "        fundamental_data['pe'] = np.nan\n",
    "    \n",
    "    # Clean up extreme values\n",
    "    fundamental_data['roaa'] = fundamental_data['roaa'].clip(-1, 1)  # ROAA between -100% and 100%\n",
    "    fundamental_data['pe'] = fundamental_data['pe'].clip(0, 100)  # P/E between 0 and 100\n",
    "    \n",
    "    print(f\"   ✅ Fundamental factors computed: {len(fundamental_data)} records\")\n",
    "    return fundamental_data\n",
    "\n",
    "def precompute_momentum_factors(config: dict, db_engine):\n",
    "    \"\"\"Precompute momentum factors for all dates.\"\"\"\n",
    "    print(\"📊 Precomputing momentum factors...\")\n",
    "    \n",
    "    start_date = pd.Timestamp(config['backtest_start_date']) - pd.DateOffset(days=max(config['factors']['momentum_horizons']) + 30)\n",
    "    end_date = config['backtest_end_date']\n",
    "    \n",
    "    query = text(\"\"\"\n",
    "        SELECT \n",
    "            trading_date,\n",
    "            ticker,\n",
    "            close_price_adjusted as close\n",
    "        FROM vcsc_daily_data_complete\n",
    "        WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "        ORDER BY ticker, trading_date\n",
    "    \"\"\")\n",
    "    \n",
    "    price_data = pd.read_sql(query, db_engine, params={'start_date': start_date, 'end_date': end_date})\n",
    "    \n",
    "    # Calculate returns for each horizon\n",
    "    momentum_data = []\n",
    "    for ticker in price_data['ticker'].unique():\n",
    "        ticker_data = price_data[price_data['ticker'] == ticker].sort_values('trading_date')\n",
    "        ticker_data['returns'] = ticker_data['close'].pct_change()\n",
    "        \n",
    "        for horizon in config['factors']['momentum_horizons']:\n",
    "            ticker_data[f'momentum_{horizon}'] = ticker_data['returns'].rolling(horizon).mean()\n",
    "        \n",
    "        momentum_data.append(ticker_data)\n",
    "    \n",
    "    momentum_df = pd.concat(momentum_data, ignore_index=True)\n",
    "    \n",
    "    # Calculate composite momentum score\n",
    "    momentum_columns = [f'momentum_{h}' for h in config['factors']['momentum_horizons']]\n",
    "    momentum_df['momentum_score'] = momentum_df[momentum_columns].mean(axis=1)\n",
    "    \n",
    "    print(f\"   ✅ Momentum factors computed: {len(momentum_df)} records\")\n",
    "    return momentum_df\n",
    "\n",
    "def precompute_all_data(config: dict, db_engine):\n",
    "    \"\"\"Precompute all data for the backtest.\"\"\"\n",
    "    print(\"🚀 Precomputing all data for optimized strategy...\")\n",
    "    \n",
    "    universe_rankings = precompute_universe_rankings(config, db_engine)\n",
    "    fundamental_factors = precompute_fundamental_factors(config, db_engine)\n",
    "    momentum_factors = precompute_momentum_factors(config, db_engine)\n",
    "    \n",
    "    precomputed_data = {\n",
    "        'universe': universe_rankings,\n",
    "        'fundamentals': fundamental_factors,\n",
    "        'momentum': momentum_factors\n",
    "    }\n",
    "    \n",
    "    print(\"✅ All data precomputed successfully!\")\n",
    "    return precomputed_data\n",
    "\n",
    "# OPTIMIZED FACTOR CALCULATOR\n",
    "class OptimizedFactorCalculator:\n",
    "    \"\"\"Optimized factor calculator with simplified 3-factor structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, engine):\n",
    "        self.engine = engine\n",
    "    \n",
    "    def calculate_sector_aware_pe(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate sector-aware P/E ratios.\"\"\"\n",
    "        if 'pe' not in data.columns:\n",
    "            return data\n",
    "        \n",
    "        # Simple P/E normalization (no sector adjustment for simplicity)\n",
    "        pe_data = data['pe'].dropna()\n",
    "        if len(pe_data) > 1 and pe_data.std() > 0:\n",
    "            data['pe_normalized'] = (data['pe'] - pe_data.mean()) / pe_data.std()\n",
    "        else:\n",
    "            data['pe_normalized'] = 0\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def calculate_momentum_score(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate momentum score from precomputed data.\"\"\"\n",
    "        if 'momentum_score' not in data.columns:\n",
    "            return data\n",
    "        \n",
    "        # Simple momentum normalization\n",
    "        momentum_data = data['momentum_score'].dropna()\n",
    "        if len(momentum_data) > 1 and momentum_data.std() > 0:\n",
    "            data['momentum_normalized'] = (data['momentum_score'] - momentum_data.mean()) / momentum_data.std()\n",
    "        else:\n",
    "            data['momentum_normalized'] = 0\n",
    "        \n",
    "        return data\n",
    "\n",
    "# OPTIMIZED QVM ENGINE\n",
    "class QVMEngineV3jOptimized:\n",
    "    \"\"\"Optimized QVM Engine with simplified factor structure.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, price_data: pd.DataFrame, fundamental_data: pd.DataFrame,\n",
    "                 returns_matrix: pd.DataFrame, benchmark_returns: pd.Series, db_engine, precomputed_data: dict):\n",
    "        \n",
    "        self.config = config\n",
    "        self.price_data = price_data\n",
    "        self.fundamental_data = fundamental_data\n",
    "        self.daily_returns_matrix = returns_matrix\n",
    "        self.benchmark_returns = benchmark_returns\n",
    "        self.db_engine = db_engine\n",
    "        self.precomputed_data = precomputed_data\n",
    "        \n",
    "        # Initialize factor calculator\n",
    "        self.factor_calculator = OptimizedFactorCalculator(db_engine)\n",
    "        \n",
    "        # Setup precomputed data\n",
    "        self._setup_precomputed_data()\n",
    "        \n",
    "        print(f\"✅ QVM Engine v3j Optimized initialized\")\n",
    "        print(f\"   - Simplified 3-factor structure\")\n",
    "        print(f\"   - Optimized factor weights\")\n",
    "        print(f\"   - Improved data quality handling\")\n",
    "    \n",
    "    def _setup_precomputed_data(self):\n",
    "        \"\"\"Setup precomputed data for the engine.\"\"\"\n",
    "        self.universe_rankings = self.precomputed_data.get('universe', pd.DataFrame())\n",
    "        self.fundamental_factors = self.precomputed_data.get('fundamentals', pd.DataFrame())\n",
    "        self.momentum_factors = self.precomputed_data.get('momentum', pd.DataFrame())\n",
    "        \n",
    "        print(f\"   📊 Precomputed data loaded:\")\n",
    "        print(f\"      - Universe: {len(self.universe_rankings)} records\")\n",
    "        print(f\"      - Fundamentals: {len(self.fundamental_factors)} records\")\n",
    "        print(f\"      - Momentum: {len(self.momentum_factors)} records\")\n",
    "    \n",
    "    def run_backtest(self) -> (pd.Series, pd.DataFrame):\n",
    "        \"\"\"Run the optimized backtest.\"\"\"\n",
    "        print(f\"\\n🚀 Running QVM Engine v3j Optimized Backtest...\")\n",
    "        \n",
    "        # Generate rebalancing dates\n",
    "        rebalance_dates = self._generate_rebalance_dates()\n",
    "        \n",
    "        # Run backtesting loop\n",
    "        returns, diagnostics = self._run_optimized_backtesting_loop(rebalance_dates)\n",
    "        \n",
    "        return returns, diagnostics\n",
    "    \n",
    "    def _generate_rebalance_dates(self) -> list:\n",
    "        \"\"\"Generate monthly rebalancing dates using the working strategy approach.\"\"\"\n",
    "        print(\"   📊 Generating monthly rebalancing dates...\")\n",
    "        \n",
    "        # Use the same approach as the working strategy\n",
    "        all_trading_dates = self.daily_returns_matrix.index\n",
    "        rebal_dates_calendar = pd.date_range(\n",
    "            start=self.config['backtest_start_date'],\n",
    "            end=self.config['backtest_end_date'],\n",
    "            freq=self.config['rebalance_frequency']\n",
    "        )\n",
    "        \n",
    "        actual_rebal_dates = []\n",
    "        for d in rebal_dates_calendar:\n",
    "            if d >= all_trading_dates.min():\n",
    "                # Find the closest trading date before or on the calendar date\n",
    "                idx = all_trading_dates.searchsorted(d, side='left')\n",
    "                if idx > 0:\n",
    "                    actual_rebal_dates.append(all_trading_dates[idx-1])\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        actual_rebal_dates = sorted(list(set(actual_rebal_dates)))\n",
    "        \n",
    "        # Convert to the format expected by the backtesting loop\n",
    "        rebalancing_dates = [{'date': date, 'allocation': 1.0} for date in actual_rebal_dates]\n",
    "        \n",
    "        print(f\"   ✅ Generated {len(rebalancing_dates)} monthly rebalancing dates\")\n",
    "        return rebalancing_dates\n",
    "    \n",
    "    def _run_optimized_backtesting_loop(self, rebalance_dates: list) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"Run the optimized backtesting loop.\"\"\"\n",
    "        print(\"   🔄 Running optimized backtesting loop...\")\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        daily_holdings = pd.DataFrame(0.0, index=self.daily_returns_matrix.index, columns=self.daily_returns_matrix.columns)\n",
    "        diagnostics_log = []\n",
    "        \n",
    "        for i, rebal_info in enumerate(rebalance_dates):\n",
    "            rebal_date = rebal_info['date']\n",
    "            allocation = rebal_info['allocation']\n",
    "            \n",
    "            print(f\"   🔄 Rebalancing {i+1}/{len(rebalance_dates)}: {rebal_date.strftime('%Y-%m-%d')}\")\n",
    "            \n",
    "            # Get universe\n",
    "            universe = self._get_universe_from_precomputed(rebal_date)\n",
    "            if not universe:\n",
    "                print(f\"   ⚠️  No universe found for {rebal_date}\")\n",
    "                continue\n",
    "            \n",
    "            # Get factors\n",
    "            factors_df = self._get_factors_from_precomputed(universe, rebal_date)\n",
    "            if factors_df.empty:\n",
    "                print(f\"   ⚠️  No factors found for {rebal_date}\")\n",
    "                continue\n",
    "            \n",
    "            # Calculate composite score\n",
    "            factors_df = self._calculate_optimized_composite_score(factors_df)\n",
    "            \n",
    "            # Apply entry criteria\n",
    "            qualified_df = self._apply_entry_criteria(factors_df)\n",
    "            if qualified_df.empty:\n",
    "                print(f\"   ⚠️  No qualified stocks for {rebal_date}\")\n",
    "                continue\n",
    "            \n",
    "            # Construct portfolio\n",
    "            portfolio = self._construct_portfolio(qualified_df, allocation)\n",
    "            if portfolio.empty:\n",
    "                print(f\"   ⚠️  No portfolio constructed for {rebal_date}\")\n",
    "                continue\n",
    "            \n",
    "            # Update holdings\n",
    "            daily_holdings.loc[rebal_date:, portfolio.index] = portfolio.values\n",
    "            \n",
    "            # Log diagnostics\n",
    "            turnover = self._calculate_turnover(daily_holdings, rebal_date)\n",
    "            diagnostics_log.append({\n",
    "                'date': rebal_date,\n",
    "                'universe_size': len(universe),\n",
    "                'portfolio_size': len(portfolio),\n",
    "                'allocation': allocation,\n",
    "                'turnover': turnover\n",
    "            })\n",
    "            \n",
    "            print(f\"   ✅ Universe: {len(universe)}, Portfolio: {len(portfolio)}, Allocation: {allocation:.1%}, Turnover: {turnover:.1%}\")\n",
    "        \n",
    "        # Calculate net returns\n",
    "        net_returns = self._calculate_net_returns(daily_holdings)\n",
    "        diagnostics_df = pd.DataFrame(diagnostics_log)\n",
    "        \n",
    "        return net_returns, diagnostics_df\n",
    "    \n",
    "    def _get_universe_from_precomputed(self, analysis_date: pd.Timestamp) -> list:\n",
    "        \"\"\"Get universe from precomputed data.\"\"\"\n",
    "        # Convert trading_date column to datetime if needed\n",
    "        if self.universe_rankings['trading_date'].dtype == 'object':\n",
    "            self.universe_rankings['trading_date'] = pd.to_datetime(self.universe_rankings['trading_date'])\n",
    "        \n",
    "        # Debug: Check available dates\n",
    "        if len(self.universe_rankings) == 0:\n",
    "            print(f\"   ⚠️  No universe rankings data available\")\n",
    "            return []\n",
    "        \n",
    "        # Find exact match first\n",
    "        universe_data = self.universe_rankings[self.universe_rankings['trading_date'] == analysis_date]\n",
    "        \n",
    "        # If no exact match, find closest date\n",
    "        if len(universe_data) == 0:\n",
    "            available_dates = self.universe_rankings['trading_date'].unique()\n",
    "            if len(available_dates) > 0:\n",
    "                closest_date = min(available_dates, key=lambda x: abs(x - analysis_date))\n",
    "                print(f\"   ⚠️  Date {analysis_date.date()} not found, using closest date: {closest_date.date()}\")\n",
    "                universe_data = self.universe_rankings[self.universe_rankings['trading_date'] == closest_date]\n",
    "        \n",
    "        return universe_data['ticker'].tolist()\n",
    "    \n",
    "    def _get_factors_from_precomputed(self, universe: list, analysis_date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"Get factors from precomputed data.\"\"\"\n",
    "        # Get fundamental data\n",
    "        fundamental_data = self.fundamental_factors[\n",
    "            (self.fundamental_factors['ticker'].isin(universe)) &\n",
    "            (self.fundamental_factors['year'] <= analysis_date.year)\n",
    "        ].copy()\n",
    "        \n",
    "        # Get momentum data\n",
    "        # Convert trading_date to datetime if it's not already\n",
    "        if self.momentum_factors['trading_date'].dtype == 'object':\n",
    "            self.momentum_factors['trading_date'] = pd.to_datetime(self.momentum_factors['trading_date'])\n",
    "        \n",
    "        momentum_data = self.momentum_factors[\n",
    "            (self.momentum_factors['ticker'].isin(universe)) &\n",
    "            (self.momentum_factors['trading_date'] <= analysis_date)\n",
    "        ].copy()\n",
    "        \n",
    "        # Merge data\n",
    "        factors_df = pd.DataFrame({'ticker': universe})\n",
    "        \n",
    "        # Add fundamental factors\n",
    "        if not fundamental_data.empty:\n",
    "            latest_fundamentals = fundamental_data.groupby('ticker').last().reset_index()\n",
    "            factors_df = factors_df.merge(latest_fundamentals[['ticker', 'roaa', 'pe']], on='ticker', how='left')\n",
    "        \n",
    "        # Add momentum factors\n",
    "        if not momentum_data.empty:\n",
    "            latest_momentum = momentum_data.groupby('ticker').last().reset_index()\n",
    "            factors_df = factors_df.merge(latest_momentum[['ticker', 'momentum_score']], on='ticker', how='left')\n",
    "        \n",
    "        return factors_df\n",
    "    \n",
    "    def _calculate_optimized_composite_score(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate optimized composite score using simplified 3-factor structure.\"\"\"\n",
    "        factors_df['composite_score'] = 0.0\n",
    "        \n",
    "        # ROAA component (quality factor - positive signal)\n",
    "        if 'roaa' in factors_df.columns and not factors_df['roaa'].isna().all():\n",
    "            roaa_weight = self.config['factors']['roaa_weight']\n",
    "            roaa_data = factors_df['roaa'].dropna()\n",
    "            if len(roaa_data) > 1 and roaa_data.std() > 0:\n",
    "                factors_df['roaa_normalized'] = (factors_df['roaa'] - roaa_data.mean()) / roaa_data.std()\n",
    "                factors_df['composite_score'] += factors_df['roaa_normalized'].fillna(0) * roaa_weight\n",
    "                print(f\"   ✅ ROAA factor calculated\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Insufficient ROAA data\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  No ROAA data available\")\n",
    "        \n",
    "        # P/E component (value factor - contrarian signal)\n",
    "        if 'pe' in factors_df.columns and not factors_df['pe'].isna().all():\n",
    "            pe_weight = self.config['factors']['pe_weight']\n",
    "            pe_data = factors_df['pe'].dropna()\n",
    "            if len(pe_data) > 1 and pe_data.std() > 0:\n",
    "                factors_df['pe_normalized'] = (factors_df['pe'] - pe_data.mean()) / pe_data.std()\n",
    "                factors_df['composite_score'] += (-factors_df['pe_normalized'].fillna(0)) * pe_weight  # Negative for contrarian\n",
    "                print(f\"   ✅ P/E factor calculated\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Insufficient P/E data\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  No P/E data available\")\n",
    "        \n",
    "        # Momentum component (momentum factor - positive signal)\n",
    "        if 'momentum_score' in factors_df.columns and not factors_df['momentum_score'].isna().all():\n",
    "            momentum_weight = self.config['factors']['momentum_weight']\n",
    "            momentum_data = factors_df['momentum_score'].dropna()\n",
    "            if len(momentum_data) > 1 and momentum_data.std() > 0:\n",
    "                factors_df['momentum_normalized'] = (factors_df['momentum_score'] - momentum_data.mean()) / momentum_data.std()\n",
    "                factors_df['composite_score'] += factors_df['momentum_normalized'].fillna(0) * momentum_weight\n",
    "                print(f\"   ✅ Momentum factor calculated\")\n",
    "            else:\n",
    "                print(f\"   ⚠️  Insufficient momentum data\")\n",
    "        else:\n",
    "            print(f\"   ⚠️  No momentum data available\")\n",
    "        \n",
    "        print(f\"   ✅ Composite scores calculated for {len(factors_df)} stocks\")\n",
    "        return factors_df\n",
    "    \n",
    "    def _apply_entry_criteria(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply entry criteria to filter stocks (relaxed version).\"\"\"\n",
    "        qualified = factors_df.copy()\n",
    "        \n",
    "        # Basic quality filters (relaxed)\n",
    "        if 'roaa' in qualified.columns:\n",
    "            # Only filter out extremely negative ROAA, allow slightly negative\n",
    "            qualified = qualified[qualified['roaa'] > -0.5]  # Allow ROAA > -50%\n",
    "        \n",
    "        # Remove extreme P/E values (relaxed)\n",
    "        if 'pe' in qualified.columns:\n",
    "            qualified = qualified[(qualified['pe'] > 0) & (qualified['pe'] < 100)]  # Allow P/E up to 100\n",
    "        \n",
    "        # Remove stocks with missing composite scores\n",
    "        qualified = qualified[qualified['composite_score'].notna()]\n",
    "        \n",
    "        # If still no stocks, relax further\n",
    "        if len(qualified) == 0:\n",
    "            print(f\"   ⚠️  No stocks qualified with strict criteria, relaxing filters...\")\n",
    "            qualified = factors_df.copy()\n",
    "            qualified = qualified[qualified['composite_score'].notna()]\n",
    "            \n",
    "            # Only filter out extreme outliers\n",
    "            if 'roaa' in qualified.columns:\n",
    "                qualified = qualified[qualified['roaa'] > -1.0]  # Allow any ROAA > -100%\n",
    "            if 'pe' in qualified.columns:\n",
    "                qualified = qualified[(qualified['pe'] > 0) & (qualified['pe'] < 200)]  # Allow P/E up to 200\n",
    "        \n",
    "        # If still no stocks, accept all stocks with composite scores\n",
    "        if len(qualified) == 0:\n",
    "            print(f\"   ⚠️  Still no stocks qualified, accepting all stocks with composite scores...\")\n",
    "            qualified = factors_df[factors_df['composite_score'].notna()].copy()\n",
    "            \n",
    "            # Debug: Show data distribution\n",
    "            print(f\"   🔍 Debug - Total stocks with composite scores: {len(qualified)}\")\n",
    "            if 'roaa' in qualified.columns:\n",
    "                print(f\"   🔍 Debug - ROAA range: {qualified['roaa'].min():.3f} to {qualified['roaa'].max():.3f}\")\n",
    "            if 'pe' in qualified.columns:\n",
    "                print(f\"   🔍 Debug - P/E range: {qualified['pe'].min():.1f} to {qualified['pe'].max():.1f}\")\n",
    "            print(f\"   🔍 Debug - Composite score range: {qualified['composite_score'].min():.3f} to {qualified['composite_score'].max():.3f}\")\n",
    "        \n",
    "        print(f\"   ✅ {len(qualified)} stocks qualified for portfolio construction\")\n",
    "        return qualified\n",
    "    \n",
    "    def _construct_portfolio(self, qualified_df: pd.DataFrame, allocation: float) -> pd.Series:\n",
    "        \"\"\"Construct the portfolio using the qualified stocks.\"\"\"\n",
    "        if qualified_df.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Sort by composite score\n",
    "        qualified_df = qualified_df.sort_values('composite_score', ascending=False)\n",
    "        \n",
    "        # Select top stocks\n",
    "        target_size = self.config['universe']['target_portfolio_size']\n",
    "        selected_stocks = qualified_df.head(target_size)\n",
    "        \n",
    "        if selected_stocks.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Equal weight portfolio\n",
    "        portfolio = pd.Series(allocation / len(selected_stocks), index=selected_stocks['ticker'])\n",
    "        \n",
    "        return portfolio\n",
    "    \n",
    "    def _calculate_turnover(self, daily_holdings: pd.DataFrame, rebal_date: pd.Timestamp) -> float:\n",
    "        \"\"\"Calculate portfolio turnover.\"\"\"\n",
    "        if rebal_date == daily_holdings.index[0]:\n",
    "            return 0.0\n",
    "        \n",
    "        prev_date = daily_holdings.index[daily_holdings.index.get_loc(rebal_date) - 1]\n",
    "        current_holdings = daily_holdings.loc[rebal_date]\n",
    "        prev_holdings = daily_holdings.loc[prev_date]\n",
    "        \n",
    "        turnover = (current_holdings - prev_holdings).abs().sum() / 2.0\n",
    "        return turnover\n",
    "    \n",
    "    def _calculate_net_returns(self, daily_holdings: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate net returns with transaction costs.\"\"\"\n",
    "        holdings_shifted = daily_holdings.shift(1).fillna(0.0)\n",
    "        gross_returns = (holdings_shifted * self.daily_returns_matrix).sum(axis=1)\n",
    "        \n",
    "        # Calculate turnover and costs\n",
    "        turnover = (holdings_shifted - holdings_shifted.shift(1)).abs().sum(axis=1) / 2.0\n",
    "        costs = turnover * (self.config['transaction_cost_bps'] / 10000)\n",
    "        net_returns = (gross_returns - costs).rename(self.config['strategy_name'])\n",
    "        \n",
    "        print(f\"\\n💸 Net returns calculated.\")\n",
    "        print(f\"   - Total Gross Return: {(1 + gross_returns).prod() - 1:.2%}\")\n",
    "        print(f\"   - Total Net Return: {(1 + net_returns).prod() - 1:.2%}\")\n",
    "        print(f\"   - Total Cost Drag: {(gross_returns.sum() - net_returns.sum()):.2%}\")\n",
    "        \n",
    "        return net_returns\n",
    "\n",
    "# DATA LOADING AND MAIN EXECUTION\n",
    "def load_all_data_for_backtest(config: dict, db_engine):\n",
    "    \"\"\"Load all necessary data for the backtest.\"\"\"\n",
    "    start_date = config['backtest_start_date']\n",
    "    end_date = config['backtest_end_date']\n",
    "    \n",
    "    # Add buffer for rolling calculations\n",
    "    buffer_start_date = pd.Timestamp(start_date) - pd.DateOffset(months=6)\n",
    "    \n",
    "    print(f\"📂 Loading all data for period: {buffer_start_date.date()} to {end_date}...\")\n",
    "\n",
    "    # 1. Price and Volume Data\n",
    "    print(\"   - Loading price and volume data...\")\n",
    "    price_query = text(\"\"\"\n",
    "        SELECT \n",
    "            trading_date as date,\n",
    "            ticker,\n",
    "            close_price_adjusted as close,\n",
    "            total_volume as volume,\n",
    "            market_cap\n",
    "        FROM vcsc_daily_data_complete\n",
    "        WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "        ORDER BY trading_date, ticker\n",
    "    \"\"\")\n",
    "    \n",
    "    price_data = pd.read_sql(price_query, db_engine, params={'start_date': buffer_start_date, 'end_date': end_date})\n",
    "    price_data['date'] = pd.to_datetime(price_data['date'])\n",
    "    \n",
    "    # 2. Benchmark Data (VNINDEX)\n",
    "    print(\"   - Loading benchmark data...\")\n",
    "    benchmark_query = text(\"\"\"\n",
    "        SELECT \n",
    "            date,\n",
    "            close\n",
    "        FROM etf_history\n",
    "        WHERE ticker = 'VNINDEX'\n",
    "        AND date BETWEEN :start_date AND :end_date\n",
    "        ORDER BY date\n",
    "    \"\"\")\n",
    "    \n",
    "    benchmark_data = pd.read_sql(benchmark_query, db_engine, params={'start_date': start_date, 'end_date': end_date})\n",
    "    benchmark_data['date'] = pd.to_datetime(benchmark_data['date'])\n",
    "    \n",
    "    # 3. Fundamental Data\n",
    "    print(\"   - Loading fundamental data...\")\n",
    "    fundamental_query = text(\"\"\"\n",
    "        WITH netprofit_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as netprofit_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 1\n",
    "            AND fv.statement_type = 'PL'\n",
    "            AND fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        ),\n",
    "        totalassets_ttm AS (\n",
    "            SELECT \n",
    "                fv.ticker,\n",
    "                fv.year,\n",
    "                fv.quarter,\n",
    "                SUM(fv.value / 1e9) as totalassets_ttm\n",
    "            FROM fundamental_values fv\n",
    "            WHERE fv.item_id = 2\n",
    "            AND fv.statement_type = 'BS'\n",
    "            AND fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)\n",
    "            GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "        )\n",
    "        SELECT \n",
    "            np.ticker,\n",
    "            np.year,\n",
    "            np.quarter,\n",
    "            np.netprofit_ttm,\n",
    "            ta.totalassets_ttm,\n",
    "            CASE \n",
    "                WHEN ta.totalassets_ttm > 0 THEN np.netprofit_ttm / ta.totalassets_ttm \n",
    "                ELSE NULL \n",
    "            END as roaa\n",
    "        FROM netprofit_ttm np\n",
    "        LEFT JOIN totalassets_ttm ta ON np.ticker = ta.ticker AND np.year = ta.year AND np.quarter = ta.quarter\n",
    "        WHERE np.netprofit_ttm > 0 \n",
    "        AND ta.totalassets_ttm > 0\n",
    "        ORDER BY np.ticker, np.year, np.quarter\n",
    "    \"\"\")\n",
    "    \n",
    "    fundamental_data = pd.read_sql(fundamental_query, db_engine, params={'start_date': buffer_start_date, 'end_date': end_date})\n",
    "    \n",
    "    # Process data\n",
    "    print(\"   - Processing data...\")\n",
    "    \n",
    "    # Create returns matrix\n",
    "    price_pivot = price_data.pivot(index='date', columns='ticker', values='close')\n",
    "    returns_matrix = price_pivot.pct_change().dropna()\n",
    "    \n",
    "    # Create benchmark returns\n",
    "    benchmark_returns = benchmark_data.set_index('date')['close'].pct_change().dropna()\n",
    "    \n",
    "    # Align dates\n",
    "    common_dates = returns_matrix.index.intersection(benchmark_returns.index)\n",
    "    returns_matrix = returns_matrix.loc[common_dates]\n",
    "    benchmark_returns = benchmark_returns.loc[common_dates]\n",
    "    \n",
    "    print(f\"   ✅ Data loaded successfully:\")\n",
    "    print(f\"      - Price data: {len(price_data)} records\")\n",
    "    print(f\"      - Returns matrix: {returns_matrix.shape}\")\n",
    "    print(f\"      - Benchmark returns: {len(benchmark_returns)} records\")\n",
    "    print(f\"      - Common dates: {len(common_dates)}\")\n",
    "    \n",
    "    return price_data, fundamental_data, returns_matrix, benchmark_returns\n",
    "\n",
    "# PERFORMANCE METRICS AND TEARSHEET\n",
    "def calculate_performance_metrics(returns: pd.Series, benchmark: pd.Series, periods_per_year: int = 252) -> dict:\n",
    "    \"\"\"Calculate comprehensive performance metrics.\"\"\"\n",
    "    \n",
    "    # Align returns and benchmark\n",
    "    aligned_data = pd.concat([returns, benchmark], axis=1).dropna()\n",
    "    strategy_returns = aligned_data.iloc[:, 0]\n",
    "    benchmark_returns = aligned_data.iloc[:, 1]\n",
    "    \n",
    "    # Basic metrics\n",
    "    total_return = (1 + strategy_returns).prod() - 1\n",
    "    benchmark_total_return = (1 + benchmark_returns).prod() - 1\n",
    "    \n",
    "    # Annualized metrics\n",
    "    years = len(strategy_returns) / periods_per_year\n",
    "    annualized_return = (1 + total_return) ** (1 / years) - 1\n",
    "    benchmark_annualized_return = (1 + benchmark_total_return) ** (1 / years) - 1\n",
    "    \n",
    "    # Risk metrics\n",
    "    volatility = strategy_returns.std() * np.sqrt(periods_per_year)\n",
    "    benchmark_volatility = benchmark_returns.std() * np.sqrt(periods_per_year)\n",
    "    \n",
    "    # Sharpe ratio\n",
    "    risk_free_rate = 0.02  # Assume 2% risk-free rate\n",
    "    sharpe_ratio = (annualized_return - risk_free_rate) / volatility\n",
    "    benchmark_sharpe = (benchmark_annualized_return - risk_free_rate) / benchmark_volatility\n",
    "    \n",
    "    # Maximum drawdown\n",
    "    cumulative_returns = (1 + strategy_returns).cumprod()\n",
    "    running_max = cumulative_returns.expanding().max()\n",
    "    drawdown = (cumulative_returns - running_max) / running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    benchmark_cumulative = (1 + benchmark_returns).cumprod()\n",
    "    benchmark_running_max = benchmark_cumulative.expanding().max()\n",
    "    benchmark_drawdown = (benchmark_cumulative - benchmark_running_max) / benchmark_running_max\n",
    "    benchmark_max_drawdown = benchmark_drawdown.min()\n",
    "    \n",
    "    # Information ratio\n",
    "    excess_returns = strategy_returns - benchmark_returns\n",
    "    information_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(periods_per_year)\n",
    "    \n",
    "    # Beta\n",
    "    covariance = np.cov(strategy_returns, benchmark_returns)[0, 1]\n",
    "    benchmark_variance = np.var(benchmark_returns)\n",
    "    beta = covariance / benchmark_variance\n",
    "    \n",
    "    # Calmar ratio\n",
    "    calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown != 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'total_return': total_return,\n",
    "        'annualized_return': annualized_return,\n",
    "        'volatility': volatility,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'max_drawdown': max_drawdown,\n",
    "        'calmar_ratio': calmar_ratio,\n",
    "        'information_ratio': information_ratio,\n",
    "        'beta': beta,\n",
    "        'benchmark_total_return': benchmark_total_return,\n",
    "        'benchmark_annualized_return': benchmark_annualized_return,\n",
    "        'benchmark_volatility': benchmark_volatility,\n",
    "        'benchmark_sharpe': benchmark_sharpe,\n",
    "        'benchmark_max_drawdown': benchmark_max_drawdown\n",
    "    }\n",
    "\n",
    "def generate_comprehensive_tearsheet(strategy_returns: pd.Series, benchmark_returns: pd.Series, diagnostics: pd.DataFrame, title: str):\n",
    "    \"\"\"Generate comprehensive tearsheet with optimized strategy results.\"\"\"\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    metrics = calculate_performance_metrics(strategy_returns, benchmark_returns)\n",
    "    \n",
    "    # Set up plotting\n",
    "    plt.switch_backend('Agg')  # Use non-interactive backend\n",
    "    fig = plt.figure(figsize=(20, 16))\n",
    "    gs = fig.add_gridspec(4, 2, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Equity Curve\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    cumulative_strategy = (1 + strategy_returns).cumprod()\n",
    "    cumulative_benchmark = (1 + benchmark_returns).cumprod()\n",
    "    \n",
    "    ax1.plot(cumulative_strategy.index, cumulative_strategy.values, label='Optimized Strategy', linewidth=2, color='#2E86AB')\n",
    "    ax1.plot(cumulative_benchmark.index, cumulative_benchmark.values, label='VNINDEX Benchmark', linewidth=2, color='#A23B72', alpha=0.7)\n",
    "    ax1.set_title('Optimized Strategy vs Benchmark Performance', fontweight='bold', fontsize=14)\n",
    "    ax1.set_ylabel('Cumulative Return')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Drawdown Analysis\n",
    "    ax2 = fig.add_subplot(gs[1, 0])\n",
    "    running_max = cumulative_strategy.expanding().max()\n",
    "    drawdown = (cumulative_strategy - running_max) / running_max\n",
    "    ax2.fill_between(drawdown.index, drawdown.values, 0, alpha=0.3, color='red')\n",
    "    ax2.plot(drawdown.index, drawdown.values, color='red', linewidth=1)\n",
    "    ax2.set_title('Strategy Drawdown', fontweight='bold')\n",
    "    ax2.set_ylabel('Drawdown')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Annual Returns\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    annual_returns = strategy_returns.groupby(strategy_returns.index.year).apply(lambda x: (1 + x).prod() - 1)\n",
    "    benchmark_annual = benchmark_returns.groupby(benchmark_returns.index.year).apply(lambda x: (1 + x).prod() - 1)\n",
    "    \n",
    "    x = np.arange(len(annual_returns))\n",
    "    width = 0.35\n",
    "    ax3.bar(x - width/2, annual_returns.values, width, label='Strategy', color='#2E86AB', alpha=0.7)\n",
    "    ax3.bar(x + width/2, benchmark_annual.values, width, label='Benchmark', color='#A23B72', alpha=0.7)\n",
    "    ax3.set_title('Annual Returns', fontweight='bold')\n",
    "    ax3.set_ylabel('Return')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(annual_returns.index)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Rolling Sharpe Ratio\n",
    "    ax4 = fig.add_subplot(gs[2, 0])\n",
    "    rolling_sharpe = strategy_returns.rolling(252).mean() / strategy_returns.rolling(252).std() * np.sqrt(252)\n",
    "    ax4.plot(rolling_sharpe.index, rolling_sharpe.values, color='#2E86AB', linewidth=2)\n",
    "    ax4.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax4.set_title('Rolling Sharpe Ratio (252-day)', fontweight='bold')\n",
    "    ax4.set_ylabel('Sharpe Ratio')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Portfolio Turnover\n",
    "    ax5 = fig.add_subplot(gs[2, 1])\n",
    "    if not diagnostics.empty and 'turnover' in diagnostics.columns:\n",
    "        diagnostics['turnover'].plot(ax=ax5, color='#E67E22', linewidth=2)\n",
    "        ax5.set_title('Portfolio Turnover', fontweight='bold')\n",
    "        ax5.set_ylabel('Turnover Rate')\n",
    "        ax5.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # 6. Performance Metrics Table\n",
    "    ax6 = fig.add_subplot(gs[3, :])\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    # Create metrics table\n",
    "    metrics_data = [\n",
    "        ['Metric', 'Strategy', 'Benchmark'],\n",
    "        ['Annualized Return', f\"{metrics['annualized_return']:.2%}\", f\"{metrics['benchmark_annualized_return']:.2%}\"],\n",
    "        ['Volatility', f\"{metrics['volatility']:.2%}\", f\"{metrics['benchmark_volatility']:.2%}\"],\n",
    "        ['Sharpe Ratio', f\"{metrics['sharpe_ratio']:.2f}\", f\"{metrics['benchmark_sharpe']:.2f}\"],\n",
    "        ['Max Drawdown', f\"{metrics['max_drawdown']:.2%}\", f\"{metrics['benchmark_max_drawdown']:.2%}\"],\n",
    "        ['Information Ratio', f\"{metrics['information_ratio']:.2f}\", '-'],\n",
    "        ['Beta', f\"{metrics['beta']:.2f}\", '1.00'],\n",
    "        ['Calmar Ratio', f\"{metrics['calmar_ratio']:.2f}\", '-']\n",
    "    ]\n",
    "    \n",
    "    table = ax6.table(cellText=metrics_data[1:], colLabels=metrics_data[0], \n",
    "                     cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(metrics_data)):\n",
    "        for j in range(len(metrics_data[0])):\n",
    "            if i == 0:  # Header row\n",
    "                table[(i, j)].set_facecolor('#2E86AB')\n",
    "                table[(i, j)].set_text_props(weight='bold', color='white')\n",
    "            else:\n",
    "                table[(i, j)].set_facecolor('#F8F9FA')\n",
    "    \n",
    "    ax6.set_title('Performance Metrics Summary', fontweight='bold', fontsize=14, pad=20)\n",
    "    \n",
    "    # Save the plot\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"tearsheet_optimized_{timestamp}.png\"\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"✅ Tearsheet saved as: {filename}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# MAIN EXECUTION\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🚀 QVM ENGINE V3J OPTIMIZED STRATEGY EXECUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # 1. Database Connection\n",
    "        print(\"📊 Step 1: Establishing database connection...\")\n",
    "        db_engine = create_db_connection()\n",
    "        \n",
    "        # 2. Load Data\n",
    "        print(\"\\n📊 Step 2: Loading data...\")\n",
    "        price_data, fundamental_data, returns_matrix, benchmark_returns = load_all_data_for_backtest(QVM_CONFIG, db_engine)\n",
    "        \n",
    "        # 3. Precompute Data\n",
    "        print(\"\\n📊 Step 3: Precomputing data...\")\n",
    "        precomputed_data = precompute_all_data(QVM_CONFIG, db_engine)\n",
    "        \n",
    "        # 4. Initialize Engine\n",
    "        print(\"\\n📊 Step 4: Initializing optimized engine...\")\n",
    "        engine = QVMEngineV3jOptimized(\n",
    "            QVM_CONFIG, price_data, fundamental_data, \n",
    "            returns_matrix, benchmark_returns, db_engine, precomputed_data\n",
    "        )\n",
    "        \n",
    "        # 5. Run Backtest\n",
    "        print(\"\\n📊 Step 5: Running optimized backtest...\")\n",
    "        strategy_returns, diagnostics = engine.run_backtest()\n",
    "        \n",
    "        # 6. Generate Tearsheet\n",
    "        print(\"\\n📊 Step 6: Generating optimized tearsheet...\")\n",
    "        metrics = generate_comprehensive_tearsheet(strategy_returns, benchmark_returns, diagnostics, \"QVM Engine v3j Optimized\")\n",
    "        \n",
    "        # 7. Performance Summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"📊 QVM ENGINE V3J: OPTIMIZED STRATEGY RESULTS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"📈 Performance Summary:\")\n",
    "        print(f\"   - Strategy Annualized Return: {metrics['annualized_return']:.2%}\")\n",
    "        print(f\"   - Benchmark Annualized Return: {metrics['benchmark_annualized_return']:.2%}\")\n",
    "        print(f\"   - Strategy Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\n",
    "        print(f\"   - Benchmark Sharpe Ratio: {metrics['benchmark_sharpe']:.2f}\")\n",
    "        print(f\"   - Strategy Max Drawdown: {metrics['max_drawdown']:.2%}\")\n",
    "        print(f\"   - Benchmark Max Drawdown: {metrics['benchmark_max_drawdown']:.2%}\")\n",
    "        print(f\"   - Information Ratio: {metrics['information_ratio']:.2f}\")\n",
    "        print(f\"   - Beta: {metrics['beta']:.2f}\")\n",
    "        \n",
    "        print(f\"\\n🔧 Optimized Configuration:\")\n",
    "        print(f\"   - Simplified 3-factor structure (ROAA, P/E, Momentum)\")\n",
    "        print(f\"   - Removed redundant factors (F-Score, FCF Yield, Low-Volatility)\")\n",
    "        print(f\"   - Optimized factor weights based on investigation\")\n",
    "        print(f\"   - Improved data quality handling\")\n",
    "        \n",
    "        print(f\"\\n✅ QVM Engine v3j Optimized strategy execution complete!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
