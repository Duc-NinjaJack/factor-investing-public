{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eeea9a",
   "metadata": {},
   "source": [
    "# SECTION 1: FINAL PRODUCTION STRATEGY DEFINITION\n",
    "\n",
    "This section provides the definitive technical specification for the **\"Aureus Sigma Vietnam Value Concentrated\"** strategy. This specification is the final output of all research and validation conducted in Phases 1 through 20 and represents the exact logic that will be deployed for live trading.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1 Investment Philosophy & Objectives**\n",
    "\n",
    "- **Philosophy:** A systematic, quantitative strategy designed to capture the Value premium in a concentrated portfolio of liquid Vietnamese equities, while actively managing downside risk through a hybrid, rules-based overlay.\n",
    "- **Primary Objective:** Deliver superior, risk-adjusted alpha over a full market cycle.\n",
    "- **Secondary Objective:** Adhere to institutional risk constraints, specifically a maximum drawdown target of **-35%** and a fixed, manageable number of holdings.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Technical Specification Table**\n",
    "\n",
    "| Component | Parameter | Specification | Rationale & Validation Source |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Universe** | **Definition** | **ASC-VN-Liquid-150** (Top ~150-200 stocks) | Validated in Phase 12 as the optimal tradable universe. |\n",
    "| | **Liquidity Filter** | 63-day ADTV > 10 Billion VND | Determined in Phase 10 Liquidity Deep Dive to balance size and tradability. |\n",
    "| | **Reconstruction** | **Quarterly, Dynamic** | Proven in Phase 16b to eliminate survivorship bias and capture market evolution. |\n",
    "| **Alpha Signal**| **Factor** | **Standalone Value** | Proven in Phase 16b to be the most robust and powerful alpha signal (Sharpe 2.60) on a standalone basis. |\n",
    "| | **Metric** | `Value_Composite` from `qvm_engine_v2_enhanced` | Engine validated through extensive unit testing in early project phases. |\n",
    "| **Portfolio**| **Construction** | **Select Top 25 Stocks** | **New Constraint.** Industry standard for a high-conviction, manageable active portfolio. Addresses feedback on impractical diversification. |\n",
    "| **Construction**| **Weighting** | **Equal-Weighted** | Simple, robust, and prevents single-stock concentration. |\n",
    "| | **Sector Constraint** | **Max 35% per Sector** | **New Constraint.** Prevents unintended sector bets and ensures risk is driven by the Value factor, not sector concentration. |\n",
    "| **Rebalancing**| **Frequency** | **Quarterly** | Proven in Phase 17 sensitivity analysis to be the optimal balance of signal freshness and cost efficiency. |\n",
    "| **Risk Overlay**| **Model** | **Hybrid (Regime + Volatility)** | Validated in Phase 18 as the most effective model for improving risk-adjusted returns. |\n",
    "| | **Regime Layer** | **50% Exposure** during \"Bear\" or \"Stress\" periods. | Acts as a primary, decisive circuit-breaker during major market dislocations. |\n",
    "| | **Volatility Layer** | Target **15% Annualized Volatility** (60-day lookback). | Provides fine-tuned, dynamic risk adjustments based on the portfolio's own realized volatility. |\n",
    "| | **Final Exposure** | **MINIMUM(Regime Exposure, Volatility Exposure)** | Ensures the most conservative risk posture is always taken. |\n",
    "| **Execution** | **Transaction Costs** | **30 bps per trade** | Baseline assumption for commissions and market impact. To be stress-tested. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "539e4251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading all raw data required for the final backtest...\n",
      "   âœ… Loaded 1,567,488 raw factor observations.\n",
      "   âœ… Loaded and processed 2,339,155 raw price observations.\n",
      "   âœ… Loaded market regime signals.\n",
      "   âœ… Loaded sector mappings for 728 tickers.\n",
      "\n",
      "ðŸ“Š Data Integrity Validation:\n",
      "   - Factor Data: (1567488, 5)\n",
      "   - Daily Returns Matrix: (4129, 728)\n",
      "   - Market Regimes: 2381 days\n",
      "   - Sector Mappings: 728 tickers\n",
      "\n",
      "âœ… All data loaded. Ready to define the final backtesting engine.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: DATA LOADING & PREPARATION (CORRECTED)\n",
    "# ============================================================================\n",
    "import pandas as pd # *** CRITICAL FIX: Added missing pandas import ***\n",
    "import numpy as np # *** CRITICAL FIX: Added missing numpy import ***\n",
    "from sqlalchemy import create_engine, text\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# --- Helper function to create DB connection ---\n",
    "def create_db_connection():\n",
    "    # Assumes notebook is in a subfolder like `tests/phase21...`\n",
    "    config_path = Path.cwd().parent.parent.parent / 'config' / 'database.yml'\n",
    "    if not config_path.exists():\n",
    "        # Fallback for different directory structures\n",
    "        config_path = Path.cwd() / 'config' / 'database.yml'\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(\"Could not locate database.yml\")\n",
    "            \n",
    "    with open(config_path, 'r') as f:\n",
    "        db_config = yaml.safe_load(f)['production']\n",
    "    connection_string = (\n",
    "        f\"mysql+pymysql://{db_config['username']}:{db_config['password']}\"\n",
    "        f\"@{db_config['host']}/{db_config['schema_name']}\"\n",
    "    )\n",
    "    return create_engine(connection_string, pool_pre_ping=True)\n",
    "\n",
    "print(\"ðŸ“‚ Loading all raw data required for the final backtest...\")\n",
    "\n",
    "# --- 1. Database Connection & Date Range ---\n",
    "engine = create_db_connection()\n",
    "db_params = {\n",
    "    'start_date': \"2015-Û±Û²-01\", # Start early to ensure data for first rebalance\n",
    "    'end_date': \"2025-07-28\"\n",
    "}\n",
    "\n",
    "# --- 2. Load Factor Scores (Full Universe) ---\n",
    "factor_query = text(\"\"\"\n",
    "    SELECT date, ticker, Quality_Composite, Value_Composite, Momentum_Composite\n",
    "    FROM factor_scores_qvm\n",
    "    WHERE date BETWEEN :start_date AND :end_date AND strategy_version = 'qvm_v2.0_enhanced'\n",
    "\"\"\")\n",
    "factor_data_raw = pd.read_sql(factor_query, engine, params=db_params, parse_dates=['date'])\n",
    "print(f\"   âœ… Loaded {len(factor_data_raw):,} raw factor observations.\")\n",
    "\n",
    "# --- 3. Load Price Data & Calculate Returns ---\n",
    "price_query = text(\"SELECT date, ticker, close FROM equity_history WHERE date BETWEEN :start_date AND :end_date\")\n",
    "price_data_raw = pd.read_sql(price_query, engine, params=db_params, parse_dates=['date'])\n",
    "price_data_raw['return'] = price_data_raw.groupby('ticker')['close'].pct_change()\n",
    "daily_returns_matrix = price_data_raw.pivot(index='date', columns='ticker', values='return')\n",
    "print(f\"   âœ… Loaded and processed {len(price_data_raw):,} raw price observations.\")\n",
    "\n",
    "# --- 4. Load Market Regime Signals ---\n",
    "project_root = Path.cwd().parent.parent\n",
    "archive_path = project_root / \"tests\" / \"phase8_risk_management\"\n",
    "phase8_results_file = archive_path / \"phase8_results.pkl\"\n",
    "with open(phase8_results_file, \"rb\") as f:\n",
    "    phase8_results = pickle.load(f)\n",
    "market_regimes = phase8_results['market_regimes']\n",
    "market_regimes['risk_on'] = ~market_regimes['regime'].isin(['Bear', 'Stress'])\n",
    "print(\"   âœ… Loaded market regime signals.\")\n",
    "\n",
    "# --- 5. Load Sector Mappings ---\n",
    "sector_info_query = text(\"SELECT ticker, sector FROM master_info WHERE sector IS NOT NULL\")\n",
    "sector_info = pd.read_sql(sector_info_query, engine).drop_duplicates(subset=['ticker']).set_index('ticker')\n",
    "print(f\"   âœ… Loaded sector mappings for {len(sector_info)} tickers.\")\n",
    "\n",
    "engine.dispose()\n",
    "\n",
    "# --- 6. Data Integrity Check ---\n",
    "print(\"\\nðŸ“Š Data Integrity Validation:\")\n",
    "print(f\"   - Factor Data: {factor_data_raw.shape}\")\n",
    "print(f\"   - Daily Returns Matrix: {daily_returns_matrix.shape}\")\n",
    "print(f\"   - Market Regimes: {len(market_regimes)} days\")\n",
    "print(f\"   - Sector Mappings: {len(sector_info)} tickers\")\n",
    "\n",
    "print(\"\\nâœ… All data loaded. Ready to define the final backtesting engine.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rebalance_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 1. Establish Rebalancing Schedule ---\u001b[39;00m\n\u001b[32m      2\u001b[39m all_trading_dates = daily_returns_matrix.index\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m freq_ends = pd.date_range(start=all_trading_dates.min(), end=all_trading_dates.max(), freq=\u001b[43mrebalance_freq\u001b[49m)\n\u001b[32m      4\u001b[39m rebalance_dates = [\n\u001b[32m      5\u001b[39m     all_trading_dates[all_trading_dates.searchsorted(d, side=\u001b[33m'\u001b[39m\u001b[33mright\u001b[39m\u001b[33m'\u001b[39m) - \u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m freq_ends\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   - Generated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rebalance_dates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rebalance dates.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'rebalance_freq' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 1. Establish Rebalancing Schedule ---\n",
    "all_trading_dates = daily_returns_matrix.index\n",
    "freq_ends = pd.date_range(start=all_trading_dates.min(), end=all_trading_dates.max(), freq=rebalance_freq)\n",
    "rebalance_dates = [\n",
    "    all_trading_dates[all_trading_dates.searchsorted(d, side='right') - 1] for d in freq_ends\n",
    "]\n",
    "print(f\"   - Generated {len(rebalance_dates)} rebalance dates.\")\n",
    "\n",
    "# --- 2. Generate Daily Holdings Matrix ---\n",
    "aggressive_holdings = pd.DataFrame(0.0, index=all_trading_dates, columns=daily_returns_matrix.columns)\n",
    "\n",
    "for i in range(len(rebalance_dates)):\n",
    "    rebal_date = rebalance_dates[i]\n",
    "    \n",
    "    # a. Construct the liquid universe for this date\n",
    "    universe_df = get_liquid_universe_dataframe(\n",
    "        analysis_date=rebal_date, engine=engine,\n",
    "        config={'lookback_days': 63, 'adtv_threshold_bn': 10.0, 'top_n': 200, 'min_trading_coverage': 0.6}\n",
    "    )\n",
    "    if universe_df.empty or len(universe_df) < 50: continue\n",
    "    \n",
    "    # b. Filter factor data to this universe and date\n",
    "    factors_on_date = factor_data_raw[\n",
    "        (factor_data_raw['date'] == rebal_date) &\n",
    "        (factor_data_raw['ticker'].isin(universe_df['ticker']))\n",
    "    ].copy()\n",
    "    if len(factors_on_date) < 25: continue\n",
    "\n",
    "    # c. Select top 25 stocks based on the pure Value signal\n",
    "    top_25_stocks = factors_on_date.nlargest(25, 'Value_Composite').copy()\n",
    "    \n",
    "    # d. Apply Sector Constraints\n",
    "    top_25_with_sectors = top_25_stocks.join(sector_info, on='ticker')\n",
    "    sector_counts = top_25_with_sectors['sector'].value_counts()\n",
    "    max_stocks_per_sector = int(25 * 0.35) # Max 35% = 8 stocks\n",
    "    \n",
    "    final_tickers = set()\n",
    "    for sector, count in sector_counts.items():\n",
    "        sector_stocks = top_25_with_sectors[top_25_with_sectors['sector'] == sector]\n",
    "        num_to_keep = min(count, max_stocks_per_sector)\n",
    "        final_tickers.update(sector_stocks.head(num_to_keep)['ticker'])\n",
    "    \n",
    "    final_portfolio_df = top_25_stocks[top_25_stocks['ticker'].isin(final_tickers)]\n",
    "    \n",
    "    # e. Assign equal weights\n",
    "    if not final_portfolio_df.empty:\n",
    "        weight = 1.0 / len(final_portfolio_df)\n",
    "        portfolio_weights = pd.Series(weight, index=final_portfolio_df['ticker'])\n",
    "\n",
    "        # f. Propagate weights to the daily holdings matrix\n",
    "        start_period = rebal_date + pd.Timedelta(days=1)\n",
    "        end_period = rebalance_dates[i+1] if i + 1 < len(rebalance_dates) else all_trading_dates.max()\n",
    "        holding_dates = aggressive_holdings.index[(aggressive_holdings.index >= start_period) & (aggressive_holdings.index <= end_period)]\n",
    "        valid_tickers = portfolio_weights.index.intersection(aggressive_holdings.columns)\n",
    "        aggressive_holdings.loc[holding_dates, valid_tickers] = portfolio_weights[valid_tickers].values\n",
    "\n",
    "print(\"   - Aggressive Growth holdings matrix constructed.\")\n",
    "\n",
    "# --- 3. Apply Hybrid Risk Overlay ---\n",
    "aggressive_returns_temp = (aggressive_holdings.shift(1).fillna(0.0) * daily_returns_matrix).sum(axis=1)\n",
    "realized_vol = aggressive_returns_temp.rolling(window=60).std() * np.sqrt(252)\n",
    "vol_exposure = (0.15 / realized_vol).shift(1).clip(lower=0.2, upper=1.0).fillna(1.0)\n",
    "regime_exposure = market_regimes['risk_on'].apply(lambda x: 1.0 if x else 0.5)\n",
    "common_index = vol_exposure.index.intersection(regime_exposure.index)\n",
    "hybrid_exposure = pd.DataFrame({'vol': vol_exposure.loc[common_index], 'regime': regime_exposure.loc[common_index]}).min(axis=1)\n",
    "risk_managed_holdings = aggressive_holdings.mul(hybrid_exposure, axis=0)\n",
    "print(\"   - Hybrid risk overlay applied.\")\n",
    "\n",
    "# --- 4. Calculate Final Net Returns ---\n",
    "holdings_shifted = risk_managed_holdings.shift(1).fillna(0.0)\n",
    "gross_returns = (holdings_shifted * daily_returns_matrix).sum(axis=1)\n",
    "turnover = (holdings_shifted - holdings_shifted.shift(1)).abs().sum(axis=1) / 2\n",
    "costs = turnover * (30 / 10000)\n",
    "net_returns = gross_returns - costs\n",
    "print(\"   - Final net returns calculated.\")\n",
    "\n",
    "return net_returns.rename(strategy_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8d7071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vn_factor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
