{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Robustness Testing - Quantifying Model Risk\n",
    "## Vietnam Factor Investing Platform - Phase 7 Institutional Backtesting Framework\n",
    "\n",
    "**Objective**: Quantify the fragility of our model by testing sensitivity to core assumptions. We need to determine at what point our 1.52 Sharpe ratio breaks down.\n",
    "\n",
    "**Key Questions**:\n",
    "1. How sensitive are returns to transaction costs?\n",
    "2. Does rebalancing frequency materially impact performance?\n",
    "3. What happens if we change the selection percentile?\n",
    "4. How do sector and position constraints affect results?\n",
    "\n",
    "**Success Criteria**:\n",
    "- Generate tornado plots showing parameter sensitivity\n",
    "- Identify which parameters are most critical\n",
    "- Determine the \"breaking points\" of the strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Aureus Sigma Capital - Robustness Testing Framework\n",
    "# Notebook: 05_robustness_testing.ipynb\n",
    "#\n",
    "# Description:\n",
    "# This notebook systematically tests the sensitivity of our QVM strategy to\n",
    "# various parameter assumptions, quantifying model risk and identifying\n",
    "# critical breaking points.\n",
    "#\n",
    "# Author: Duc Nguyen, Quantitative Finance Expert\n",
    "# Date: July 27, 2025\n",
    "# Version: 1.0 - Institutional Robustness Framework\n",
    "# ============================================================================\n",
    "\n",
    "# Core imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import yaml\n",
    "from sqlalchemy import create_engine\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- INSTITUTIONAL PALETTE ---\n",
    "FACTOR_COLORS = {\n",
    "    'Strategy': '#16A085', 'Benchmark': '#34495E', 'Positive': '#27AE60',\n",
    "    'Negative': '#C0392B', 'Drawdown': '#E67E22', 'Sharpe': '#2980B9',\n",
    "    'Grid': '#BDC3C7', 'Text_Primary': '#2C3E50', 'Neutral': '#7F8C8D',\n",
    "    'Baseline': '#3498DB', 'Sensitive': '#E74C3C', 'Robust': '#2ECC71'\n",
    "}\n",
    "\n",
    "# --- VISUALIZATION CONFIGURATION ---\n",
    "plt.style.use('default')\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 300, 'savefig.dpi': 300, 'figure.figsize': (15, 8),\n",
    "    'figure.facecolor': 'white', 'font.size': 11,\n",
    "    'axes.facecolor': 'white', 'axes.edgecolor': FACTOR_COLORS['Text_Primary'],\n",
    "    'axes.linewidth': 1.0, 'axes.grid': True, 'axes.axisbelow': True,\n",
    "    'axes.labelcolor': FACTOR_COLORS['Text_Primary'], 'axes.titlesize': 14,\n",
    "    'axes.titleweight': 'bold', 'axes.titlecolor': FACTOR_COLORS['Text_Primary'],\n",
    "    'grid.color': FACTOR_COLORS['Grid'], 'grid.alpha': 0.3, 'grid.linewidth': 0.5,\n",
    "    'legend.frameon': False, 'legend.fontsize': 10,\n",
    "    'xtick.color': FACTOR_COLORS['Text_Primary'], 'ytick.color': FACTOR_COLORS['Text_Primary'],\n",
    "    'xtick.labelsize': 10, 'ytick.labelsize': 10,\n",
    "    'lines.linewidth': 2.0, 'lines.solid_capstyle': 'round'\n",
    "})\n",
    "\n",
    "print(\"📊 Robustness testing environment configured.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"🔬 Aureus Sigma: Robustness Testing Framework\")\n",
    "print(f\"   Version: 1.0 - Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\n📊 Sensitivity Tests to Execute:\")\n",
    "print(\"   1. Transaction Cost Sensitivity\")\n",
    "print(\"   2. Rebalancing Frequency Impact\")\n",
    "print(\"   3. Selection Percentile Analysis\")\n",
    "print(\"   4. Sector Constraint Variations\")\n",
    "print(\"   5. Position Limit Effects\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Core Data and Baseline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from previous notebooks\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / 'production').exists() and not (project_root / 'config').exists():\n",
    "    if project_root.parent == project_root:\n",
    "        raise FileNotFoundError(\"Could not find project root\")\n",
    "    project_root = project_root.parent\n",
    "\n",
    "data_path = project_root / \"production\" / \"tests\" / \"phase7_institutional_backtesting\"\n",
    "\n",
    "print(\"📂 Loading core data objects and baseline results...\")\n",
    "\n",
    "# Load the core data objects\n",
    "with open(data_path / \"factor_data.pkl\", \"rb\") as f:\n",
    "    factor_data_obj = pickle.load(f)\n",
    "with open(data_path / \"daily_returns.pkl\", \"rb\") as f:\n",
    "    returns_data_obj = pickle.load(f)\n",
    "with open(data_path / \"benchmark_returns.pkl\", \"rb\") as f:\n",
    "    benchmark_data_obj = pickle.load(f)\n",
    "\n",
    "# Extract data\n",
    "factor_data = factor_data_obj['data']\n",
    "daily_returns = returns_data_obj['data']\n",
    "benchmark_returns = benchmark_data_obj['data']\n",
    "\n",
    "# Load baseline backtest results\n",
    "with open(data_path / \"canonical_backtest_results.pkl\", \"rb\") as f:\n",
    "    baseline_results = pickle.load(f)\n",
    "\n",
    "baseline_performance = baseline_results['performance_summary']\n",
    "baseline_config = baseline_results['strategy_config']\n",
    "\n",
    "print(\"\\n✅ Baseline Performance Loaded:\")\n",
    "print(f\"   Annual Return: {baseline_performance['annual_return']:.2%}\")\n",
    "print(f\"   Sharpe Ratio: {baseline_performance['sharpe_ratio']:.2f}\")\n",
    "print(f\"   Annual Volatility: {baseline_performance['annual_vol']:.2%}\")\n",
    "print(f\"   Total Return: {baseline_performance['total_return']:.2%}\")\n",
    "\n",
    "# Load sector mappings\n",
    "print(\"\\n🏗️ Loading sector information...\")\n",
    "config_path = project_root / 'config' / 'database.yml'\n",
    "with open(config_path, 'r') as f:\n",
    "    db_config = yaml.safe_load(f)['production']\n",
    "    \n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{db_config['username']}:{db_config['password']}@\"\n",
    "    f\"{db_config['host']}/{db_config['schema_name']}\"\n",
    ")\n",
    "\n",
    "sector_info = pd.read_sql(\"SELECT ticker, sector FROM master_info WHERE sector IS NOT NULL\", engine)\n",
    "sector_info = sector_info.drop_duplicates(subset=['ticker']).set_index('ticker')\n",
    "engine.dispose()\n",
    "\n",
    "print(f\"✅ Loaded sector mappings for {len(sector_info)} tickers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Backtesting Engine from Notebook 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the backtesting functions from Notebook 03\n",
    "# We'll copy the key functions here for self-contained execution\n",
    "\n",
    "def construct_constrained_portfolio(\n",
    "    factor_scores: pd.Series, \n",
    "    sector_info: pd.DataFrame, \n",
    "    config: dict\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Constructs a single, constrained portfolio for a given rebalance date.\n",
    "    \"\"\"\n",
    "    if factor_scores.empty:\n",
    "        return pd.DataFrame(columns=['weight', 'sector'])\n",
    "\n",
    "    # Select top stocks based on percentile\n",
    "    top_percentile_cutoff = factor_scores.quantile(1 - config['selection_percentile'])\n",
    "    selected_stocks_df = factor_scores[factor_scores >= top_percentile_cutoff].to_frame('factor_score')\n",
    "    \n",
    "    # Merge with sector information\n",
    "    portfolio_df = selected_stocks_df.join(sector_info)\n",
    "    \n",
    "    # Handle potential missing sectors after join\n",
    "    if portfolio_df['sector'].isnull().any():\n",
    "        portfolio_df.dropna(subset=['sector'], inplace=True)\n",
    "\n",
    "    if portfolio_df.empty:\n",
    "        return pd.DataFrame(columns=['weight', 'sector'])\n",
    "\n",
    "    # Apply sector constraints\n",
    "    sector_counts = portfolio_df['sector'].value_counts()\n",
    "    max_stocks_in_portfolio = len(portfolio_df)\n",
    "    max_stocks_per_sector = int(max_stocks_in_portfolio * config['max_sector_weight'])\n",
    "    \n",
    "    final_tickers = set()\n",
    "    for sector, count in sector_counts.items():\n",
    "        sector_stocks = portfolio_df[portfolio_df['sector'] == sector]\n",
    "        if count > max_stocks_per_sector and max_stocks_per_sector > 0:\n",
    "            top_in_sector = sector_stocks.nlargest(max_stocks_per_sector, 'factor_score').index\n",
    "            final_tickers.update(top_in_sector)\n",
    "        else:\n",
    "            final_tickers.update(sector_stocks.index)\n",
    "            \n",
    "    final_portfolio = portfolio_df.loc[list(final_tickers)].copy()\n",
    "    \n",
    "    # Assign equal weights\n",
    "    num_stocks = len(final_portfolio)\n",
    "    if num_stocks > 0:\n",
    "        final_portfolio['weight'] = 1.0 / num_stocks\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['weight', 'sector'])\n",
    "        \n",
    "    # Apply position weight cap if specified\n",
    "    if 'max_position_weight' in config:\n",
    "        max_pos_weight = config['max_position_weight']\n",
    "        if final_portfolio['weight'].max() > max_pos_weight:\n",
    "            # Rescale weights to comply with position limit\n",
    "            final_portfolio['weight'] = final_portfolio['weight'].clip(upper=max_pos_weight)\n",
    "            final_portfolio['weight'] = final_portfolio['weight'] / final_portfolio['weight'].sum()\n",
    "\n",
    "    return final_portfolio[['weight', 'sector']]\n",
    "\n",
    "\n",
    "def run_backtest_with_config(\n",
    "    qvm_scores: pd.DataFrame,\n",
    "    daily_returns: pd.DataFrame,\n",
    "    sector_info: pd.DataFrame,\n",
    "    config: dict\n",
    ") -> Tuple[pd.Series, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Runs the backtest with a specific configuration.\n",
    "    Returns net returns series and performance metrics.\n",
    "    \"\"\"\n",
    "    # 1. IDENTIFY REBALANCE DATES\n",
    "    ideal_rebalance_dates = pd.date_range(\n",
    "        start=qvm_scores.index.min(), \n",
    "        end=qvm_scores.index.max(), \n",
    "        freq=config['rebalance_freq']\n",
    "    )\n",
    "\n",
    "    # 2. Construct Daily Holdings Matrix\n",
    "    daily_holdings = pd.DataFrame(index=daily_returns.index, columns=daily_returns.columns).fillna(0.0)\n",
    "    \n",
    "    factor_scores_on_rebal_dates = qvm_scores.reindex(ideal_rebalance_dates, method='ffill')\n",
    "\n",
    "    for i in range(len(factor_scores_on_rebal_dates.index)):\n",
    "        rebal_date = factor_scores_on_rebal_dates.index[i]\n",
    "        \n",
    "        try:\n",
    "            next_rebal_date = factor_scores_on_rebal_dates.index[i+1]\n",
    "        except IndexError:\n",
    "            next_rebal_date = daily_returns.index[-1] + pd.Timedelta(days=1)\n",
    "\n",
    "        factor_scores_at_rebal = factor_scores_on_rebal_dates.loc[rebal_date].dropna()\n",
    "        \n",
    "        if len(factor_scores_at_rebal) > 20:\n",
    "            portfolio_df = construct_constrained_portfolio(factor_scores_at_rebal, sector_info, config)\n",
    "            \n",
    "            if not portfolio_df.empty:\n",
    "                # Define the holding period for this portfolio\n",
    "                relevant_days = daily_returns.index[(daily_returns.index > rebal_date) & (daily_returns.index < next_rebal_date)]\n",
    "                \n",
    "                if not relevant_days.empty:\n",
    "                    for day in relevant_days:\n",
    "                        valid_tickers = portfolio_df.index.intersection(daily_holdings.columns)\n",
    "                        daily_holdings.loc[day, valid_tickers] = portfolio_df.loc[valid_tickers, 'weight']\n",
    "\n",
    "    # 3. PREVENT LOOK-AHEAD BIAS\n",
    "    daily_holdings_shifted = daily_holdings.shift(1).fillna(0)\n",
    "\n",
    "    # 4. CALCULATE GROSS PORTFOLIO RETURNS\n",
    "    gross_returns = (daily_holdings_shifted * daily_returns).sum(axis=1)\n",
    "\n",
    "    # 5. MODEL TRANSACTION COSTS\n",
    "    turnover = (daily_holdings_shifted - daily_holdings_shifted.shift(1)).abs().sum(axis=1) / 2\n",
    "    transaction_costs = turnover * (config['transaction_cost_bps'] / 10000)\n",
    "    \n",
    "    net_returns = gross_returns - transaction_costs\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    total_return = (1 + net_returns).prod() - 1\n",
    "    n_years = len(net_returns) / 252\n",
    "    annual_return = (1 + total_return) ** (1 / n_years) - 1 if n_years > 0 else 0\n",
    "    annual_vol = net_returns.std() * np.sqrt(252)\n",
    "    sharpe_ratio = (annual_return - 0.0) / annual_vol if annual_vol > 0 else 0\n",
    "    \n",
    "    # Calculate max drawdown\n",
    "    cumulative = (1 + net_returns).cumprod()\n",
    "    drawdown = (cumulative / cumulative.cummax() - 1)\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    metrics = {\n",
    "        'annual_return': annual_return,\n",
    "        'annual_vol': annual_vol,\n",
    "        'sharpe_ratio': sharpe_ratio,\n",
    "        'total_return': total_return,\n",
    "        'max_drawdown': max_drawdown\n",
    "    }\n",
    "    \n",
    "    return net_returns, metrics\n",
    "\n",
    "print(\"✅ Backtesting engine loaded and ready for sensitivity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Sensitivity Test Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sensitivity tests as specified by the senior quant\n",
    "sensitivity_tests = {\n",
    "    'transaction_costs': [30, 50, 70, 100],  # bps\n",
    "    'rebalance_freq': ['M', 'Q'],  # Monthly, Quarterly\n",
    "    'selection_pct': [0.15, 0.20, 0.25],  # Top 15%, 20%, 25%\n",
    "    'sector_constraint': [0.30, 0.40, 0.50],  # Max sector weight\n",
    "    'position_limit': [0.03, 0.05, 0.07]  # Max position weight\n",
    "}\n",
    "\n",
    "# Extract QVM scores\n",
    "qvm_scores = factor_data.loc[:, ('qvm_composite_score', slice(None))]\n",
    "qvm_scores.columns = qvm_scores.columns.droplevel(0)\n",
    "\n",
    "# Store all results\n",
    "sensitivity_results = {}\n",
    "\n",
    "# Calculate total number of tests\n",
    "total_tests = sum(len(values) for values in sensitivity_tests.values())\n",
    "print(f\"\\n📊 Sensitivity Test Configuration:\")\n",
    "print(f\"   Total parameter variations to test: {total_tests}\")\n",
    "for param, values in sensitivity_tests.items():\n",
    "    print(f\"   {param}: {values}\")\n",
    "    \n",
    "print(f\"\\n🎯 Baseline configuration:\")\n",
    "print(f\"   Transaction costs: {baseline_config['transaction_cost_bps']} bps\")\n",
    "print(f\"   Rebalance frequency: {baseline_config['rebalance_freq']}\")\n",
    "print(f\"   Selection percentile: {baseline_config['selection_percentile']}\")\n",
    "print(f\"   Max sector weight: {baseline_config['max_sector_weight']}\")\n",
    "print(f\"   Max position weight: {baseline_config['max_position_weight']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Sensitivity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run a single sensitivity test\n",
    "def run_sensitivity_test(param_name: str, param_values: List, \n",
    "                        baseline_config: dict, qvm_scores: pd.DataFrame,\n",
    "                        daily_returns: pd.DataFrame, sector_info: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Run sensitivity test for a single parameter.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\n🔬 Testing sensitivity to: {param_name}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for value in tqdm(param_values, desc=f\"Testing {param_name}\"):\n",
    "        # Create test configuration\n",
    "        test_config = baseline_config.copy()\n",
    "        \n",
    "        # Map parameter names to config keys\n",
    "        param_mapping = {\n",
    "            'transaction_costs': 'transaction_cost_bps',\n",
    "            'rebalance_freq': 'rebalance_freq',\n",
    "            'selection_pct': 'selection_percentile',\n",
    "            'sector_constraint': 'max_sector_weight',\n",
    "            'position_limit': 'max_position_weight'\n",
    "        }\n",
    "        \n",
    "        test_config[param_mapping[param_name]] = value\n",
    "        \n",
    "        # Run backtest with modified configuration\n",
    "        try:\n",
    "            _, metrics = run_backtest_with_config(\n",
    "                qvm_scores, daily_returns, sector_info, test_config\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'parameter': param_name,\n",
    "                'value': value,\n",
    "                'annual_return': metrics['annual_return'],\n",
    "                'sharpe_ratio': metrics['sharpe_ratio'],\n",
    "                'max_drawdown': metrics['max_drawdown'],\n",
    "                'annual_vol': metrics['annual_vol']\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error testing {param_name}={value}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"\\n📊 Results for {param_name}:\")\n",
    "    display(results_df.round(4))\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Run all sensitivity tests\n",
    "print(\"🚀 Executing comprehensive sensitivity analysis...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for param_name, param_values in sensitivity_tests.items():\n",
    "    sensitivity_results[param_name] = run_sensitivity_test(\n",
    "        param_name, param_values, baseline_config, \n",
    "        qvm_scores, daily_returns, sector_info\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\n✅ Sensitivity analysis complete in {(end_time - start_time)/60:.1f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Tornado Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity ranges for tornado plot\n",
    "def calculate_sensitivity_impact(results_df: pd.DataFrame, metric: str = 'sharpe_ratio') -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate the impact range of each parameter on the specified metric.\n",
    "    \"\"\"\n",
    "    min_val = results_df[metric].min()\n",
    "    max_val = results_df[metric].max()\n",
    "    range_val = max_val - min_val\n",
    "    baseline_val = baseline_performance['sharpe_ratio'] if metric == 'sharpe_ratio' else baseline_performance['annual_return']\n",
    "    \n",
    "    return {\n",
    "        'min': min_val,\n",
    "        'max': max_val,\n",
    "        'range': range_val,\n",
    "        'baseline': baseline_val,\n",
    "        'min_impact': (min_val - baseline_val) / baseline_val * 100,\n",
    "        'max_impact': (max_val - baseline_val) / baseline_val * 100\n",
    "    }\n",
    "\n",
    "# Create tornado plot for Sharpe Ratio\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Prepare data for tornado plot\n",
    "tornado_data = []\n",
    "param_labels = {\n",
    "    'transaction_costs': 'Transaction Costs',\n",
    "    'rebalance_freq': 'Rebalancing Frequency',\n",
    "    'selection_pct': 'Selection Percentile',\n",
    "    'sector_constraint': 'Sector Constraint',\n",
    "    'position_limit': 'Position Limit'\n",
    "}\n",
    "\n",
    "for param_name, results_df in sensitivity_results.items():\n",
    "    impact = calculate_sensitivity_impact(results_df, 'sharpe_ratio')\n",
    "    tornado_data.append({\n",
    "        'parameter': param_labels[param_name],\n",
    "        'min_impact': impact['min_impact'],\n",
    "        'max_impact': impact['max_impact'],\n",
    "        'range': abs(impact['max_impact'] - impact['min_impact'])\n",
    "    })\n",
    "\n",
    "# Sort by impact range\n",
    "tornado_df = pd.DataFrame(tornado_data).sort_values('range', ascending=True)\n",
    "\n",
    "# Plot 1: Sharpe Ratio Tornado\n",
    "y_pos = np.arange(len(tornado_df))\n",
    "baseline_sharpe = baseline_performance['sharpe_ratio']\n",
    "\n",
    "# Plot bars\n",
    "for i, row in tornado_df.iterrows():\n",
    "    # Negative impact (red)\n",
    "    ax1.barh(i, row['min_impact'], left=0, height=0.6, \n",
    "             color=FACTOR_COLORS['Negative'] if row['min_impact'] < 0 else FACTOR_COLORS['Positive'],\n",
    "             alpha=0.7)\n",
    "    # Positive impact (green)\n",
    "    ax1.barh(i, row['max_impact'] - row['min_impact'], left=row['min_impact'], height=0.6,\n",
    "             color=FACTOR_COLORS['Positive'] if row['max_impact'] > 0 else FACTOR_COLORS['Negative'],\n",
    "             alpha=0.7)\n",
    "\n",
    "ax1.axvline(x=0, color='black', linewidth=2, label=f'Baseline (SR={baseline_sharpe:.2f})')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(tornado_df['parameter'])\n",
    "ax1.set_xlabel('Impact on Sharpe Ratio (%)')\n",
    "ax1.set_title('Sharpe Ratio Sensitivity Analysis - Tornado Plot', fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "ax1.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, row in tornado_df.iterrows():\n",
    "    ax1.text(row['min_impact'] - 1, i, f\"{row['min_impact']:.1f}%\", \n",
    "             ha='right', va='center', fontsize=9)\n",
    "    ax1.text(row['max_impact'] + 1, i, f\"{row['max_impact']:.1f}%\", \n",
    "             ha='left', va='center', fontsize=9)\n",
    "\n",
    "# Plot 2: Annual Return Tornado\n",
    "tornado_data_return = []\n",
    "for param_name, results_df in sensitivity_results.items():\n",
    "    impact = calculate_sensitivity_impact(results_df, 'annual_return')\n",
    "    tornado_data_return.append({\n",
    "        'parameter': param_labels[param_name],\n",
    "        'min_impact': impact['min_impact'],\n",
    "        'max_impact': impact['max_impact'],\n",
    "        'range': abs(impact['max_impact'] - impact['min_impact'])\n",
    "    })\n",
    "\n",
    "tornado_df_return = pd.DataFrame(tornado_data_return).sort_values('range', ascending=True)\n",
    "\n",
    "# Plot bars for returns\n",
    "for i, row in tornado_df_return.iterrows():\n",
    "    ax2.barh(i, row['min_impact'], left=0, height=0.6,\n",
    "             color=FACTOR_COLORS['Negative'] if row['min_impact'] < 0 else FACTOR_COLORS['Positive'],\n",
    "             alpha=0.7)\n",
    "    ax2.barh(i, row['max_impact'] - row['min_impact'], left=row['min_impact'], height=0.6,\n",
    "             color=FACTOR_COLORS['Positive'] if row['max_impact'] > 0 else FACTOR_COLORS['Negative'],\n",
    "             alpha=0.7)\n",
    "\n",
    "baseline_return = baseline_performance['annual_return']\n",
    "ax2.axvline(x=0, color='black', linewidth=2, label=f'Baseline ({baseline_return:.1%})')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(tornado_df_return['parameter'])\n",
    "ax2.set_xlabel('Impact on Annual Return (%)')\n",
    "ax2.set_title('Annual Return Sensitivity Analysis - Tornado Plot', fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "ax2.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, row in tornado_df_return.iterrows():\n",
    "    ax2.text(row['min_impact'] - 1, i, f\"{row['min_impact']:.1f}%\", \n",
    "             ha='right', va='center', fontsize=9)\n",
    "    ax2.text(row['max_impact'] + 1, i, f\"{row['max_impact']:.1f}%\", \n",
    "             ha='left', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print sensitivity ranking\n",
    "print(\"\\n🎯 PARAMETER SENSITIVITY RANKING (by Sharpe Ratio impact):\")\n",
    "print(\"=\" * 60)\n",
    "tornado_sorted = tornado_df.sort_values('range', ascending=False)\n",
    "for i, row in tornado_sorted.iterrows():\n",
    "    print(f\"{i+1}. {row['parameter']:25s}: {row['range']:5.1f}% total impact\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Parameter Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed plots for each parameter\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 18))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (param_name, results_df) in enumerate(sensitivity_results.items()):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Plot Sharpe ratio vs parameter value\n",
    "    x_values = results_df['value']\n",
    "    \n",
    "    # Convert categorical values to numeric for plotting\n",
    "    if param_name == 'rebalance_freq':\n",
    "        x_numeric = [1 if v == 'M' else 3 for v in x_values]\n",
    "        x_labels = ['Monthly', 'Quarterly']\n",
    "    else:\n",
    "        x_numeric = x_values\n",
    "        x_labels = x_values\n",
    "    \n",
    "    # Plot lines\n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    # Sharpe Ratio (left y-axis)\n",
    "    line1 = ax.plot(x_numeric, results_df['sharpe_ratio'], \n",
    "                    color=FACTOR_COLORS['Sharpe'], marker='o', markersize=8,\n",
    "                    linewidth=2.5, label='Sharpe Ratio')\n",
    "    ax.axhline(y=baseline_performance['sharpe_ratio'], \n",
    "               color=FACTOR_COLORS['Sharpe'], linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Annual Return (right y-axis)\n",
    "    line2 = ax2.plot(x_numeric, results_df['annual_return'], \n",
    "                     color=FACTOR_COLORS['Strategy'], marker='s', markersize=8,\n",
    "                     linewidth=2.5, label='Annual Return', linestyle='--')\n",
    "    ax2.axhline(y=baseline_performance['annual_return'], \n",
    "                color=FACTOR_COLORS['Strategy'], linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel(param_labels[param_name])\n",
    "    ax.set_ylabel('Sharpe Ratio', color=FACTOR_COLORS['Sharpe'])\n",
    "    ax2.set_ylabel('Annual Return', color=FACTOR_COLORS['Strategy'])\n",
    "    ax.tick_params(axis='y', labelcolor=FACTOR_COLORS['Sharpe'])\n",
    "    ax2.tick_params(axis='y', labelcolor=FACTOR_COLORS['Strategy'])\n",
    "    \n",
    "    # Set x-axis labels\n",
    "    if param_name == 'rebalance_freq':\n",
    "        ax.set_xticks(x_numeric)\n",
    "        ax.set_xticklabels(x_labels)\n",
    "    \n",
    "    ax.set_title(f'Sensitivity to {param_labels[param_name]}', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Combined legend\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='best')\n",
    "\n",
    "# Remove extra subplot\n",
    "axes[-1].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify breaking points\n",
    "print(\"\\n⚠️ STRATEGY BREAKING POINTS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for param_name, results_df in sensitivity_results.items():\n",
    "    # Find where Sharpe < 1.0 or Annual Return < 10%\n",
    "    breaking_points = results_df[\n",
    "        (results_df['sharpe_ratio'] < 1.0) | \n",
    "        (results_df['annual_return'] < 0.10)\n",
    "    ]\n",
    "    \n",
    "    if not breaking_points.empty:\n",
    "        print(f\"\\n{param_labels[param_name]}:\")\n",
    "        for _, row in breaking_points.iterrows():\n",
    "            print(f\"   At {row['value']}: Sharpe={row['sharpe_ratio']:.2f}, Return={row['annual_return']:.1%}\")\n",
    "    else:\n",
    "        print(f\"\\n{param_labels[param_name]}: Strategy robust across all tested values ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Maximum Drawdown Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how parameters affect maximum drawdown\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Prepare data for drawdown comparison\n",
    "dd_comparison = []\n",
    "for param_name, results_df in sensitivity_results.items():\n",
    "    for _, row in results_df.iterrows():\n",
    "        dd_comparison.append({\n",
    "            'Parameter': param_labels[param_name],\n",
    "            'Value': str(row['value']),\n",
    "            'Max_Drawdown': abs(row['max_drawdown']) * 100,\n",
    "            'Sharpe': row['sharpe_ratio']\n",
    "        })\n",
    "\n",
    "dd_df = pd.DataFrame(dd_comparison)\n",
    "\n",
    "# Create grouped bar chart\n",
    "params = dd_df['Parameter'].unique()\n",
    "x = np.arange(len(params))\n",
    "width = 0.15\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    param_data = dd_df[dd_df['Parameter'] == param]\n",
    "    values = param_data['Value'].unique()\n",
    "    \n",
    "    for j, value in enumerate(values):\n",
    "        value_data = param_data[param_data['Value'] == value]\n",
    "        dd = value_data['Max_Drawdown'].values[0]\n",
    "        \n",
    "        # Color based on severity\n",
    "        if dd > 50:\n",
    "            color = FACTOR_COLORS['Negative']\n",
    "        elif dd > 40:\n",
    "            color = FACTOR_COLORS['Drawdown']\n",
    "        else:\n",
    "            color = FACTOR_COLORS['Positive']\n",
    "            \n",
    "        ax.bar(i + j*width - (len(values)-1)*width/2, dd, width, \n",
    "               label=value if i == 0 else \"\", color=color, alpha=0.7)\n",
    "\n",
    "# Add baseline line\n",
    "baseline_dd = abs(baseline_performance['total_return'] / (1 + baseline_performance['total_return']) - 1) * 100\n",
    "ax.axhline(y=45.17, color='red', linestyle='--', linewidth=2, label='Baseline DD')\n",
    "\n",
    "ax.set_xlabel('Parameter')\n",
    "ax.set_ylabel('Maximum Drawdown (%)')\n",
    "ax.set_title('Maximum Drawdown Sensitivity Analysis', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(params, rotation=45, ha='right')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 MAXIMUM DRAWDOWN INSIGHTS:\")\n",
    "print(\"=\" * 60)\n",
    "for param_name, results_df in sensitivity_results.items():\n",
    "    dd_range = results_df['max_drawdown'].max() - results_df['max_drawdown'].min()\n",
    "    print(f\"{param_labels[param_name]:25s}: DD range = {abs(dd_range)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Robustness Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 COMPREHENSIVE ROBUSTNESS TESTING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Summary statistics for each parameter\n",
    "summary_data = []\n",
    "for param_name, results_df in sensitivity_results.items():\n",
    "    summary_data.append({\n",
    "        'Parameter': param_labels[param_name],\n",
    "        'Baseline Value': baseline_config[list(sensitivity_tests.keys())[list(sensitivity_tests.keys()).index(param_name)]],\n",
    "        'Sharpe Min': results_df['sharpe_ratio'].min(),\n",
    "        'Sharpe Max': results_df['sharpe_ratio'].max(),\n",
    "        'Sharpe Range': results_df['sharpe_ratio'].max() - results_df['sharpe_ratio'].min(),\n",
    "        'Return Min': f\"{results_df['annual_return'].min():.1%}\",\n",
    "        'Return Max': f\"{results_df['annual_return'].max():.1%}\",\n",
    "        'Critical Level': 'Yes' if results_df['sharpe_ratio'].min() < 1.0 else 'No'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n🎯 KEY FINDINGS:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Most sensitive parameters\n",
    "most_sensitive = summary_df.sort_values('Sharpe Range', ascending=False).iloc[0]\n",
    "print(f\"\\n1. MOST SENSITIVE PARAMETER: {most_sensitive['Parameter']}\")\n",
    "print(f\"   - Sharpe ratio range: {most_sensitive['Sharpe Range']:.3f}\")\n",
    "print(f\"   - Can reduce Sharpe from {most_sensitive['Sharpe Max']:.2f} to {most_sensitive['Sharpe Min']:.2f}\")\n",
    "\n",
    "# Breaking points\n",
    "critical_params = summary_df[summary_df['Critical Level'] == 'Yes']\n",
    "if not critical_params.empty:\n",
    "    print(\"\\n2. PARAMETERS WITH CRITICAL BREAKING POINTS:\")\n",
    "    for _, param in critical_params.iterrows():\n",
    "        print(f\"   - {param['Parameter']}: Sharpe can fall below 1.0\")\n",
    "else:\n",
    "    print(\"\\n2. NO CRITICAL BREAKING POINTS: Strategy maintains Sharpe > 1.0 across all tests ✅\")\n",
    "\n",
    "# Robust parameters\n",
    "robust_params = summary_df[summary_df['Sharpe Range'] < 0.2]\n",
    "print(\"\\n3. MOST ROBUST PARAMETERS (low sensitivity):\")\n",
    "for _, param in robust_params.iterrows():\n",
    "    print(f\"   - {param['Parameter']}: Sharpe range only {param['Sharpe Range']:.3f}\")\n",
    "\n",
    "# Final assessment\n",
    "print(\"\\n📋 FINAL ROBUSTNESS ASSESSMENT:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "avg_sharpe_range = summary_df['Sharpe Range'].mean()\n",
    "if avg_sharpe_range < 0.3:\n",
    "    robustness_grade = \"HIGHLY ROBUST\"\n",
    "    grade_color = \"✅\"\n",
    "elif avg_sharpe_range < 0.5:\n",
    "    robustness_grade = \"MODERATELY ROBUST\"\n",
    "    grade_color = \"⚠️\"\n",
    "else:\n",
    "    robustness_grade = \"FRAGILE\"\n",
    "    grade_color = \"❌\"\n",
    "\n",
    "print(f\"\\nStrategy Robustness Grade: {grade_color} {robustness_grade}\")\n",
    "print(f\"Average Sharpe sensitivity: {avg_sharpe_range:.3f}\")\n",
    "print(f\"\\nBaseline Sharpe of {baseline_performance['sharpe_ratio']:.2f} is {'sustainable' if avg_sharpe_range < 0.3 else 'sensitive to assumptions'}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n💡 RECOMMENDATIONS FOR PHASE 8:\")\n",
    "print(\"=\" * 60)\n",
    "if 'transaction_costs' in critical_params['Parameter'].values:\n",
    "    print(\"1. Implement smart execution algorithms to minimize transaction costs\")\n",
    "if 'rebalance_freq' in summary_df.sort_values('Sharpe Range', ascending=False).head(2)['Parameter'].values:\n",
    "    print(\"2. Consider dynamic rebalancing based on market volatility\")\n",
    "if 'position_limit' in robust_params['Parameter'].values:\n",
    "    print(\"3. Position limits can be relaxed without significant impact\")\n",
    "    \n",
    "print(\"\\n4. PRIMARY FOCUS: Implement drawdown control mechanisms\")\n",
    "print(\"   - Current -45% drawdown is the main risk, not parameter sensitivity\")\n",
    "print(\"   - Consider volatility targeting or regime-based exposure adjustment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Robustness Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results for future reference\n",
    "robustness_results = {\n",
    "    'test_date': datetime.now(),\n",
    "    'baseline_performance': baseline_performance,\n",
    "    'sensitivity_tests': sensitivity_tests,\n",
    "    'sensitivity_results': sensitivity_results,\n",
    "    'summary_table': summary_df,\n",
    "    'tornado_data': {\n",
    "        'sharpe': tornado_df,\n",
    "        'returns': tornado_df_return\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to pickle\n",
    "save_path = data_path / \"robustness_testing_results.pkl\"\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(robustness_results, f)\n",
    "\n",
    "print(f\"\\n💾 Robustness testing results saved to: {save_path.name}\")\n",
    "print(f\"   File size: {save_path.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Generate markdown report\n",
    "report_path = data_path / \"05_robustness_testing_report.md\"\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(f\"# Robustness Testing Report\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M')}\\n\\n\")\n",
    "    f.write(f\"## Executive Summary\\n\")\n",
    "    f.write(f\"- Strategy Grade: {robustness_grade}\\n\")\n",
    "    f.write(f\"- Baseline Sharpe: {baseline_performance['sharpe_ratio']:.2f}\\n\")\n",
    "    f.write(f\"- Most Sensitive Parameter: {most_sensitive['Parameter']}\\n\")\n",
    "    f.write(f\"- Average Sharpe Sensitivity: {avg_sharpe_range:.3f}\\n\\n\")\n",
    "    f.write(f\"## Detailed Results\\n\")\n",
    "    f.write(summary_df.to_markdown())\n",
    "    \n",
    "print(f\"📄 Markdown report saved to: {report_path.name}\")\n",
    "\n",
    "print(\"\\n✅ PHASE 7 COMPLETE: Ready for Phase 8 Risk-Managed Strategy Development\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}