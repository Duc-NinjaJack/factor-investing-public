# %% [markdown]
# # QVM Engine v3g - Integrated Innovation Implementation
# 
# **Objective:** Complete integration of all innovations from v3f, v3e percentile, and v3e optimized:
# - Top 200 stocks by ADTV (from v3f)
# - Adaptive percentile-based regime detection with historical learning (from v3e percentile)
# - Performance optimization with pre-loaded data (from v3e optimized)
# - Extended backtest period 2016-2025 with multiple tearsheets
# 
# **Key Innovations:**
# - Top 200 universe by ADTV instead of hard thresholds
# - Dynamic percentile-based regime detection that adapts to market conditions
# - Historical learning for regime thresholds
# - Optimized data preloading for 70-90% speedup
# - Vectorized factor calculations
# - No synthetic data, no look-ahead bias
# 
# **File:** 06_qvm_engine_v3g_integrated.py
# 
# ---

# %% [code]
# Core scientific libraries
import pandas as pd
import numpy as np
from datetime import datetime
import warnings
from pathlib import Path
import sys
import yaml

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Database connectivity
from sqlalchemy import create_engine, text

# --- Environment Setup ---
warnings.filterwarnings('ignore')

# %% [code]
# --- Add Project Root to Python Path ---
try:
    current_path = Path.cwd()
    while not (current_path / 'production').is_dir():
        if current_path.parent == current_path:
            raise FileNotFoundError("Could not find the 'production' directory.")
        current_path = current_path.parent
    
    project_root = current_path
    
    if str(project_root) not in sys.path:
        sys.path.insert(0, str(project_root))
    
    from production.database.connection import get_database_manager
    from production.database.mappings.financial_mapping_manager import FinancialMappingManager
    print(f"âœ… Successfully imported production modules.")
    print(f"   - Project Root set to: {project_root}")

except (ImportError, FileNotFoundError) as e:
    print(f"âŒ ERROR: Could not import production modules. Please check your directory structure.")
    print(f"   - Final Path Searched: {project_root}")
    print(f"   - Error: {e}")
    raise

# %% [code]
# --- QVM Engine v3g Integrated Configuration ---
QVM_CONFIG = {
    # --- Backtest Parameters ---
    "strategy_name": "QVM_Engine_v3g_Integrated",
    "backtest_start_date": "2016-01-01",
    "backtest_end_date": "2025-12-31",
    "rebalance_frequency": "M", # Monthly
    "transaction_cost_bps": 30, # Flat 30bps

    # --- Universe Construction (Top 200 by ADTV) ---
    "universe": {
        "lookback_days": 63,
        "top_n_stocks": 200,  # Top 200 stocks by ADTV (from v3f)
        "max_position_size": 0.05,
        "max_sector_exposure": 0.30,
        "target_portfolio_size": 20,
    },

    # --- Factor Configuration ---
    "factors": {
        "roaa_weight": 0.3,
        "pe_weight": 0.3,
        "momentum_weight": 0.4,
        "momentum_horizons": [21, 63, 126, 252], # 1M, 3M, 6M, 12M
        "skip_months": 1,
        "fundamental_lag_days": 45,  # 45-day lag for announcement delay
    },

    # --- Adaptive Regime Detection (from v3e percentile) ---
    "regime": {
        "lookback_period": 90,                    # 90 days lookback period
        "volatility_percentile_high": 75.0,       # 75th percentile for high volatility
        "return_percentile_high": 75.0,           # 75th percentile for high return
        "return_percentile_low": 25.0,            # 25th percentile for low return
        "min_history_required": 30,               # Minimum data points for percentile calculation
        "regime_allocation": {
            'momentum': 0.8,    # High allocation in momentum regime
            'stress': 0.3,      # Low allocation in stress regime
            'normal': 0.6       # Moderate allocation in normal regime
        }
    }
}

print("\nâš™ï¸  QVM Engine v3g Integrated Configuration Loaded:")
print(f"   - Strategy: {QVM_CONFIG['strategy_name']}")
print(f"   - Period: {QVM_CONFIG['backtest_start_date']} to {QVM_CONFIG['backtest_end_date']}")
print(f"   - Universe: Top {QVM_CONFIG['universe']['top_n_stocks']} stocks by ADTV")
print(f"   - Factors: ROAA + P/E + Multi-horizon Momentum")
print(f"   - Regime Detection: Adaptive percentile-based with historical learning")
print(f"   - Performance: Optimized with pre-loaded data")

# %% [code]
# --- Database Connection ---
def create_db_connection():
    """Establishes a SQLAlchemy database engine connection."""
    try:
        db_manager = get_database_manager()
        engine = db_manager.get_engine()
        
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print(f"\nâœ… Database connection established successfully.")
        return engine

    except Exception as e:
        print(f"âŒ FAILED to connect to the database.")
        print(f"   - Error: {e}")
        return None

# Create the engine for this session
engine = create_db_connection()

if engine is None:
    raise ConnectionError("Database connection failed. Halting execution.")

# %% [markdown]
# ## INTEGRATED ADAPTIVE REGIME DETECTOR
# 
# Combines percentile-based thresholds with historical learning from v3e percentile.

# %% [code]
class AdaptiveRegimeDetector:
    """
    Adaptive regime detector with percentile-based thresholds and historical learning.
    Integrated from v3e percentile with enhanced features.
    """
    
    def __init__(self, config: dict):
        """
        Initialize adaptive regime detector
        
        Args:
            config: Configuration dictionary with regime parameters
        """
        regime_config = config['regime']
        self.lookback_period = regime_config['lookback_period']
        self.volatility_percentile_high = regime_config['volatility_percentile_high']
        self.return_percentile_high = regime_config['return_percentile_high']
        self.return_percentile_low = regime_config['return_percentile_low']
        self.min_history_required = regime_config['min_history_required']
        self.regime_allocation = regime_config['regime_allocation']
        
        # Historical data storage for percentile calculation
        self.volatility_history = []
        self.return_history = []
        
        print(f"âœ… AdaptiveRegimeDetector initialized:")
        print(f"   - Lookback Period: {self.lookback_period} days")
        print(f"   - Volatility Percentile: {self.volatility_percentile_high}th")
        print(f"   - Return Percentiles: {self.return_percentile_high}th (high), {self.return_percentile_low}th (low)")
        print(f"   - Min History Required: {self.min_history_required} data points")
    
    def detect_regime(self, price_data: pd.DataFrame) -> str:
        """
        Detect market regime using adaptive percentile-based thresholds
        
        Args:
            price_data: DataFrame with 'close' column
            
        Returns:
            Regime classification: 'momentum', 'stress', or 'normal'
        """
        if len(price_data) < self.lookback_period:
            return 'normal'  # Default regime for insufficient data
            
        # Calculate rolling volatility and returns
        returns = price_data['close'].pct_change().dropna()
        volatility = returns.rolling(window=20).std().dropna()
        
        # Update historical data
        if len(volatility) > 0:
            self.volatility_history.append(volatility.iloc[-1])
        if len(returns) > 0:
            self.return_history.append(returns.iloc[-1])
            
        # Keep only recent history for percentile calculation
        if len(self.volatility_history) > self.lookback_period:
            self.volatility_history = self.volatility_history[-self.lookback_period:]
        if len(self.return_history) > self.lookback_period:
            self.return_history = self.return_history[-self.lookback_period:]
            
        # Calculate dynamic thresholds using percentiles
        if len(self.volatility_history) >= self.min_history_required:
            vol_threshold = np.percentile(self.volatility_history, self.volatility_percentile_high)
            return_threshold_high = np.percentile(self.return_history, self.return_percentile_high)
            return_threshold_low = np.percentile(self.return_history, self.return_percentile_low)
        else:
            # Fallback to reasonable defaults if insufficient data
            vol_threshold = 0.02  # 2% daily volatility
            return_threshold_high = 0.01  # 1% daily return
            return_threshold_low = -0.01  # -1% daily return
            
        # Get current values
        current_vol = volatility.iloc[-1] if len(volatility) > 0 else 0
        current_return = returns.iloc[-1] if len(returns) > 0 else 0
        
        # Regime classification logic
        if current_vol > vol_threshold:
            if current_return > return_threshold_high:
                regime = 'momentum'
            elif current_return < return_threshold_low:
                regime = 'stress'
            else:
                regime = 'normal'
        else:
            regime = 'normal'
        
        # Debug output
        print(f"   ðŸ” Regime Debug: Vol={current_vol:.2%} (thresh={vol_threshold:.2%}), Ret={current_return:.2%} (high={return_threshold_high:.2%}, low={return_threshold_low:.2%})")
        print(f"   ðŸ“ˆ Detected: {regime}")
        
        return regime
    
    def get_regime_allocation(self, regime: str) -> float:
        """
        Get portfolio allocation based on detected regime
        
        Args:
            regime: Detected regime ('momentum', 'stress', 'normal')
            
        Returns:
            Portfolio allocation percentage
        """
        return self.regime_allocation.get(regime, 0.6)

# %% [markdown]
# ## OPTIMIZED DATA PRELOADER (from v3e optimized)
# 
# Pre-loads all data upfront for performance optimization.

# %% [code]
class OptimizedDataPreloader:
    """
    Pre-loads all data upfront to eliminate database queries in loops.
    This is the key optimization that provides 70-90% speedup.
    """
    def __init__(self, config: dict, db_engine):
        self.config = config
        self.engine = db_engine
        self.start_date = pd.Timestamp(config['backtest_start_date'])
        self.end_date = pd.Timestamp(config['backtest_end_date'])
        
        # Add buffer for rolling calculations
        self.buffer_start = self.start_date - pd.DateOffset(months=6)
        
        print(f"ðŸ“‚ Initializing optimized data pre-loader...")
        print(f"   - Period: {self.buffer_start.date()} to {self.end_date.date()}")
    
    def load_all_data(self):
        """Load all data upfront in optimized batches."""
        print("\nðŸ”„ Loading all data in optimized batches...")
        
        # 1. Load price and market data
        price_data = self._load_price_data()
        
        # 2. Load fundamental data
        fundamental_data = self._load_fundamental_data()
        
        # 3. Load benchmark data
        benchmark_data = self._load_benchmark_data()
        
        # 4. Pre-calculate momentum factors
        momentum_data = self._pre_calculate_momentum(price_data)
        
        # 5. Pre-calculate universe eligibility (Top 200 by ADTV)
        universe_data = self._pre_calculate_universe(price_data)
        
        # 6. Pre-calculate regime detection
        regime_data = self._pre_calculate_regime(benchmark_data)
        
        print("âœ… All data pre-loaded successfully!")
        return {
            'price_data': price_data,
            'fundamental_data': fundamental_data,
            'benchmark_data': benchmark_data,
            'momentum_data': momentum_data,
            'universe_data': universe_data,
            'regime_data': regime_data
        }
    
    def _load_price_data(self):
        """Load all price data in one optimized query."""
        print("   - Loading price data...")
        
        query = text("""
            SELECT 
                trading_date as date,
                ticker,
                close_price_adjusted as close,
                total_volume as volume,
                market_cap,
                total_volume * close_price_adjusted as adtv_vnd
            FROM vcsc_daily_data_complete
            WHERE trading_date BETWEEN :start_date AND :end_date
            ORDER BY trading_date, ticker
        """)
        
        price_data = pd.read_sql(query, self.engine, 
                                params={'start_date': self.buffer_start, 'end_date': self.end_date},
                                parse_dates=['date'])
        
        # Create returns matrix
        price_data['return'] = price_data.groupby('ticker')['close'].pct_change()
        returns_matrix = price_data.pivot(index='date', columns='ticker', values='return')
        
        print(f"     âœ… Loaded {len(price_data):,} price observations")
        print(f"     âœ… Created returns matrix: {returns_matrix.shape}")
        
        return {
            'raw_data': price_data,
            'returns_matrix': returns_matrix,
            'price_matrix': price_data.pivot(index='date', columns='ticker', values='close'),
            'volume_matrix': price_data.pivot(index='date', columns='ticker', values='volume'),
            'market_cap_matrix': price_data.pivot(index='date', columns='ticker', values='market_cap'),
            'adtv_matrix': price_data.pivot(index='date', columns='ticker', values='adtv_vnd')
        }
    
    def _load_fundamental_data(self):
        """Load all fundamental data in one optimized query."""
        print("   - Loading fundamental data...")
        
        query = text("""
            WITH quarterly_fundamentals AS (
                SELECT 
                    fv.ticker,
                    fv.year,
                    fv.quarter,
                    DATE(CONCAT(fv.year, '-', LPAD(fv.quarter * 3, 2, '0'), '-01')) as quarter_date,
                    SUM(CASE WHEN fv.item_id = 1 AND fv.statement_type = 'PL' THEN fv.value / 1e9 ELSE 0 END) as netprofit,
                    SUM(CASE WHEN fv.item_id = 2 AND fv.statement_type = 'BS' THEN fv.value / 1e9 ELSE 0 END) as totalassets,
                    SUM(CASE WHEN fv.item_id = 2 AND fv.statement_type = 'PL' THEN fv.value / 1e9 ELSE 0 END) as revenue
                FROM fundamental_values fv
                WHERE fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)
                GROUP BY fv.ticker, fv.year, fv.quarter
            )
            SELECT 
                qf.ticker,
                mi.sector,
                qf.quarter_date as date,
                qf.netprofit,
                qf.totalassets,
                qf.revenue,
                CASE 
                    WHEN qf.totalassets > 0 THEN qf.netprofit / qf.totalassets 
                    ELSE NULL 
                END as roaa,
                CASE 
                    WHEN qf.revenue > 0 THEN qf.netprofit / qf.revenue
                    ELSE NULL 
                END as net_margin,
                CASE 
                    WHEN qf.totalassets > 0 THEN qf.revenue / qf.totalassets
                    ELSE NULL 
                END as asset_turnover
            FROM quarterly_fundamentals qf
            LEFT JOIN master_info mi ON qf.ticker = mi.ticker
            WHERE qf.netprofit > 0 
            AND qf.totalassets > 0
            AND qf.revenue > 0
            ORDER BY qf.ticker, qf.quarter_date
        """)
        
        fundamental_data = pd.read_sql(query, self.engine, 
                                      params={'start_date': self.buffer_start, 'end_date': self.end_date},
                                      parse_dates=['date'])
        
        print(f"     âœ… Loaded {len(fundamental_data):,} fundamental observations")
        return fundamental_data
    
    def _load_benchmark_data(self):
        """Load benchmark data (VN-Index)."""
        print("   - Loading benchmark data...")
        
        query = text("""
            SELECT date, close
            FROM etf_history
            WHERE ticker = 'VNINDEX' AND date BETWEEN :start_date AND :end_date
            ORDER BY date
        """)
        
        benchmark_data = pd.read_sql(query, self.engine, 
                                    params={'start_date': self.buffer_start, 'end_date': self.end_date},
                                    parse_dates=['date'])
        
        benchmark_returns = benchmark_data.set_index('date')['close'].pct_change()
        
        print(f"     âœ… Loaded {len(benchmark_data):,} benchmark observations")
        return benchmark_returns
    
    def _pre_calculate_momentum(self, price_data):
        """Pre-calculate momentum factors for all dates and tickers."""
        print("   - Pre-calculating momentum factors...")
        
        price_matrix = price_data['price_matrix']
        momentum_horizons = self.config['factors']['momentum_horizons']
        skip_months = self.config['factors']['skip_months']
        
        momentum_data = {}
        
        for horizon in momentum_horizons:
            momentum_data[f'momentum_{horizon}d'] = {}
            
            for ticker in price_matrix.columns:
                ticker_prices = price_matrix[ticker].dropna()
                
                if len(ticker_prices) < horizon + skip_months:
                    continue
                
                # Calculate momentum with skip month
                current_price = ticker_prices.iloc[skip_months]
                past_price = ticker_prices.iloc[horizon + skip_months - 1]
                momentum = (current_price / past_price) - 1
                
                momentum_data[f'momentum_{horizon}d'][ticker] = momentum
        
        print(f"     âœ… Pre-calculated momentum for {len(momentum_horizons)} horizons")
        return momentum_data
    
    def _pre_calculate_universe(self, price_data):
        """Pre-calculate universe eligibility (Top 200 by ADTV)."""
        print("   - Pre-calculating universe eligibility (Top 200 by ADTV)...")
        
        adtv_matrix = price_data['adtv_matrix']
        lookback_days = self.config['universe']['lookback_days']
        top_n_stocks = self.config['universe']['top_n_stocks']
        
        universe_mask = pd.DataFrame(index=adtv_matrix.index, columns=adtv_matrix.columns, dtype=bool)
        
        for date in adtv_matrix.index:
            # Get rolling ADTV for lookback period
            start_date = date - pd.Timedelta(days=lookback_days)
            rolling_adtv = adtv_matrix.loc[start_date:date].mean()
            
            # Select top N stocks by ADTV
            top_stocks = rolling_adtv.nlargest(top_n_stocks).index
            
            # Mark as eligible
            universe_mask.loc[date, top_stocks] = True
            universe_mask.loc[date, universe_mask.columns.difference(top_stocks)] = False
        
        print(f"     âœ… Pre-calculated universe eligibility for {len(adtv_matrix.index)} dates")
        return universe_mask
    
    def _pre_calculate_regime(self, benchmark_returns):
        """Pre-calculate regime detection for all dates."""
        print("   - Pre-calculating regime detection...")
        
        regime_detector = AdaptiveRegimeDetector(self.config)
        regime_data = {}
        
        for date in benchmark_returns.index:
            # Get historical data up to this date
            historical_data = benchmark_returns.loc[:date]
            
            if len(historical_data) < 20:  # Minimum data requirement
                regime_data[date] = 'normal'
                continue
            
            # Create price series for regime detection
            price_series = (1 + historical_data).cumprod()
            price_data = pd.DataFrame({'close': price_series})
            
            # Detect regime
            regime = regime_detector.detect_regime(price_data)
            regime_data[date] = regime
        
        print(f"     âœ… Pre-calculated regime detection for {len(regime_data)} dates")
        return regime_data 