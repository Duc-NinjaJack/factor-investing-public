{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f779b7c",
   "metadata": {},
   "source": [
    "# QVM Engine v3g - Integrated Innovation Implementation\n",
    "\n",
    "**Objective:** Complete integration of all innovations from v3f, v3e percentile, and v3e optimized:\n",
    "- Top 200 stocks by ADTV (from v3f)\n",
    "- Adaptive percentile-based regime detection with historical learning (from v3e percentile)\n",
    "- Performance optimization with pre-loaded data (from v3e optimized)\n",
    "- Extended backtest period 2016-2025 with multiple tearsheets\n",
    "\n",
    "**Key Innovations:**\n",
    "- Top 200 universe by ADTV instead of hard thresholds\n",
    "- Dynamic percentile-based regime detection that adapts to market conditions\n",
    "- Historical learning for regime thresholds\n",
    "- Optimized data preloading for 70-90% speedup\n",
    "- Vectorized factor calculations\n",
    "- No synthetic data, no look-ahead bias\n",
    "\n",
    "**File:** 06_qvm_engine_v3g_integrated.py\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc462ef0",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Core scientific libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Database connectivity\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# --- Environment Setup ---\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6de47bda",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported production modules.\n",
      "   - Project Root set to: /Users/raymond/Documents/Projects/factor-investing-public\n"
     ]
    }
   ],
   "source": [
    "# --- Add Project Root to Python Path ---\n",
    "try:\n",
    "    current_path = Path.cwd()\n",
    "    while not (current_path / 'production').is_dir():\n",
    "        if current_path.parent == current_path:\n",
    "            raise FileNotFoundError(\"Could not find the 'production' directory.\")\n",
    "        current_path = current_path.parent\n",
    "    \n",
    "    project_root = current_path\n",
    "    \n",
    "    if str(project_root) not in sys.path:\n",
    "        sys.path.insert(0, str(project_root))\n",
    "    \n",
    "    from production.database.connection import get_database_manager\n",
    "    from production.database.mappings.financial_mapping_manager import FinancialMappingManager\n",
    "    print(f\"✅ Successfully imported production modules.\")\n",
    "    print(f\"   - Project Root set to: {project_root}\")\n",
    "\n",
    "except (ImportError, FileNotFoundError) as e:\n",
    "    print(f\"❌ ERROR: Could not import production modules. Please check your directory structure.\")\n",
    "    print(f\"   - Final Path Searched: {project_root}\")\n",
    "    print(f\"   - Error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7c6cfd",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️  QVM Engine v3g Integrated Configuration Loaded:\n",
      "   - Strategy: QVM_Engine_v3g_Integrated\n",
      "   - Period: 2016-01-01 to 2025-12-31\n",
      "   - Universe: Top 200 stocks by ADTV\n",
      "   - Factors: ROAA + P/E + Multi-horizon Momentum\n",
      "   - Regime Detection: Adaptive percentile-based with historical learning\n",
      "   - Performance: Optimized with pre-loaded data\n"
     ]
    }
   ],
   "source": [
    "# --- QVM Engine v3g Integrated Configuration ---\n",
    "QVM_CONFIG = {\n",
    "    # --- Backtest Parameters ---\n",
    "    \"strategy_name\": \"QVM_Engine_v3g_Integrated\",\n",
    "    \"backtest_start_date\": \"2016-01-01\",\n",
    "    \"backtest_end_date\": \"2025-12-31\",\n",
    "    \"rebalance_frequency\": \"M\", # Monthly\n",
    "    \"transaction_cost_bps\": 30, # Flat 30bps\n",
    "\n",
    "    # --- Universe Construction (Top 200 by ADTV) ---\n",
    "    \"universe\": {\n",
    "        \"lookback_days\": 63,\n",
    "        \"top_n_stocks\": 200,  # Top 200 stocks by ADTV (from v3f)\n",
    "        \"max_position_size\": 0.05,\n",
    "        \"max_sector_exposure\": 0.30,\n",
    "        \"target_portfolio_size\": 20,\n",
    "    },\n",
    "\n",
    "    # --- Factor Configuration ---\n",
    "    \"factors\": {\n",
    "        \"roaa_weight\": 0.3,\n",
    "        \"pe_weight\": 0.3,\n",
    "        \"momentum_weight\": 0.4,\n",
    "        \"momentum_horizons\": [21, 63, 126, 252], # 1M, 3M, 6M, 12M\n",
    "        \"skip_months\": 1,\n",
    "        \"fundamental_lag_days\": 45,  # 45-day lag for announcement delay\n",
    "    },\n",
    "\n",
    "    # --- Adaptive Regime Detection (from v3e percentile) ---\n",
    "    \"regime\": {\n",
    "        \"lookback_period\": 90,                    # 90 days lookback period\n",
    "        \"volatility_percentile_high\": 75.0,       # 75th percentile for high volatility\n",
    "        \"return_percentile_high\": 75.0,           # 75th percentile for high return\n",
    "        \"return_percentile_low\": 25.0,            # 25th percentile for low return\n",
    "        \"min_history_required\": 30,               # Minimum data points for percentile calculation\n",
    "        \"regime_allocation\": {\n",
    "            'momentum': 0.8,    # High allocation in momentum regime\n",
    "            'stress': 0.3,      # Low allocation in stress regime\n",
    "            'normal': 0.6       # Moderate allocation in normal regime\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n⚙️  QVM Engine v3g Integrated Configuration Loaded:\")\n",
    "print(f\"   - Strategy: {QVM_CONFIG['strategy_name']}\")\n",
    "print(f\"   - Period: {QVM_CONFIG['backtest_start_date']} to {QVM_CONFIG['backtest_end_date']}\")\n",
    "print(f\"   - Universe: Top {QVM_CONFIG['universe']['top_n_stocks']} stocks by ADTV\")\n",
    "print(f\"   - Factors: ROAA + P/E + Multi-horizon Momentum\")\n",
    "print(f\"   - Regime Detection: Adaptive percentile-based with historical learning\")\n",
    "print(f\"   - Performance: Optimized with pre-loaded data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae2b2a7c",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Database connection established successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Database Connection ---\n",
    "def create_db_connection():\n",
    "    \"\"\"Establishes a SQLAlchemy database engine connection.\"\"\"\n",
    "    try:\n",
    "        db_manager = get_database_manager()\n",
    "        engine = db_manager.get_engine()\n",
    "        \n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(\"SELECT 1\"))\n",
    "        print(f\"\\n✅ Database connection established successfully.\")\n",
    "        return engine\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ FAILED to connect to the database.\")\n",
    "        print(f\"   - Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create the engine for this session\n",
    "engine = create_db_connection()\n",
    "\n",
    "if engine is None:\n",
    "    raise ConnectionError(\"Database connection failed. Halting execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa73af",
   "metadata": {},
   "source": [
    "## INTEGRATED ADAPTIVE REGIME DETECTOR\n",
    "\n",
    "Combines percentile-based thresholds with historical learning from v3e percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8a6e17d",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "class AdaptiveRegimeDetector:\n",
    "    \"\"\"\n",
    "    Adaptive regime detector with percentile-based thresholds and historical learning.\n",
    "    Integrated from v3e percentile with enhanced features.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        \"\"\"\n",
    "        Initialize adaptive regime detector\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary with regime parameters\n",
    "        \"\"\"\n",
    "        regime_config = config['regime']\n",
    "        self.lookback_period = regime_config['lookback_period']\n",
    "        self.volatility_percentile_high = regime_config['volatility_percentile_high']\n",
    "        self.return_percentile_high = regime_config['return_percentile_high']\n",
    "        self.return_percentile_low = regime_config['return_percentile_low']\n",
    "        self.min_history_required = regime_config['min_history_required']\n",
    "        self.regime_allocation = regime_config['regime_allocation']\n",
    "        \n",
    "        # Historical data storage for percentile calculation\n",
    "        self.volatility_history = []\n",
    "        self.return_history = []\n",
    "        \n",
    "        print(f\"✅ AdaptiveRegimeDetector initialized:\")\n",
    "        print(f\"   - Lookback Period: {self.lookback_period} days\")\n",
    "        print(f\"   - Volatility Percentile: {self.volatility_percentile_high}th\")\n",
    "        print(f\"   - Return Percentiles: {self.return_percentile_high}th (high), {self.return_percentile_low}th (low)\")\n",
    "        print(f\"   - Min History Required: {self.min_history_required} data points\")\n",
    "    \n",
    "    def detect_regime(self, price_data: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Detect market regime using adaptive percentile-based thresholds\n",
    "        \n",
    "        Args:\n",
    "            price_data: DataFrame with 'close' column\n",
    "            \n",
    "        Returns:\n",
    "            Regime classification: 'momentum', 'stress', or 'normal'\n",
    "        \"\"\"\n",
    "        if len(price_data) < self.lookback_period:\n",
    "            return 'normal'  # Default regime for insufficient data\n",
    "            \n",
    "        # Calculate rolling volatility and returns\n",
    "        returns = price_data['close'].pct_change().dropna()\n",
    "        volatility = returns.rolling(window=20).std().dropna()\n",
    "        \n",
    "        # Update historical data\n",
    "        if len(volatility) > 0:\n",
    "            self.volatility_history.append(volatility.iloc[-1])\n",
    "        if len(returns) > 0:\n",
    "            self.return_history.append(returns.iloc[-1])\n",
    "            \n",
    "        # Keep only recent history for percentile calculation\n",
    "        if len(self.volatility_history) > self.lookback_period:\n",
    "            self.volatility_history = self.volatility_history[-self.lookback_period:]\n",
    "        if len(self.return_history) > self.lookback_period:\n",
    "            self.return_history = self.return_history[-self.lookback_period:]\n",
    "            \n",
    "        # Calculate dynamic thresholds using percentiles\n",
    "        if len(self.volatility_history) >= self.min_history_required:\n",
    "            vol_threshold = np.percentile(self.volatility_history, self.volatility_percentile_high)\n",
    "            return_threshold_high = np.percentile(self.return_history, self.return_percentile_high)\n",
    "            return_threshold_low = np.percentile(self.return_history, self.return_percentile_low)\n",
    "        else:\n",
    "            # Fallback to reasonable defaults if insufficient data\n",
    "            vol_threshold = 0.02  # 2% daily volatility\n",
    "            return_threshold_high = 0.01  # 1% daily return\n",
    "            return_threshold_low = -0.01  # -1% daily return\n",
    "            \n",
    "        # Get current values\n",
    "        current_vol = volatility.iloc[-1] if len(volatility) > 0 else 0\n",
    "        current_return = returns.iloc[-1] if len(returns) > 0 else 0\n",
    "        \n",
    "        # Regime classification logic\n",
    "        if current_vol > vol_threshold:\n",
    "            if current_return > return_threshold_high:\n",
    "                regime = 'momentum'\n",
    "            elif current_return < return_threshold_low:\n",
    "                regime = 'stress'\n",
    "            else:\n",
    "                regime = 'normal'\n",
    "        else:\n",
    "            regime = 'normal'\n",
    "        \n",
    "        # Debug output\n",
    "        print(f\"   🔍 Regime Debug: Vol={current_vol:.2%} (thresh={vol_threshold:.2%}), Ret={current_return:.2%} (high={return_threshold_high:.2%}, low={return_threshold_low:.2%})\")\n",
    "        print(f\"   📈 Detected: {regime}\")\n",
    "        \n",
    "        return regime\n",
    "    \n",
    "    def get_regime_allocation(self, regime: str) -> float:\n",
    "        \"\"\"\n",
    "        Get portfolio allocation based on detected regime\n",
    "        \n",
    "        Args:\n",
    "            regime: Detected regime ('momentum', 'stress', 'normal')\n",
    "            \n",
    "        Returns:\n",
    "            Portfolio allocation percentage\n",
    "        \"\"\"\n",
    "        return self.regime_allocation.get(regime, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41868a52",
   "metadata": {},
   "source": [
    "## OPTIMIZED DATA PRELOADER (from v3e optimized)\n",
    "\n",
    "Pre-loads all data upfront for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ecc843bd",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "class OptimizedDataPreloader:\n",
    "    \"\"\"\n",
    "    Pre-loads all data upfront to eliminate database queries in loops.\n",
    "    This is the key optimization that provides 70-90% speedup.\n",
    "    \"\"\"\n",
    "    def __init__(self, config: dict, db_engine):\n",
    "        self.config = config\n",
    "        self.engine = db_engine\n",
    "        self.start_date = pd.Timestamp(config['backtest_start_date'])\n",
    "        self.end_date = pd.Timestamp(config['backtest_end_date'])\n",
    "        \n",
    "        # Add buffer for rolling calculations\n",
    "        self.buffer_start = self.start_date - pd.DateOffset(months=6)\n",
    "        \n",
    "        print(f\"📂 Initializing optimized data pre-loader...\")\n",
    "        print(f\"   - Period: {self.buffer_start.date()} to {self.end_date.date()}\")\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load all data upfront in optimized batches.\"\"\"\n",
    "        print(\"\\n🔄 Loading all data in optimized batches...\")\n",
    "        \n",
    "        # 1. Load price and market data\n",
    "        price_data = self._load_price_data()\n",
    "        \n",
    "        # 2. Load fundamental data\n",
    "        fundamental_data = self._load_fundamental_data()\n",
    "        \n",
    "        # 3. Load benchmark data\n",
    "        benchmark_data = self._load_benchmark_data()\n",
    "        \n",
    "        # 4. Pre-calculate momentum factors\n",
    "        momentum_data = self._pre_calculate_momentum(price_data)\n",
    "        \n",
    "        # 5. Pre-calculate universe eligibility (Top 200 by ADTV)\n",
    "        universe_data = self._pre_calculate_universe(price_data)\n",
    "        \n",
    "        # 6. Pre-calculate regime detection\n",
    "        regime_data = self._pre_calculate_regime(benchmark_data)\n",
    "        \n",
    "        print(\"✅ All data pre-loaded successfully!\")\n",
    "        return {\n",
    "            'price_data': price_data,\n",
    "            'fundamental_data': fundamental_data,\n",
    "            'benchmark_data': benchmark_data,\n",
    "            'momentum_data': momentum_data,\n",
    "            'universe_data': universe_data,\n",
    "            'regime_data': regime_data\n",
    "        }\n",
    "    \n",
    "    def _load_price_data(self):\n",
    "        \"\"\"Load price data in chunks to avoid disk space issues.\"\"\"\n",
    "        print(\"   - Loading price data in chunks...\")\n",
    "        \n",
    "        # Test database connection first\n",
    "        try:\n",
    "            test_query = text(\"SELECT COUNT(*) as count FROM vcsc_daily_data_complete LIMIT 1\")\n",
    "            test_result = pd.read_sql(test_query, self.engine)\n",
    "            total_records = test_result['count'].iloc[0]\n",
    "            print(f\"     ✅ Database connection test: {total_records:,} records available\")\n",
    "            \n",
    "            # If too many records, warn user about potential disk space issues\n",
    "            if total_records > 1000000:  # More than 1M records\n",
    "                print(f\"     ⚠️ WARNING: Large dataset detected ({total_records:,} records)\")\n",
    "                print(f\"     ⚠️ This may cause disk space issues during data loading\")\n",
    "                print(f\"     ⚠️ Consider using a smaller date range or ensuring sufficient disk space\")\n",
    "                print(f\"     ⚠️ Proceeding with real data loading...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Database connection failed: {e}\")\n",
    "            raise Exception(f\"Database connection failed: {e}. Cannot proceed without database access.\")\n",
    "        \n",
    "        # Load data in much smaller chunks to avoid disk space issues\n",
    "        chunk_size = pd.DateOffset(months=3)  # 3-month chunks instead of 2-year\n",
    "        all_price_data = []\n",
    "        \n",
    "        current_start = self.buffer_start\n",
    "        chunk_count = 0\n",
    "        \n",
    "        while current_start < self.end_date:\n",
    "            chunk_end = min(current_start + chunk_size, self.end_date)\n",
    "            chunk_count += 1\n",
    "            \n",
    "            print(f\"     - Loading chunk {chunk_count}: {current_start.date()} to {chunk_end.date()}\")\n",
    "            \n",
    "            try:\n",
    "                # First, check if data exists for this period\n",
    "                check_query = text(\"\"\"\n",
    "                    SELECT COUNT(*) as count \n",
    "                    FROM vcsc_daily_data_complete \n",
    "                    WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "                \"\"\")\n",
    "                \n",
    "                check_result = pd.read_sql(check_query, self.engine, \n",
    "                                          params={'start_date': current_start, 'end_date': chunk_end})\n",
    "                \n",
    "                available_count = check_result['count'].iloc[0]\n",
    "                print(f\"       - Available records for this period: {available_count:,}\")\n",
    "                \n",
    "                if available_count == 0:\n",
    "                    print(f\"       ⚠️ No data available for chunk {chunk_count}\")\n",
    "                    current_start = chunk_end\n",
    "                    continue\n",
    "                \n",
    "                # If too many records, try even smaller chunk\n",
    "                if available_count > 50000:  # Threshold for disk space issues\n",
    "                    print(f\"       ⚠️ Too many records ({available_count:,}), trying 1-month chunk\")\n",
    "                    smaller_chunk_size = pd.DateOffset(months=1)\n",
    "                    chunk_end = min(current_start + smaller_chunk_size, self.end_date)\n",
    "                \n",
    "                query = text(\"\"\"\n",
    "                    SELECT \n",
    "                        trading_date as date,\n",
    "                        ticker,\n",
    "                        close_price_adjusted as close,\n",
    "                        total_volume as volume,\n",
    "                        market_cap,\n",
    "                        total_volume * close_price_adjusted as adtv_vnd\n",
    "                    FROM vcsc_daily_data_complete\n",
    "                    WHERE trading_date BETWEEN :start_date AND :end_date\n",
    "                    ORDER BY trading_date, ticker\n",
    "                \"\"\")\n",
    "                \n",
    "                chunk_data = pd.read_sql(query, self.engine, \n",
    "                                        params={'start_date': current_start, 'end_date': chunk_end},\n",
    "                                        parse_dates=['date'])\n",
    "                \n",
    "                if not chunk_data.empty:\n",
    "                    all_price_data.append(chunk_data)\n",
    "                    print(f\"       ✅ Loaded {len(chunk_data):,} observations\")\n",
    "                    print(f\"       - Date range: {chunk_data['date'].min()} to {chunk_data['date'].max()}\")\n",
    "                    print(f\"       - Unique tickers: {chunk_data['ticker'].nunique()}\")\n",
    "                    \n",
    "                    # Clear memory after each chunk\n",
    "                    del chunk_data\n",
    "                    import gc\n",
    "                    gc.collect()\n",
    "                else:\n",
    "                    print(f\"       ⚠️ No data for chunk {chunk_count}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"       ❌ Error loading chunk {chunk_count}: {e}\")\n",
    "                # Try with much smaller chunk\n",
    "                smaller_chunk_size = pd.DateOffset(weeks=2)  # 2-week chunks\n",
    "                chunk_end = min(current_start + smaller_chunk_size, self.end_date)\n",
    "                \n",
    "                try:\n",
    "                    print(f\"       - Trying 2-week chunk: {current_start.date()} to {chunk_end.date()}\")\n",
    "                    chunk_data = pd.read_sql(query, self.engine, \n",
    "                                            params={'start_date': current_start, 'end_date': chunk_end},\n",
    "                                            parse_dates=['date'])\n",
    "                    \n",
    "                    if not chunk_data.empty:\n",
    "                        all_price_data.append(chunk_data)\n",
    "                        print(f\"       ✅ Loaded {len(chunk_data):,} observations (2-week chunk)\")\n",
    "                        \n",
    "                        # Clear memory after each chunk\n",
    "                        del chunk_data\n",
    "                        import gc\n",
    "                        gc.collect()\n",
    "                    else:\n",
    "                        print(f\"       ⚠️ No data for 2-week chunk {chunk_count}\")\n",
    "                        \n",
    "                except Exception as e2:\n",
    "                    print(f\"       ❌ Error loading 2-week chunk {chunk_count}: {e2}\")\n",
    "                    # Try with 1-week chunk as last resort\n",
    "                    try:\n",
    "                        tiny_chunk_size = pd.DateOffset(weeks=1)\n",
    "                        chunk_end = min(current_start + tiny_chunk_size, self.end_date)\n",
    "                        print(f\"       - Trying 1-week chunk: {current_start.date()} to {chunk_end.date()}\")\n",
    "                        \n",
    "                        chunk_data = pd.read_sql(query, self.engine, \n",
    "                                                params={'start_date': current_start, 'end_date': chunk_end},\n",
    "                                                parse_dates=['date'])\n",
    "                        \n",
    "                        if not chunk_data.empty:\n",
    "                            all_price_data.append(chunk_data)\n",
    "                            print(f\"       ✅ Loaded {len(chunk_data):,} observations (1-week chunk)\")\n",
    "                            \n",
    "                            # Clear memory after each chunk\n",
    "                            del chunk_data\n",
    "                            import gc\n",
    "                            gc.collect()\n",
    "                        else:\n",
    "                            print(f\"       ⚠️ No data for 1-week chunk {chunk_count}\")\n",
    "                    except Exception as e3:\n",
    "                        print(f\"       ❌ Error loading 1-week chunk {chunk_count}: {e3}\")\n",
    "                        # Skip this chunk and continue\n",
    "                        pass\n",
    "            \n",
    "            current_start = chunk_end\n",
    "        \n",
    "        if not all_price_data:\n",
    "            print(\"     ❌ ERROR: No real price data could be loaded\")\n",
    "            print(\"     ❌ This may be due to disk space issues or database problems\")\n",
    "            print(\"     ❌ Please ensure sufficient disk space and database connectivity\")\n",
    "            raise Exception(\"Failed to load any real price data. Cannot proceed with synthetic data.\")\n",
    "        else:\n",
    "            # Combine all chunks\n",
    "            price_data = pd.concat(all_price_data, ignore_index=True)\n",
    "            price_data = price_data.drop_duplicates(subset=['date', 'ticker'])\n",
    "            price_data = price_data.sort_values(['date', 'ticker'])\n",
    "            print(f\"     ✅ Loaded {len(price_data):,} total real price observations\")\n",
    "        \n",
    "        # Create returns matrix\n",
    "        price_data['return'] = price_data.groupby('ticker')['close'].pct_change()\n",
    "        returns_matrix = price_data.pivot(index='date', columns='ticker', values='return')\n",
    "        \n",
    "        print(f\"     ✅ Created returns matrix: {returns_matrix.shape}\")\n",
    "        print(f\"     ✅ Date range: {price_data['date'].min()} to {price_data['date'].max()}\")\n",
    "        print(f\"     ✅ Unique tickers: {price_data['ticker'].nunique()}\")\n",
    "        \n",
    "        return {\n",
    "            'raw_data': price_data,\n",
    "            'returns_matrix': returns_matrix,\n",
    "            'price_matrix': price_data.pivot(index='date', columns='ticker', values='close'),\n",
    "            'volume_matrix': price_data.pivot(index='date', columns='ticker', values='volume'),\n",
    "            'market_cap_matrix': price_data.pivot(index='date', columns='ticker', values='market_cap'),\n",
    "            'adtv_matrix': price_data.pivot(index='date', columns='ticker', values='adtv_vnd')\n",
    "        }\n",
    "    \n",
    "\n",
    "    \n",
    "    def _load_fundamental_data(self):\n",
    "        \"\"\"Load fundamental data in chunks to avoid disk space issues.\"\"\"\n",
    "        print(\"   - Loading fundamental data in chunks...\")\n",
    "        \n",
    "        # Load data in year chunks to avoid disk space issues\n",
    "        chunk_size = pd.DateOffset(years=1)\n",
    "        all_fundamental_data = []\n",
    "        \n",
    "        current_start = self.buffer_start\n",
    "        chunk_count = 0\n",
    "        \n",
    "        while current_start < self.end_date:\n",
    "            chunk_end = min(current_start + chunk_size, self.end_date)\n",
    "            chunk_count += 1\n",
    "            \n",
    "            print(f\"     - Loading fundamental chunk {chunk_count}: {current_start.date()} to {chunk_end.date()}\")\n",
    "            \n",
    "            try:\n",
    "                query = text(\"\"\"\n",
    "                    WITH quarterly_fundamentals AS (\n",
    "                        SELECT \n",
    "                            fv.ticker,\n",
    "                            fv.year,\n",
    "                            fv.quarter,\n",
    "                            DATE(CONCAT(fv.year, '-', LPAD(fv.quarter * 3, 2, '0'), '-01')) as quarter_date,\n",
    "                            SUM(CASE WHEN fv.item_id = 1 AND fv.statement_type = 'PL' THEN fv.value / 1e9 ELSE 0 END) as netprofit,\n",
    "                            SUM(CASE WHEN fv.item_id = 2 AND fv.statement_type = 'BS' THEN fv.value / 1e9 ELSE 0 END) as totalassets,\n",
    "                            SUM(CASE WHEN fv.item_id = 2 AND fv.statement_type = 'PL' THEN fv.value / 1e9 ELSE 0 END) as revenue\n",
    "                        FROM fundamental_values fv\n",
    "                        WHERE fv.year BETWEEN YEAR(:start_date) AND YEAR(:end_date)\n",
    "                        GROUP BY fv.ticker, fv.year, fv.quarter\n",
    "                    )\n",
    "                    SELECT \n",
    "                        qf.ticker,\n",
    "                        mi.sector,\n",
    "                        qf.quarter_date as date,\n",
    "                        qf.netprofit,\n",
    "                        qf.totalassets,\n",
    "                        qf.revenue,\n",
    "                        CASE \n",
    "                            WHEN qf.totalassets > 0 THEN qf.netprofit / qf.totalassets \n",
    "                            ELSE NULL \n",
    "                        END as roaa,\n",
    "                        CASE \n",
    "                            WHEN qf.revenue > 0 THEN qf.netprofit / qf.revenue\n",
    "                            ELSE NULL \n",
    "                        END as net_margin,\n",
    "                        CASE \n",
    "                            WHEN qf.totalassets > 0 THEN qf.revenue / qf.totalassets\n",
    "                            ELSE NULL \n",
    "                        END as asset_turnover\n",
    "                    FROM quarterly_fundamentals qf\n",
    "                    LEFT JOIN master_info mi ON qf.ticker = mi.ticker\n",
    "                    WHERE qf.netprofit > 0 \n",
    "                    AND qf.totalassets > 0\n",
    "                    AND qf.revenue > 0\n",
    "                    ORDER BY qf.ticker, qf.quarter_date\n",
    "                \"\"\")\n",
    "                \n",
    "                chunk_data = pd.read_sql(query, self.engine, \n",
    "                                        params={'start_date': current_start, 'end_date': chunk_end},\n",
    "                                        parse_dates=['date'])\n",
    "                \n",
    "                if not chunk_data.empty:\n",
    "                    all_fundamental_data.append(chunk_data)\n",
    "                    print(f\"       ✅ Loaded {len(chunk_data):,} fundamental observations\")\n",
    "                else:\n",
    "                    print(f\"       ⚠️ No fundamental data for chunk {chunk_count}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"       ❌ Error loading fundamental chunk {chunk_count}: {e}\")\n",
    "                # Skip this chunk and continue\n",
    "                pass\n",
    "            \n",
    "            current_start = chunk_end\n",
    "        \n",
    "        if not all_fundamental_data:\n",
    "            print(\"     ❌ ERROR: No real fundamental data could be loaded\")\n",
    "            print(\"     ❌ This may be due to disk space issues or database problems\")\n",
    "            print(\"     ❌ Please ensure sufficient disk space and database connectivity\")\n",
    "            raise Exception(\"Failed to load any real fundamental data. Cannot proceed with synthetic data.\")\n",
    "        else:\n",
    "            # Combine all chunks\n",
    "            fundamental_data = pd.concat(all_fundamental_data, ignore_index=True)\n",
    "            fundamental_data = fundamental_data.drop_duplicates(subset=['date', 'ticker'])\n",
    "            fundamental_data = fundamental_data.sort_values(['date', 'ticker'])\n",
    "            print(f\"     ✅ Loaded {len(fundamental_data):,} total real fundamental observations\")\n",
    "        \n",
    "        print(f\"     ✅ Loaded {len(fundamental_data):,} total fundamental observations\")\n",
    "        return fundamental_data\n",
    "        return fundamental_data\n",
    "    \n",
    "    def _load_benchmark_data(self):\n",
    "        \"\"\"Load benchmark data (VN-Index) with error handling.\"\"\"\n",
    "        print(\"   - Loading benchmark data...\")\n",
    "        \n",
    "        try:\n",
    "            query = text(\"\"\"\n",
    "                SELECT date, close\n",
    "                FROM etf_history\n",
    "                WHERE ticker = 'VNINDEX' AND date BETWEEN :start_date AND :end_date\n",
    "                ORDER BY date\n",
    "            \"\"\")\n",
    "            \n",
    "            benchmark_data = pd.read_sql(query, self.engine, \n",
    "                                        params={'start_date': self.buffer_start, 'end_date': self.end_date},\n",
    "                                        parse_dates=['date'])\n",
    "            \n",
    "            if benchmark_data.empty:\n",
    "                print(\"     ❌ ERROR: No benchmark data found\")\n",
    "                print(\"     ❌ This may be due to database problems\")\n",
    "                print(\"     ❌ Please ensure database connectivity and data availability\")\n",
    "                raise Exception(\"Failed to load any benchmark data. Cannot proceed with synthetic data.\")\n",
    "            \n",
    "            benchmark_returns = benchmark_data.set_index('date')['close'].pct_change()\n",
    "            \n",
    "            print(f\"     ✅ Loaded {len(benchmark_data):,} benchmark observations\")\n",
    "            return benchmark_returns\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"     ❌ Error loading benchmark data: {e}\")\n",
    "            print(\"     ❌ Cannot proceed without benchmark data\")\n",
    "            raise Exception(f\"Failed to load benchmark data: {e}. Cannot proceed with synthetic data.\")\n",
    "    \n",
    "    def _pre_calculate_momentum(self, price_data):\n",
    "        \"\"\"Pre-calculate momentum factors for all dates and tickers.\"\"\"\n",
    "        print(\"   - Pre-calculating momentum factors...\")\n",
    "        \n",
    "        price_matrix = price_data['price_matrix']\n",
    "        momentum_horizons = self.config['factors']['momentum_horizons']\n",
    "        skip_months = self.config['factors']['skip_months']\n",
    "        \n",
    "        momentum_data = {}\n",
    "        \n",
    "        for horizon in momentum_horizons:\n",
    "            momentum_data[f'momentum_{horizon}d'] = {}\n",
    "            \n",
    "            for ticker in price_matrix.columns:\n",
    "                ticker_prices = price_matrix[ticker].dropna()\n",
    "                \n",
    "                if len(ticker_prices) < horizon + skip_months:\n",
    "                    continue\n",
    "                \n",
    "                # Calculate momentum with skip month\n",
    "                current_price = ticker_prices.iloc[skip_months]\n",
    "                past_price = ticker_prices.iloc[horizon + skip_months - 1]\n",
    "                momentum = (current_price / past_price) - 1\n",
    "                \n",
    "                momentum_data[f'momentum_{horizon}d'][ticker] = momentum\n",
    "        \n",
    "        print(f\"     ✅ Pre-calculated momentum for {len(momentum_horizons)} horizons\")\n",
    "        return momentum_data\n",
    "    \n",
    "    def _pre_calculate_universe(self, price_data):\n",
    "        \"\"\"Pre-calculate universe eligibility (Top 200 by ADTV).\"\"\"\n",
    "        print(\"   - Pre-calculating universe eligibility (Top 200 by ADTV)...\")\n",
    "        \n",
    "        adtv_matrix = price_data['adtv_matrix']\n",
    "        lookback_days = self.config['universe']['lookback_days']\n",
    "        top_n_stocks = self.config['universe']['top_n_stocks']\n",
    "        \n",
    "        universe_mask = pd.DataFrame(index=adtv_matrix.index, columns=adtv_matrix.columns, dtype=bool)\n",
    "        \n",
    "        for date in adtv_matrix.index:\n",
    "            # Get rolling ADTV for lookback period\n",
    "            start_date = date - pd.Timedelta(days=lookback_days)\n",
    "            rolling_adtv = adtv_matrix.loc[start_date:date].mean()\n",
    "            \n",
    "            # Select top N stocks by ADTV\n",
    "            top_stocks = rolling_adtv.nlargest(top_n_stocks).index\n",
    "            \n",
    "            # Mark as eligible\n",
    "            universe_mask.loc[date, top_stocks] = True\n",
    "            universe_mask.loc[date, universe_mask.columns.difference(top_stocks)] = False\n",
    "        \n",
    "        print(f\"     ✅ Pre-calculated universe eligibility for {len(adtv_matrix.index)} dates\")\n",
    "        return universe_mask\n",
    "    \n",
    "    def _pre_calculate_regime(self, benchmark_returns):\n",
    "        \"\"\"Pre-calculate regime detection for all dates.\"\"\"\n",
    "        print(\"   - Pre-calculating regime detection...\")\n",
    "        \n",
    "        regime_detector = AdaptiveRegimeDetector(self.config)\n",
    "        regime_data = {}\n",
    "        \n",
    "        for date in benchmark_returns.index:\n",
    "            # Get historical data up to this date\n",
    "            historical_data = benchmark_returns.loc[:date]\n",
    "            \n",
    "            if len(historical_data) < 20:  # Minimum data requirement\n",
    "                regime_data[date] = 'normal'\n",
    "                continue\n",
    "            \n",
    "            # Create price series for regime detection\n",
    "            price_series = (1 + historical_data).cumprod()\n",
    "            price_data = pd.DataFrame({'close': price_series})\n",
    "            \n",
    "            # Detect regime\n",
    "            regime = regime_detector.detect_regime(price_data)\n",
    "            regime_data[date] = regime\n",
    "        \n",
    "        print(f\"     ✅ Pre-calculated regime detection for {len(regime_data)} dates\")\n",
    "        return regime_data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc497002",
   "metadata": {},
   "source": [
    "## OPTIMIZED FACTOR CALCULATOR (from v3e optimized)\n",
    "\n",
    "Vectorized factor calculations with sector-aware adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c71cb2cc",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "class OptimizedFactorCalculator:\n",
    "    \"\"\"\n",
    "    Optimized factor calculator with vectorized operations and sector-aware calculations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict):\n",
    "        self.config = config\n",
    "    \n",
    "    def calculate_factors_for_date(self, date: pd.Timestamp, \n",
    "                                 fundamental_data: pd.DataFrame,\n",
    "                                 momentum_data: dict,\n",
    "                                 universe_mask: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate all factors for a specific date using vectorized operations.\n",
    "        \"\"\"\n",
    "        # Get universe for this date\n",
    "        if date not in universe_mask.index:\n",
    "            print(f\"       ⚠️ Date {date.date()} not in universe mask index\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        universe_tickers = universe_mask.loc[date][universe_mask.loc[date] == True].index.tolist()\n",
    "        \n",
    "        if not universe_tickers:\n",
    "            print(f\"       ⚠️ No universe tickers for date {date.date()}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"       - Universe tickers: {len(universe_tickers)}\")\n",
    "        \n",
    "        # Get fundamental data for universe\n",
    "        lag_days = self.config['factors']['fundamental_lag_days']\n",
    "        lag_date = date - pd.Timedelta(days=lag_days)\n",
    "        \n",
    "        fundamental_subset = fundamental_data[\n",
    "            (fundamental_data['ticker'].isin(universe_tickers)) &\n",
    "            (fundamental_data['date'] <= lag_date)\n",
    "        ]\n",
    "        \n",
    "        if fundamental_subset.empty:\n",
    "            print(f\"       ⚠️ No fundamental data available before {lag_date.date()}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"       - Available fundamental records: {len(fundamental_subset)}\")\n",
    "        \n",
    "        # Get latest fundamental data for each ticker\n",
    "        latest_fundamentals = fundamental_subset.groupby('ticker').last().reset_index()\n",
    "        \n",
    "        print(f\"       - Universe tickers with fundamental data: {len(latest_fundamentals)}\")\n",
    "        \n",
    "        # Add momentum factors\n",
    "        factors_df = self._add_momentum_factors(latest_fundamentals, momentum_data, date)\n",
    "        \n",
    "        # Calculate quality-adjusted PE\n",
    "        factors_df = self._calculate_quality_adjusted_pe(factors_df)\n",
    "        \n",
    "        # Calculate composite score\n",
    "        factors_df = self._calculate_composite_score_vectorized(factors_df)\n",
    "        \n",
    "        print(f\"       ✅ Calculated factors for {len(factors_df)} stocks\")\n",
    "        return factors_df\n",
    "    \n",
    "    def _add_momentum_factors(self, fundamental_df: pd.DataFrame, momentum_data: dict, date: pd.Timestamp) -> pd.DataFrame:\n",
    "        \"\"\"Add momentum factors to fundamental data.\"\"\"\n",
    "        momentum_horizons = self.config['factors']['momentum_horizons']\n",
    "        \n",
    "        for horizon in momentum_horizons:\n",
    "            momentum_key = f'momentum_{horizon}d'\n",
    "            if momentum_key in momentum_data:\n",
    "                momentum_values = momentum_data[momentum_key]\n",
    "                \n",
    "                # Add momentum values for available tickers\n",
    "                fundamental_df[momentum_key] = fundamental_df['ticker'].map(momentum_values).fillna(0)\n",
    "        \n",
    "        return fundamental_df\n",
    "    \n",
    "    def _calculate_quality_adjusted_pe(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate quality-adjusted P/E by sector.\"\"\"\n",
    "        if 'roaa' not in factors_df.columns or 'sector' not in factors_df.columns:\n",
    "            return factors_df\n",
    "        \n",
    "        # Create ROAA quintiles within each sector\n",
    "        def safe_qcut(x):\n",
    "            try:\n",
    "                if len(x) < 5:\n",
    "                    return pd.Series(['Q3'] * len(x), index=x.index)\n",
    "                return pd.qcut(x, 5, labels=['Q1', 'Q2', 'Q3', 'Q4', 'Q5'], duplicates='drop')\n",
    "            except ValueError:\n",
    "                return pd.Series(['Q3'] * len(x), index=x.index)\n",
    "        \n",
    "        factors_df['roaa_quintile'] = factors_df.groupby('sector')['roaa'].transform(safe_qcut)\n",
    "        \n",
    "        # Fill missing quintiles with Q3\n",
    "        factors_df['roaa_quintile'] = factors_df['roaa_quintile'].fillna('Q3')\n",
    "        \n",
    "        # Quality-adjusted P/E weights (higher quality = higher weight)\n",
    "        quality_weights = {\n",
    "            'Q1': 0.5,  # Low quality\n",
    "            'Q2': 0.7,\n",
    "            'Q3': 1.0,  # Medium quality\n",
    "            'Q4': 1.3,\n",
    "            'Q5': 1.5   # High quality\n",
    "        }\n",
    "        \n",
    "        factors_df['quality_adjusted_pe'] = factors_df['roaa_quintile'].map(quality_weights)\n",
    "        \n",
    "        # Calculate momentum score with correct signal directions\n",
    "        momentum_columns = [col for col in factors_df.columns if col.startswith('momentum_')]\n",
    "        \n",
    "        if momentum_columns:\n",
    "            momentum_score = 0.0\n",
    "            \n",
    "            for col in momentum_columns:\n",
    "                if 'momentum_63d' in col or 'momentum_126d' in col:  # 3M and 6M - positive\n",
    "                    momentum_score += factors_df[col]\n",
    "                elif 'momentum_21d' in col or 'momentum_252d' in col:  # 1M and 12M - contrarian\n",
    "                    momentum_score -= factors_df[col]  # Negative for contrarian\n",
    "            \n",
    "            # Equal weight the components\n",
    "            factors_df['momentum_score'] = momentum_score / len(momentum_columns)\n",
    "        \n",
    "        return factors_df\n",
    "    \n",
    "    def _calculate_composite_score_vectorized(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calculate composite score using vectorized operations.\"\"\"\n",
    "        factors_df['composite_score'] = 0.0\n",
    "        \n",
    "        # ROAA component (positive signal)\n",
    "        if 'roaa' in factors_df.columns:\n",
    "            roaa_weight = self.config['factors']['roaa_weight']\n",
    "            factors_df['roaa_normalized'] = (factors_df['roaa'] - factors_df['roaa'].mean()) / factors_df['roaa'].std()\n",
    "            factors_df['composite_score'] += factors_df['roaa_normalized'] * roaa_weight\n",
    "        \n",
    "        # P/E component (contrarian signal - lower is better)\n",
    "        if 'quality_adjusted_pe' in factors_df.columns:\n",
    "            pe_weight = self.config['factors']['pe_weight']\n",
    "            factors_df['pe_normalized'] = (factors_df['quality_adjusted_pe'] - factors_df['quality_adjusted_pe'].mean()) / factors_df['quality_adjusted_pe'].std()\n",
    "            factors_df['composite_score'] += (-factors_df['pe_normalized']) * pe_weight  # Negative for contrarian\n",
    "        \n",
    "        # Momentum component (mixed signal - 3M/6M positive, 1M/12M contrarian)\n",
    "        if 'momentum_score' in factors_df.columns:\n",
    "            momentum_weight = self.config['factors']['momentum_weight']\n",
    "            factors_df['momentum_normalized'] = (factors_df['momentum_score'] - factors_df['momentum_score'].mean()) / factors_df['momentum_score'].std()\n",
    "            factors_df['composite_score'] += factors_df['momentum_normalized'] * momentum_weight\n",
    "        \n",
    "        return factors_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2541ae96",
   "metadata": {},
   "source": [
    "## QVM ENGINE V3G INTEGRATED\n",
    "\n",
    "Main engine that integrates all innovations from v3f, v3e percentile, and v3e optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b56f3eb6",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "class QVMEngineV3gIntegrated:\n",
    "    \"\"\"\n",
    "    QVM Engine v3g with integrated innovations:\n",
    "    - Top 200 stocks by ADTV (from v3f)\n",
    "    - Adaptive percentile-based regime detection (from v3e percentile)\n",
    "    - Performance optimization with pre-loaded data (from v3e optimized)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: dict, preloaded_data: dict, db_engine):\n",
    "        self.config = config\n",
    "        self.engine = db_engine\n",
    "        self.preloaded_data = preloaded_data\n",
    "        \n",
    "        # Initialize components\n",
    "        self.regime_detector = AdaptiveRegimeDetector(config)\n",
    "        self.factor_calculator = OptimizedFactorCalculator(config)\n",
    "        \n",
    "        # Extract data from preloaded_data\n",
    "        self.price_data = preloaded_data['price_data']\n",
    "        self.fundamental_data = preloaded_data['fundamental_data']\n",
    "        self.benchmark_returns = preloaded_data['benchmark_data']\n",
    "        self.momentum_data = preloaded_data['momentum_data']\n",
    "        self.universe_mask = preloaded_data['universe_data']\n",
    "        self.regime_data = preloaded_data['regime_data']\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.portfolio_returns = []\n",
    "        self.regime_history = []\n",
    "        self.allocation_history = []\n",
    "        \n",
    "        print(\"✅ QVMEngineV3gIntegrated initialized.\")\n",
    "        print(f\"   - Strategy: {config['strategy_name']}\")\n",
    "        print(f\"   - Data Period: {self.price_data['returns_matrix'].index.min().date()} to {self.price_data['returns_matrix'].index.max().date()}\")\n",
    "\n",
    "    def run_backtest(self) -> (pd.Series, pd.DataFrame):\n",
    "        \"\"\"Executes the full backtesting pipeline.\"\"\"\n",
    "        print(\"\\n🚀 Starting QVM Engine v3g integrated backtest execution...\")\n",
    "        \n",
    "        rebalance_dates = self._generate_rebalance_dates()\n",
    "        daily_holdings, diagnostics = self._run_optimized_backtesting_loop(rebalance_dates)\n",
    "        net_returns = self._calculate_net_returns(daily_holdings)\n",
    "        \n",
    "        print(\"✅ QVM Engine v3g integrated backtest execution complete.\")\n",
    "        return net_returns, diagnostics\n",
    "\n",
    "    def _generate_rebalance_dates(self) -> list:\n",
    "        \"\"\"Generates monthly rebalance dates based on actual trading days.\"\"\"\n",
    "        all_trading_dates = self.price_data['returns_matrix'].index\n",
    "        rebal_dates_calendar = pd.date_range(\n",
    "            start=self.config['backtest_start_date'],\n",
    "            end=self.config['backtest_end_date'],\n",
    "            freq=self.config['rebalance_frequency']\n",
    "        )\n",
    "        actual_rebal_dates = [all_trading_dates[all_trading_dates.searchsorted(d, side='left')-1] for d in rebal_dates_calendar if d >= all_trading_dates.min()]\n",
    "        print(f\"   - Generated {len(actual_rebal_dates)} monthly rebalance dates.\")\n",
    "        return sorted(list(set(actual_rebal_dates)))\n",
    "\n",
    "    def _run_optimized_backtesting_loop(self, rebalance_dates: list) -> (pd.DataFrame, pd.DataFrame):\n",
    "        \"\"\"The optimized loop for portfolio construction at each rebalance date.\"\"\"\n",
    "        daily_holdings = pd.DataFrame(0.0, index=self.price_data['returns_matrix'].index, columns=self.price_data['returns_matrix'].columns)\n",
    "        diagnostics_log = []\n",
    "        \n",
    "        # Track the last successful portfolio for fallback\n",
    "        last_successful_portfolio = None\n",
    "        last_successful_date = None\n",
    "        \n",
    "        for i, rebal_date in enumerate(rebalance_dates):\n",
    "            if i % 10 == 0:\n",
    "                print(f\"   - Processing rebalance {i+1}/{len(rebalance_dates)}: {rebal_date.date()}...\", end=\"\")\n",
    "            \n",
    "            try:\n",
    "                # Get regime from pre-calculated data\n",
    "                regime = self.regime_data.get(rebal_date, 'normal')\n",
    "                regime_allocation = self.regime_detector.get_regime_allocation(regime)\n",
    "                \n",
    "                # Calculate factors using optimized calculator\n",
    "                factors_df = self.factor_calculator.calculate_factors_for_date(\n",
    "                    rebal_date, \n",
    "                    self.fundamental_data,\n",
    "                    self.momentum_data,\n",
    "                    self.universe_mask\n",
    "                )\n",
    "                \n",
    "                if factors_df.empty:\n",
    "                    if i % 10 == 0:\n",
    "                        print(\" ⚠️ No factor data.\")\n",
    "                    # Use last successful portfolio if available\n",
    "                    if last_successful_portfolio is not None:\n",
    "                        target_portfolio = last_successful_portfolio\n",
    "                        if i % 10 == 0:\n",
    "                            print(f\" 🔄 Using last successful portfolio from {last_successful_date.date()}\")\n",
    "                    else:\n",
    "                        if i % 10 == 0:\n",
    "                            print(\" ❌ No fallback portfolio available. Skipping.\")\n",
    "                        continue\n",
    "                else:\n",
    "                    # Apply entry criteria\n",
    "                    qualified_df = self._apply_entry_criteria(factors_df)\n",
    "                    \n",
    "                    if qualified_df.empty:\n",
    "                        if i % 10 == 0:\n",
    "                            print(\" ⚠️ No qualified stocks.\")\n",
    "                        # Use last successful portfolio if available\n",
    "                        if last_successful_portfolio is not None:\n",
    "                            target_portfolio = last_successful_portfolio\n",
    "                            if i % 10 == 0:\n",
    "                                print(f\" 🔄 Using last successful portfolio from {last_successful_date.date()}\")\n",
    "                        else:\n",
    "                            if i % 10 == 0:\n",
    "                                print(\" ❌ No fallback portfolio available. Skipping.\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        # Construct portfolio\n",
    "                        target_portfolio = self._construct_portfolio(qualified_df, regime_allocation)\n",
    "                        \n",
    "                        if target_portfolio.empty:\n",
    "                            if i % 10 == 0:\n",
    "                                print(\" ⚠️ Portfolio empty.\")\n",
    "                            # Use last successful portfolio if available\n",
    "                            if last_successful_portfolio is not None:\n",
    "                                target_portfolio = last_successful_portfolio\n",
    "                                if i % 10 == 0:\n",
    "                                    print(f\" 🔄 Using last successful portfolio from {last_successful_date.date()}\")\n",
    "                            else:\n",
    "                                if i % 10 == 0:\n",
    "                                    print(\" ❌ No fallback portfolio available. Skipping.\")\n",
    "                                continue\n",
    "                        else:\n",
    "                            # Update last successful portfolio\n",
    "                            last_successful_portfolio = target_portfolio.copy()\n",
    "                            last_successful_date = rebal_date\n",
    "                \n",
    "                # Apply holdings\n",
    "                start_period = rebal_date + pd.Timedelta(days=1)\n",
    "                end_period = rebalance_dates[i+1] if i + 1 < len(rebalance_dates) else self.price_data['returns_matrix'].index.max()\n",
    "                holding_dates = self.price_data['returns_matrix'].index[(self.price_data['returns_matrix'].index >= start_period) & (self.price_data['returns_matrix'].index <= end_period)]\n",
    "                \n",
    "                daily_holdings.loc[holding_dates] = 0.0\n",
    "                valid_tickers = target_portfolio.index.intersection(daily_holdings.columns)\n",
    "                daily_holdings.loc[holding_dates, valid_tickers] = target_portfolio[valid_tickers].values\n",
    "                \n",
    "                # Debug: Track last successful rebalance\n",
    "                if i % 10 == 0:\n",
    "                    print(f\" ✅ Universe: {len(factors_df) if not factors_df.empty else 0}, Portfolio: {len(target_portfolio)}, Regime: {regime}, Last Date: {end_period.date()}\")\n",
    "                \n",
    "                # Calculate turnover\n",
    "                if i > 0:\n",
    "                    prev_holdings_idx = self.price_data['returns_matrix'].index.get_loc(rebal_date) - 1\n",
    "                    prev_holdings = daily_holdings.iloc[prev_holdings_idx] if prev_holdings_idx >= 0 else pd.Series(dtype='float64')\n",
    "                else:\n",
    "                    prev_holdings = pd.Series(dtype='float64')\n",
    "\n",
    "                turnover = (target_portfolio - prev_holdings.reindex(target_portfolio.index).fillna(0)).abs().sum() / 2.0\n",
    "                \n",
    "                diagnostics_log.append({\n",
    "                    'date': rebal_date,\n",
    "                    'universe_size': len(factors_df) if not factors_df.empty else 0,\n",
    "                    'portfolio_size': len(target_portfolio),\n",
    "                    'regime': regime,\n",
    "                    'regime_allocation': regime_allocation,\n",
    "                    'turnover': turnover\n",
    "                })\n",
    "                \n",
    "                if i % 10 == 0:\n",
    "                    print(f\" ✅ Universe: {len(factors_df) if not factors_df.empty else 0}, Portfolio: {len(target_portfolio)}, Regime: {regime}, Turnover: {turnover:.2%}\")\n",
    "            \n",
    "            except Exception as e:\n",
    "                if i % 10 == 0:\n",
    "                    print(f\" ❌ Error: {e}\")\n",
    "                # Use last successful portfolio if available\n",
    "                if last_successful_portfolio is not None:\n",
    "                    target_portfolio = last_successful_portfolio\n",
    "                    if i % 10 == 0:\n",
    "                        print(f\" 🔄 Using last successful portfolio from {last_successful_date.date()}\")\n",
    "                    \n",
    "                    # Apply holdings with fallback portfolio\n",
    "                    start_period = rebal_date + pd.Timedelta(days=1)\n",
    "                    end_period = rebalance_dates[i+1] if i + 1 < len(rebalance_dates) else self.price_data['returns_matrix'].index.max()\n",
    "                    holding_dates = self.price_data['returns_matrix'].index[(self.price_data['returns_matrix'].index >= start_period) & (self.price_data['returns_matrix'].index <= end_period)]\n",
    "                    \n",
    "                    daily_holdings.loc[holding_dates] = 0.0\n",
    "                    valid_tickers = target_portfolio.index.intersection(daily_holdings.columns)\n",
    "                    daily_holdings.loc[holding_dates, valid_tickers] = target_portfolio[valid_tickers].values\n",
    "                    \n",
    "                    diagnostics_log.append({\n",
    "                        'date': rebal_date,\n",
    "                        'universe_size': 0,\n",
    "                        'portfolio_size': len(target_portfolio),\n",
    "                        'regime': 'fallback',\n",
    "                        'regime_allocation': 1.0,\n",
    "                        'turnover': 0.0\n",
    "                    })\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "        if diagnostics_log:\n",
    "            return daily_holdings, pd.DataFrame(diagnostics_log).set_index('date')\n",
    "        else:\n",
    "            return daily_holdings, pd.DataFrame()\n",
    "\n",
    "    def _apply_entry_criteria(self, factors_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Apply entry criteria to filter stocks.\"\"\"\n",
    "        # Basic quality filters\n",
    "        qualified = factors_df.copy()\n",
    "        \n",
    "        initial_count = len(qualified)\n",
    "        \n",
    "        if 'roaa' in qualified.columns:\n",
    "            roaa_filtered = qualified[qualified['roaa'] > 0]\n",
    "            roaa_count = len(roaa_filtered)\n",
    "            if roaa_count < initial_count:\n",
    "                print(f\"       - ROAA filter: {initial_count} -> {roaa_count} stocks (removed {initial_count - roaa_count})\")\n",
    "            qualified = roaa_filtered\n",
    "        \n",
    "        if 'net_margin' in qualified.columns:\n",
    "            margin_filtered = qualified[qualified['net_margin'] > 0]\n",
    "            margin_count = len(margin_filtered)\n",
    "            if margin_count < len(qualified):\n",
    "                print(f\"       - Net margin filter: {len(qualified)} -> {margin_count} stocks (removed {len(qualified) - margin_count})\")\n",
    "            qualified = margin_filtered\n",
    "        \n",
    "        final_count = len(qualified)\n",
    "        if final_count == 0:\n",
    "            print(f\"       ⚠️ All {initial_count} stocks filtered out by entry criteria\")\n",
    "        else:\n",
    "            print(f\"       ✅ {final_count}/{initial_count} stocks passed entry criteria\")\n",
    "        \n",
    "        return qualified\n",
    "\n",
    "    def _construct_portfolio(self, qualified_df: pd.DataFrame, regime_allocation: float) -> pd.Series:\n",
    "        \"\"\"Construct the portfolio using the qualified stocks.\"\"\"\n",
    "        if qualified_df.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Sort by composite score\n",
    "        qualified_df = qualified_df.sort_values('composite_score', ascending=False)\n",
    "        \n",
    "        # Select top stocks\n",
    "        target_size = self.config['universe']['target_portfolio_size']\n",
    "        selected_stocks = qualified_df.head(target_size)\n",
    "        \n",
    "        if selected_stocks.empty:\n",
    "            return pd.Series(dtype='float64')\n",
    "        \n",
    "        # Equal weight portfolio\n",
    "        portfolio = pd.Series(regime_allocation / len(selected_stocks), index=selected_stocks['ticker'])\n",
    "        \n",
    "        return portfolio\n",
    "\n",
    "    def _calculate_net_returns(self, daily_holdings: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"Calculate net returns with transaction costs.\"\"\"\n",
    "        print(f\"\\n🔍 DEBUG: _calculate_net_returns\")\n",
    "        print(f\"   - daily_holdings shape: {daily_holdings.shape}\")\n",
    "        print(f\"   - daily_holdings date range: {daily_holdings.index.min()} to {daily_holdings.index.max()}\")\n",
    "        print(f\"   - Non-zero holdings count: {(daily_holdings != 0).sum().sum()}\")\n",
    "        \n",
    "        holdings_shifted = daily_holdings.shift(1).fillna(0.0)\n",
    "        gross_returns = (holdings_shifted * self.price_data['returns_matrix']).sum(axis=1)\n",
    "        \n",
    "        # Calculate turnover and costs\n",
    "        turnover = (holdings_shifted - holdings_shifted.shift(1)).abs().sum(axis=1) / 2.0\n",
    "        costs = turnover * (self.config['transaction_cost_bps'] / 10000)\n",
    "        net_returns = (gross_returns - costs).rename(self.config['strategy_name'])\n",
    "        \n",
    "        print(f\"   - gross_returns shape: {gross_returns.shape}\")\n",
    "        print(f\"   - gross_returns date range: {gross_returns.index.min()} to {gross_returns.index.max()}\")\n",
    "        print(f\"   - Non-zero gross returns count: {(gross_returns != 0).sum()}\")\n",
    "        print(f\"   - First non-zero return date: {gross_returns[gross_returns != 0].index.min() if (gross_returns != 0).any() else 'None'}\")\n",
    "        print(f\"   - Last non-zero return date: {gross_returns[gross_returns != 0].index.max() if (gross_returns != 0).any() else 'None'}\")\n",
    "        \n",
    "        print(\"\\n💸 Net returns calculated.\")\n",
    "        print(f\"   - Total Gross Return: {(1 + gross_returns).prod() - 1:.2%}\")\n",
    "        print(f\"   - Total Net Return: {(1 + net_returns).prod() - 1:.2%}\")\n",
    "        print(f\"   - Total Cost Drag: {(gross_returns.sum() - net_returns.sum()):.2%}\")\n",
    "        \n",
    "        return net_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0baf7",
   "metadata": {},
   "source": [
    "## PERFORMANCE ANALYSIS FUNCTIONS\n",
    "\n",
    "Comprehensive performance analysis and visualization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3d9b86b",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(returns: pd.Series, benchmark: pd.Series, periods_per_year: int = 252) -> dict:\n",
    "    \"\"\"Calculates comprehensive performance metrics with corrected benchmark alignment.\"\"\"\n",
    "    print(f\"\\n🔍 DEBUG: calculate_performance_metrics\")\n",
    "    print(f\"   - returns shape: {returns.shape}\")\n",
    "    print(f\"   - returns date range: {returns.index.min()} to {returns.index.max()}\")\n",
    "    print(f\"   - Non-zero returns count: {(returns != 0).sum()}\")\n",
    "    print(f\"   - First non-zero return: {returns[returns != 0].index.min() if (returns != 0).any() else 'None'}\")\n",
    "    print(f\"   - Last non-zero return: {returns[returns != 0].index.max() if (returns != 0).any() else 'None'}\")\n",
    "    \n",
    "    # Align benchmark\n",
    "    first_trade_date = returns.loc[returns.ne(0)].index.min()\n",
    "    print(f\"   - first_trade_date: {first_trade_date}\")\n",
    "    \n",
    "    if pd.isna(first_trade_date):\n",
    "        return {metric: 0.0 for metric in ['Annualized Return (%)', 'Annualized Volatility (%)', 'Sharpe Ratio', 'Max Drawdown (%)', 'Calmar Ratio', 'Information Ratio', 'Beta']}\n",
    "    \n",
    "    aligned_returns = returns.loc[first_trade_date:]\n",
    "    aligned_benchmark = benchmark.loc[first_trade_date:]\n",
    "    \n",
    "    print(f\"   - aligned_returns shape: {aligned_returns.shape}\")\n",
    "    print(f\"   - aligned_returns date range: {aligned_returns.index.min()} to {aligned_returns.index.max()}\")\n",
    "\n",
    "    n_years = len(aligned_returns) / periods_per_year\n",
    "    annualized_return = ((1 + aligned_returns).prod() ** (1 / n_years) - 1) if n_years > 0 else 0\n",
    "    annualized_volatility = aligned_returns.std() * np.sqrt(periods_per_year)\n",
    "    sharpe_ratio = annualized_return / annualized_volatility if annualized_volatility != 0 else 0.0\n",
    "    \n",
    "    cumulative_returns = (1 + aligned_returns).cumprod()\n",
    "    max_drawdown = (cumulative_returns / cumulative_returns.cummax() - 1).min()\n",
    "    calmar_ratio = annualized_return / abs(max_drawdown) if max_drawdown < 0 else 0.0\n",
    "    \n",
    "    excess_returns = aligned_returns - aligned_benchmark\n",
    "    information_ratio = (excess_returns.mean() * periods_per_year) / (excess_returns.std() * np.sqrt(periods_per_year)) if excess_returns.std() > 0 else 0.0\n",
    "    beta = aligned_returns.cov(aligned_benchmark) / aligned_benchmark.var() if aligned_benchmark.var() > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'Annualized Return (%)': annualized_return * 100,\n",
    "        'Annualized Volatility (%)': annualized_volatility * 100,\n",
    "        'Sharpe Ratio': sharpe_ratio,\n",
    "        'Max Drawdown (%)': max_drawdown * 100,\n",
    "        'Calmar Ratio': calmar_ratio,\n",
    "        'Information Ratio': information_ratio,\n",
    "        'Beta': beta\n",
    "    }\n",
    "\n",
    "def generate_comprehensive_tearsheet(strategy_returns: pd.Series, benchmark_returns: pd.Series, diagnostics: pd.DataFrame, title: str):\n",
    "    \"\"\"Generates comprehensive institutional tearsheet with equity curve and analysis.\"\"\"\n",
    "    \n",
    "    print(f\"\\n🔍 DEBUG: generate_comprehensive_tearsheet\")\n",
    "    print(f\"   - strategy_returns shape: {strategy_returns.shape}\")\n",
    "    print(f\"   - strategy_returns date range: {strategy_returns.index.min()} to {strategy_returns.index.max()}\")\n",
    "    print(f\"   - Non-zero strategy returns: {(strategy_returns != 0).sum()}\")\n",
    "    print(f\"   - benchmark_returns shape: {benchmark_returns.shape}\")\n",
    "    print(f\"   - benchmark_returns date range: {benchmark_returns.index.min()} to {benchmark_returns.index.max()}\")\n",
    "    \n",
    "    # Use the full date range for plotting, not just from first non-zero return\n",
    "    start_date = strategy_returns.index.min()\n",
    "    end_date = strategy_returns.index.max()\n",
    "    \n",
    "    print(f\"   - Using full date range: {start_date} to {end_date}\")\n",
    "    \n",
    "    # Align data for the full period\n",
    "    aligned_strategy_returns = strategy_returns.loc[start_date:end_date].fillna(0)\n",
    "    aligned_benchmark_returns = benchmark_returns.loc[start_date:end_date].fillna(0)\n",
    "    \n",
    "    print(f\"   - aligned_strategy_returns shape: {aligned_strategy_returns.shape}\")\n",
    "    print(f\"   - aligned_benchmark_returns shape: {aligned_benchmark_returns.shape}\")\n",
    "\n",
    "    strategy_metrics = calculate_performance_metrics(strategy_returns, benchmark_returns)\n",
    "    benchmark_metrics = calculate_performance_metrics(benchmark_returns, benchmark_returns)\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 26))\n",
    "    gs = fig.add_gridspec(5, 2, height_ratios=[1.2, 0.8, 0.8, 0.8, 1.2], hspace=0.7, wspace=0.2)\n",
    "    fig.suptitle(title, fontsize=20, fontweight='bold', color='#2C3E50')\n",
    "\n",
    "    # 1. Cumulative Performance (Equity Curve) - Show full period\n",
    "    ax1 = fig.add_subplot(gs[0, :])\n",
    "    \n",
    "    # Calculate cumulative returns for full period\n",
    "    strategy_cumulative = (1 + aligned_strategy_returns).cumprod()\n",
    "    benchmark_cumulative = (1 + aligned_benchmark_returns).cumprod()\n",
    "    \n",
    "    print(f\"   - strategy_cumulative shape: {strategy_cumulative.shape}\")\n",
    "    print(f\"   - strategy_cumulative range: {strategy_cumulative.min():.4f} to {strategy_cumulative.max():.4f}\")\n",
    "    print(f\"   - benchmark_cumulative shape: {benchmark_cumulative.shape}\")\n",
    "    print(f\"   - benchmark_cumulative range: {benchmark_cumulative.min():.4f} to {benchmark_cumulative.max():.4f}\")\n",
    "    \n",
    "    strategy_cumulative.plot(ax=ax1, label='QVM Engine v3g', color='#16A085', lw=2.5)\n",
    "    benchmark_cumulative.plot(ax=ax1, label='VN-Index', color='#34495E', linestyle='--', lw=2)\n",
    "    ax1.set_title('Cumulative Performance (Log Scale)', fontweight='bold')\n",
    "    ax1.set_ylabel('Growth of 1 VND')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend(loc='upper left')\n",
    "    ax1.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 2. Drawdown Analysis - Use full period\n",
    "    ax2 = fig.add_subplot(gs[1, :])\n",
    "    drawdown = ((1 + aligned_strategy_returns).cumprod() / (1 + aligned_strategy_returns).cumprod().cummax() - 1) * 100\n",
    "    drawdown.plot(ax=ax2, color='#C0392B')\n",
    "    ax2.fill_between(drawdown.index, drawdown, 0, color='#C0392B', alpha=0.1)\n",
    "    ax2.set_title('Drawdown Analysis', fontweight='bold')\n",
    "    ax2.set_ylabel('Drawdown (%)')\n",
    "    ax2.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 3. Annual Returns - Use full period\n",
    "    ax3 = fig.add_subplot(gs[2, 0])\n",
    "    strat_annual = aligned_strategy_returns.resample('Y').apply(lambda x: (1+x).prod()-1) * 100\n",
    "    bench_annual = aligned_benchmark_returns.resample('Y').apply(lambda x: (1+x).prod()-1) * 100\n",
    "    pd.DataFrame({'Strategy': strat_annual, 'Benchmark': bench_annual}).plot(kind='bar', ax=ax3, color=['#16A085', '#34495E'])\n",
    "    ax3.set_xticklabels([d.strftime('%Y') for d in strat_annual.index], rotation=45, ha='right')\n",
    "    ax3.set_title('Annual Returns', fontweight='bold')\n",
    "    ax3.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 4. Rolling Sharpe Ratio - Use full period\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    rolling_sharpe = (aligned_strategy_returns.rolling(252).mean() * 252) / (aligned_strategy_returns.rolling(252).std() * np.sqrt(252))\n",
    "    rolling_sharpe.plot(ax=ax4, color='#E67E22')\n",
    "    ax4.axhline(1.0, color='#27AE60', linestyle='--')\n",
    "    ax4.set_title('1-Year Rolling Sharpe Ratio', fontweight='bold')\n",
    "    ax4.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 5. Regime Analysis\n",
    "    ax5 = fig.add_subplot(gs[3, 0])\n",
    "    if not diagnostics.empty and 'regime' in diagnostics.columns:\n",
    "        regime_counts = diagnostics['regime'].value_counts()\n",
    "        regime_counts.plot(kind='bar', ax=ax5, color=['#3498DB', '#E74C3C', '#F39C12', '#9B59B6'])\n",
    "        ax5.set_title('Regime Distribution', fontweight='bold')\n",
    "        ax5.set_ylabel('Number of Rebalances')\n",
    "        ax5.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 6. Portfolio Size Evolution\n",
    "    ax6 = fig.add_subplot(gs[3, 1])\n",
    "    if not diagnostics.empty and 'portfolio_size' in diagnostics.columns:\n",
    "        diagnostics['portfolio_size'].plot(ax=ax6, color='#2ECC71', marker='o', markersize=3)\n",
    "        ax6.set_title('Portfolio Size Evolution', fontweight='bold')\n",
    "        ax6.set_ylabel('Number of Stocks')\n",
    "        ax6.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "    # 7. Performance Metrics Table\n",
    "    ax7 = fig.add_subplot(gs[4:, :])\n",
    "    ax7.axis('off')\n",
    "    summary_data = [['Metric', 'Strategy', 'Benchmark']]\n",
    "    for key in strategy_metrics.keys():\n",
    "        summary_data.append([key, f\"{strategy_metrics[key]:.2f}\", f\"{benchmark_metrics.get(key, 0.0):.2f}\"])\n",
    "    \n",
    "    table = ax7.table(cellText=summary_data[1:], colLabels=summary_data[0], loc='center', cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(14)\n",
    "    table.scale(1, 2.5)\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39822766",
   "metadata": {},
   "source": [
    "## MAIN EXECUTION\n",
    "\n",
    "Execute the complete QVM Engine v3g integrated backtest with multiple tearsheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b08ad",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "🚀 QVM ENGINE V3G: INTEGRATED INNOVATION BACKTEST\n",
      "================================================================================\n",
      "📂 Initializing optimized data pre-loader...\n",
      "   - Period: 2015-07-01 to 2025-12-31\n",
      "\n",
      "🔄 Loading all data in optimized batches...\n",
      "   - Loading price data in chunks...\n",
      "     ✅ Database connection test: 2,319,796 records available\n",
      "     ⚠️ WARNING: Large dataset detected (2,319,796 records)\n",
      "     ⚠️ This may cause disk space issues during data loading\n",
      "     ⚠️ Consider using a smaller date range or ensuring sufficient disk space\n",
      "     ⚠️ Proceeding with real data loading...\n",
      "     - Loading chunk 1: 2015-07-01 to 2015-10-01\n",
      "       - Available records for this period: 35,011\n",
      "       ✅ Loaded 35,011 observations\n",
      "       - Date range: 2015-07-01 00:00:00 to 2015-10-01 00:00:00\n",
      "       - Unique tickers: 537\n",
      "     - Loading chunk 2: 2015-10-01 to 2016-01-01\n",
      "       - Available records for this period: 35,906\n",
      "       ✅ Loaded 35,906 observations\n",
      "       - Date range: 2015-10-01 00:00:00 to 2015-12-31 00:00:00\n",
      "       - Unique tickers: 550\n",
      "     - Loading chunk 3: 2016-01-01 to 2016-04-01\n",
      "       - Available records for this period: 33,169\n",
      "       ✅ Loaded 33,169 observations\n",
      "       - Date range: 2016-01-04 00:00:00 to 2016-04-01 00:00:00\n",
      "       - Unique tickers: 559\n",
      "     - Loading chunk 4: 2016-04-01 to 2016-07-01\n",
      "       - Available records for this period: 35,444\n",
      "       ✅ Loaded 35,444 observations\n",
      "       - Date range: 2016-04-01 00:00:00 to 2016-07-01 00:00:00\n",
      "       - Unique tickers: 567\n",
      "     - Loading chunk 5: 2016-07-01 to 2016-10-01\n",
      "       - Available records for this period: 37,119\n",
      "       ✅ Loaded 37,119 observations\n",
      "       - Date range: 2016-07-01 00:00:00 to 2016-09-30 00:00:00\n",
      "       - Unique tickers: 572\n",
      "     - Loading chunk 6: 2016-10-01 to 2017-01-01\n",
      "       - Available records for this period: 37,550\n",
      "       ✅ Loaded 37,550 observations\n",
      "       - Date range: 2016-10-03 00:00:00 to 2016-12-30 00:00:00\n",
      "       - Unique tickers: 583\n",
      "     - Loading chunk 7: 2017-01-01 to 2017-04-01\n",
      "       - Available records for this period: 34,899\n",
      "       ✅ Loaded 34,899 observations\n",
      "       - Date range: 2017-01-03 00:00:00 to 2017-03-31 00:00:00\n",
      "       - Unique tickers: 596\n",
      "     - Loading chunk 8: 2017-04-01 to 2017-07-01\n",
      "       - Available records for this period: 37,274\n",
      "       ✅ Loaded 37,274 observations\n",
      "       - Date range: 2017-04-03 00:00:00 to 2017-06-30 00:00:00\n",
      "       - Unique tickers: 609\n",
      "     - Loading chunk 9: 2017-07-01 to 2017-10-01\n",
      "       - Available records for this period: 39,585\n",
      "       ✅ Loaded 39,585 observations\n",
      "       - Date range: 2017-07-03 00:00:00 to 2017-09-29 00:00:00\n",
      "       - Unique tickers: 625\n",
      "     - Loading chunk 10: 2017-10-01 to 2018-01-01\n",
      "       - Available records for this period: 40,937\n",
      "       ✅ Loaded 40,937 observations\n",
      "       - Date range: 2017-10-02 00:00:00 to 2017-12-29 00:00:00\n",
      "       - Unique tickers: 638\n",
      "     - Loading chunk 11: 2018-01-01 to 2018-04-01\n",
      "       - Available records for this period: 37,965\n",
      "       ✅ Loaded 37,965 observations\n",
      "       - Date range: 2018-01-02 00:00:00 to 2018-03-30 00:00:00\n",
      "       - Unique tickers: 648\n",
      "     - Loading chunk 12: 2018-04-01 to 2018-07-01\n",
      "       - Available records for this period: 40,444\n",
      "       ✅ Loaded 40,444 observations\n",
      "       - Date range: 2018-04-02 00:00:00 to 2018-06-29 00:00:00\n",
      "       - Unique tickers: 657\n",
      "     - Loading chunk 13: 2018-07-01 to 2018-10-01\n",
      "       - Available records for this period: 42,985\n",
      "       ✅ Loaded 42,985 observations\n",
      "       - Date range: 2018-07-02 00:00:00 to 2018-10-01 00:00:00\n",
      "       - Unique tickers: 666\n",
      "     - Loading chunk 14: 2018-10-01 to 2019-01-01\n",
      "       - Available records for this period: 43,490\n",
      "       ✅ Loaded 43,490 observations\n",
      "       - Date range: 2018-10-01 00:00:00 to 2018-12-28 00:00:00\n",
      "       - Unique tickers: 674\n",
      "     - Loading chunk 15: 2019-01-01 to 2019-04-01\n",
      "       - Available records for this period: 39,969\n",
      "       ✅ Loaded 39,969 observations\n",
      "       - Date range: 2019-01-02 00:00:00 to 2019-04-01 00:00:00\n",
      "       - Unique tickers: 680\n",
      "     - Loading chunk 16: 2019-04-01 to 2019-07-01\n",
      "       - Available records for this period: 42,211\n",
      "       ✅ Loaded 42,211 observations\n",
      "       - Date range: 2019-04-01 00:00:00 to 2019-07-01 00:00:00\n",
      "       - Unique tickers: 684\n",
      "     - Loading chunk 17: 2019-07-01 to 2019-10-01\n",
      "       - Available records for this period: 45,184\n",
      "       ✅ Loaded 45,184 observations\n",
      "       - Date range: 2019-07-01 00:00:00 to 2019-10-01 00:00:00\n",
      "       - Unique tickers: 685\n",
      "     - Loading chunk 18: 2019-10-01 to 2020-01-01\n",
      "       - Available records for this period: 45,207\n",
      "       ✅ Loaded 45,207 observations\n",
      "       - Date range: 2019-10-01 00:00:00 to 2019-12-31 00:00:00\n",
      "       - Unique tickers: 686\n",
      "     - Loading chunk 19: 2020-01-01 to 2020-04-01\n",
      "       - Available records for this period: 41,294\n",
      "       ✅ Loaded 41,294 observations\n",
      "       - Date range: 2020-01-02 00:00:00 to 2020-04-01 00:00:00\n",
      "       - Unique tickers: 692\n",
      "     - Loading chunk 20: 2020-04-01 to 2020-07-01\n",
      "       - Available records for this period: 43,612\n",
      "       ✅ Loaded 43,612 observations\n",
      "       - Date range: 2020-04-01 00:00:00 to 2020-07-01 00:00:00\n",
      "       - Unique tickers: 695\n",
      "     - Loading chunk 21: 2020-07-01 to 2020-10-01\n",
      "       - Available records for this period: 45,877\n",
      "       ✅ Loaded 45,877 observations\n",
      "       - Date range: 2020-07-01 00:00:00 to 2020-10-01 00:00:00\n",
      "       - Unique tickers: 698\n",
      "     - Loading chunk 22: 2020-10-01 to 2021-01-01\n",
      "       - Available records for this period: 46,205\n",
      "       ✅ Loaded 46,205 observations\n",
      "       - Date range: 2020-10-01 00:00:00 to 2020-12-31 00:00:00\n",
      "       - Unique tickers: 705\n",
      "     - Loading chunk 23: 2021-01-01 to 2021-04-01\n",
      "       - Available records for this period: 41,782\n",
      "       ✅ Loaded 41,782 observations\n",
      "       - Date range: 2021-01-04 00:00:00 to 2021-04-01 00:00:00\n",
      "       - Unique tickers: 712\n",
      "     - Loading chunk 24: 2021-04-01 to 2021-07-01\n",
      "       - Available records for this period: 44,939\n",
      "       ✅ Loaded 44,939 observations\n",
      "       - Date range: 2021-04-01 00:00:00 to 2021-07-01 00:00:00\n",
      "       - Unique tickers: 714\n",
      "     - Loading chunk 25: 2021-07-01 to 2021-10-01\n",
      "       - Available records for this period: 46,509\n",
      "       ✅ Loaded 46,509 observations\n",
      "       - Date range: 2021-07-01 00:00:00 to 2021-10-01 00:00:00\n",
      "       - Unique tickers: 717\n",
      "     - Loading chunk 26: 2021-10-01 to 2022-01-01\n",
      "       - Available records for this period: 47,385\n",
      "       ✅ Loaded 47,385 observations\n",
      "       - Date range: 2021-10-01 00:00:00 to 2021-12-31 00:00:00\n",
      "       - Unique tickers: 720\n",
      "     - Loading chunk 27: 2022-01-01 to 2022-04-01\n",
      "       - Available records for this period: 42,535\n",
      "       ✅ Loaded 42,535 observations\n",
      "       - Date range: 2022-01-04 00:00:00 to 2022-04-01 00:00:00\n",
      "       - Unique tickers: 722\n",
      "     - Loading chunk 28: 2022-04-01 to 2022-07-01\n",
      "       - Available records for this period: 45,497\n",
      "       ✅ Loaded 45,497 observations\n",
      "       - Date range: 2022-04-01 00:00:00 to 2022-07-01 00:00:00\n",
      "       - Unique tickers: 723\n",
      "     - Loading chunk 29: 2022-07-01 to 2022-10-01\n",
      "       - Available records for this period: 46,358\n",
      "       ✅ Loaded 46,358 observations\n",
      "       - Date range: 2022-07-01 00:00:00 to 2022-09-30 00:00:00\n",
      "       - Unique tickers: 725\n",
      "     - Loading chunk 30: 2022-10-01 to 2023-01-01\n",
      "       - Available records for this period: 47,145\n",
      "       ✅ Loaded 47,145 observations\n",
      "       - Date range: 2022-10-03 00:00:00 to 2022-12-30 00:00:00\n",
      "       - Unique tickers: 726\n",
      "     - Loading chunk 31: 2023-01-01 to 2023-04-01\n",
      "       - Available records for this period: 42,821\n",
      "       ✅ Loaded 42,821 observations\n",
      "       - Date range: 2023-01-03 00:00:00 to 2023-03-31 00:00:00\n",
      "       - Unique tickers: 726\n",
      "     - Loading chunk 32: 2023-04-01 to 2023-07-01\n",
      "       - Available records for this period: 45,003\n",
      "       ✅ Loaded 45,003 observations\n",
      "       - Date range: 2023-04-03 00:00:00 to 2023-06-30 00:00:00\n",
      "       - Unique tickers: 726\n",
      "     - Loading chunk 33: 2023-07-01 to 2023-10-01\n",
      "       - Available records for this period: 45,707\n",
      "       ✅ Loaded 45,707 observations\n",
      "       - Date range: 2023-07-03 00:00:00 to 2023-09-29 00:00:00\n",
      "       - Unique tickers: 726\n",
      "     - Loading chunk 34: 2023-10-01 to 2024-01-01\n",
      "       - Available records for this period: 47,157\n",
      "       ✅ Loaded 47,157 observations\n",
      "       - Date range: 2023-10-02 00:00:00 to 2023-12-29 00:00:00\n",
      "       - Unique tickers: 727\n",
      "     - Loading chunk 35: 2024-01-01 to 2024-04-01\n",
      "       - Available records for this period: 43,539\n",
      "       ✅ Loaded 43,539 observations\n",
      "       - Date range: 2024-01-02 00:00:00 to 2024-04-01 00:00:00\n",
      "       - Unique tickers: 728\n",
      "     - Loading chunk 36: 2024-04-01 to 2024-07-01\n",
      "       - Available records for this period: 45,047\n",
      "       ✅ Loaded 45,047 observations\n",
      "       - Date range: 2024-04-01 00:00:00 to 2024-07-01 00:00:00\n",
      "       - Unique tickers: 728\n",
      "     - Loading chunk 37: 2024-07-01 to 2024-10-01\n",
      "       - Available records for this period: 47,091\n",
      "       ✅ Loaded 47,091 observations\n",
      "       - Date range: 2024-07-01 00:00:00 to 2024-10-01 00:00:00\n",
      "       - Unique tickers: 726\n",
      "     - Loading chunk 38: 2024-10-01 to 2025-01-01\n",
      "       - Available records for this period: 47,820\n",
      "       ✅ Loaded 47,820 observations\n",
      "       - Date range: 2024-10-01 00:00:00 to 2024-12-31 00:00:00\n",
      "       - Unique tickers: 725\n",
      "     - Loading chunk 39: 2025-01-01 to 2025-04-01\n",
      "       - Available records for this period: 42,690\n",
      "       ✅ Loaded 42,690 observations\n",
      "       - Date range: 2025-01-02 00:00:00 to 2025-04-01 00:00:00\n",
      "       - Unique tickers: 725\n",
      "     - Loading chunk 40: 2025-04-01 to 2025-07-01\n",
      "       - Available records for this period: 44,820\n",
      "       ✅ Loaded 44,820 observations\n",
      "       - Date range: 2025-04-01 00:00:00 to 2025-07-01 00:00:00\n",
      "       - Unique tickers: 724\n",
      "     - Loading chunk 41: 2025-07-01 to 2025-10-01\n",
      "       - Available records for this period: 13,717\n",
      "       ✅ Loaded 13,717 observations\n",
      "       - Date range: 2025-07-01 00:00:00 to 2025-07-25 00:00:00\n",
      "       - Unique tickers: 724\n",
      "     - Loading chunk 42: 2025-10-01 to 2025-12-31\n",
      "       - Available records for this period: 0\n",
      "       ⚠️ No data available for chunk 42\n",
      "     ✅ Loaded 1,695,229 total real price observations\n",
      "     ✅ Created returns matrix: (2520, 728)\n",
      "     ✅ Date range: 2015-07-01 00:00:00 to 2025-07-25 00:00:00\n",
      "     ✅ Unique tickers: 728\n",
      "   - Loading fundamental data in chunks...\n",
      "     - Loading fundamental chunk 1: 2015-07-01 to 2016-07-01\n",
      "       ✅ Loaded 1,921 fundamental observations\n",
      "     - Loading fundamental chunk 2: 2016-07-01 to 2017-07-01\n",
      "       ✅ Loaded 2,015 fundamental observations\n",
      "     - Loading fundamental chunk 3: 2017-07-01 to 2018-07-01\n",
      "       ✅ Loaded 2,141 fundamental observations\n",
      "     - Loading fundamental chunk 4: 2018-07-01 to 2019-07-01\n",
      "       ✅ Loaded 2,213 fundamental observations\n",
      "     - Loading fundamental chunk 5: 2019-07-01 to 2020-07-01\n",
      "       ✅ Loaded 2,288 fundamental observations\n",
      "     - Loading fundamental chunk 6: 2020-07-01 to 2021-07-01\n",
      "       ✅ Loaded 2,285 fundamental observations\n",
      "     - Loading fundamental chunk 7: 2021-07-01 to 2022-07-01\n",
      "       ✅ Loaded 2,252 fundamental observations\n",
      "     - Loading fundamental chunk 8: 2022-07-01 to 2023-07-01\n",
      "       ✅ Loaded 2,245 fundamental observations\n",
      "     - Loading fundamental chunk 9: 2023-07-01 to 2024-07-01\n",
      "       ✅ Loaded 2,210 fundamental observations\n",
      "     - Loading fundamental chunk 10: 2024-07-01 to 2025-07-01\n",
      "       ✅ Loaded 1,431 fundamental observations\n",
      "     - Loading fundamental chunk 11: 2025-07-01 to 2025-12-31\n"
     ]
    }
   ],
   "source": [
    "# Execute the data loading\n",
    "try:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 QVM ENGINE V3G: INTEGRATED INNOVATION BACKTEST\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Initialize data preloader\n",
    "    data_preloader = OptimizedDataPreloader(QVM_CONFIG, engine)\n",
    "    preloaded_data = data_preloader.load_all_data()\n",
    "    \n",
    "    print(\"\\n✅ All data successfully loaded and prepared for the backtest.\")\n",
    "    print(f\"   - Price Data Shape: {preloaded_data['price_data']['returns_matrix'].shape}\")\n",
    "    print(f\"   - Fundamental Data Shape: {preloaded_data['fundamental_data'].shape}\")\n",
    "    print(f\"   - Benchmark Returns: {len(preloaded_data['benchmark_data'])} days\")\n",
    "    print(f\"   - Price Data Date Range: {preloaded_data['price_data']['returns_matrix'].index.min()} to {preloaded_data['price_data']['returns_matrix'].index.max()}\")\n",
    "    print(f\"   - Benchmark Date Range: {preloaded_data['benchmark_data'].index.min()} to {preloaded_data['benchmark_data'].index.max()}\")\n",
    "    \n",
    "    # --- Instantiate and Run the QVM Engine v3g Integrated ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🚀 QVM ENGINE V3G: INTEGRATED INNOVATION EXECUTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    qvm_engine = QVMEngineV3gIntegrated(\n",
    "        config=QVM_CONFIG,\n",
    "        preloaded_data=preloaded_data,\n",
    "        db_engine=engine\n",
    "    )\n",
    "    \n",
    "    qvm_net_returns, qvm_diagnostics = qvm_engine.run_backtest()\n",
    "    \n",
    "    print(f\"\\n🔍 DEBUG: After backtest\")\n",
    "    print(f\"   - qvm_net_returns shape: {qvm_net_returns.shape}\")\n",
    "    print(f\"   - qvm_net_returns date range: {qvm_net_returns.index.min()} to {qvm_net_returns.index.max()}\")\n",
    "    print(f\"   - Non-zero returns count: {(qvm_net_returns != 0).sum()}\")\n",
    "    print(f\"   - First non-zero return date: {qvm_net_returns[qvm_net_returns != 0].index.min() if (qvm_net_returns != 0).any() else 'None'}\")\n",
    "    print(f\"   - Last non-zero return date: {qvm_net_returns[qvm_net_returns != 0].index.max() if (qvm_net_returns != 0).any() else 'None'}\")\n",
    "\n",
    "    # --- Generate Multiple Tearsheets ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"📊 QVM ENGINE V3G: MULTIPLE TEARSHEETS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Full Period Tearsheet (2016-2025)\n",
    "    print(\"\\n📈 Generating Full Period Tearsheet (2016-2025)...\")\n",
    "    # Align benchmark data with strategy returns for full period\n",
    "    aligned_benchmark = preloaded_data['benchmark_data'].reindex(qvm_net_returns.index).fillna(0)\n",
    "    \n",
    "    generate_comprehensive_tearsheet(\n",
    "        qvm_net_returns,\n",
    "        aligned_benchmark,\n",
    "        qvm_diagnostics,\n",
    "        \"QVM Engine v3g Integrated - Full Period (2016-2025)\"\n",
    "    )\n",
    "    \n",
    "    # 2. First Period Tearsheet (2016-2020)\n",
    "    print(\"\\n📈 Generating First Period Tearsheet (2016-2020)...\")\n",
    "    first_period_mask = (qvm_net_returns.index >= '2016-01-01') & (qvm_net_returns.index <= '2020-12-31')\n",
    "    first_period_returns = qvm_net_returns[first_period_mask]\n",
    "    \n",
    "    # Align benchmark data with strategy returns\n",
    "    first_period_benchmark = preloaded_data['benchmark_data'].reindex(first_period_returns.index).fillna(0)\n",
    "    \n",
    "    first_period_diagnostics = qvm_diagnostics[\n",
    "        (qvm_diagnostics.index >= '2016-01-01') & (qvm_diagnostics.index <= '2020-12-31')\n",
    "    ]\n",
    "    \n",
    "    generate_comprehensive_tearsheet(\n",
    "        first_period_returns,\n",
    "        first_period_benchmark,\n",
    "        first_period_diagnostics,\n",
    "        \"QVM Engine v3g Integrated - First Period (2016-2020)\"\n",
    "    )\n",
    "    \n",
    "    # 3. Second Period Tearsheet (2020-2025)\n",
    "    print(\"\\n📈 Generating Second Period Tearsheet (2020-2025)...\")\n",
    "    second_period_mask = (qvm_net_returns.index >= '2020-01-01') & (qvm_net_returns.index <= '2025-12-31')\n",
    "    second_period_returns = qvm_net_returns[second_period_mask]\n",
    "    \n",
    "    # Align benchmark data with strategy returns\n",
    "    second_period_benchmark = preloaded_data['benchmark_data'].reindex(second_period_returns.index).fillna(0)\n",
    "    \n",
    "    second_period_diagnostics = qvm_diagnostics[\n",
    "        (qvm_diagnostics.index >= '2020-01-01') & (qvm_diagnostics.index <= '2025-12-31')\n",
    "    ]\n",
    "    \n",
    "    generate_comprehensive_tearsheet(\n",
    "        second_period_returns,\n",
    "        second_period_benchmark,\n",
    "        second_period_diagnostics,\n",
    "        \"QVM Engine v3g Integrated - Second Period (2020-2025)\"\n",
    "    )\n",
    "\n",
    "    # --- Additional Analysis ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"🔍 ADDITIONAL ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Regime Analysis\n",
    "    if not qvm_diagnostics.empty and 'regime' in qvm_diagnostics.columns:\n",
    "        print(\"\\n📈 Regime Analysis:\")\n",
    "        regime_summary = qvm_diagnostics['regime'].value_counts()\n",
    "        for regime, count in regime_summary.items():\n",
    "            percentage = (count / len(qvm_diagnostics)) * 100\n",
    "            print(f\"   - {regime}: {count} times ({percentage:.2f}%)\")\n",
    "    \n",
    "    # Factor Configuration\n",
    "    print(\"\\n📊 Factor Configuration:\")\n",
    "    print(f\"   - ROAA Weight: {QVM_CONFIG['factors']['roaa_weight']}\")\n",
    "    print(f\"   - P/E Weight: {QVM_CONFIG['factors']['pe_weight']}\")\n",
    "    print(f\"   - Momentum Weight: {QVM_CONFIG['factors']['momentum_weight']}\")\n",
    "    print(f\"   - Momentum Horizons: {QVM_CONFIG['factors']['momentum_horizons']}\")\n",
    "    \n",
    "    # Universe Statistics\n",
    "    if not qvm_diagnostics.empty:\n",
    "        print(f\"\\n🌐 Universe Statistics:\")\n",
    "        print(f\"   - Average Universe Size: {qvm_diagnostics['universe_size'].mean():.0f} stocks\")\n",
    "        print(f\"   - Average Portfolio Size: {qvm_diagnostics['portfolio_size'].mean():.0f} stocks\")\n",
    "        print(f\"   - Average Turnover: {qvm_diagnostics['turnover'].mean():.2%}\")\n",
    "    \n",
    "    # Performance Summary\n",
    "    print(f\"\\n📊 Performance Summary:\")\n",
    "    full_metrics = calculate_performance_metrics(qvm_net_returns, aligned_benchmark)\n",
    "    for metric, value in full_metrics.items():\n",
    "        print(f\"   - {metric}: {value:.2f}\")\n",
    "\n",
    "    print(\"\\n✅ QVM Engine v3g integrated with comprehensive performance analysis complete!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ An error occurred during execution: {e}\")\n",
    "    raise "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
